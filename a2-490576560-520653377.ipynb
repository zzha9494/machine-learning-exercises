{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 2: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group number: 100, SID1: 490576560, SID2: 520653377 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and dependencies\n",
    "All the required libraries/dependencies and the plotting environment are listed and set up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Make the notebook's output stable across runs.\n",
    "# Random seed is set to 0 consistently.\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading, preprocessing, and exploration\n",
    "The documentation for the data loading function can be accessed [here](https://keras.io/api/datasets/fashion_mnist/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68db559",
   "metadata": {},
   "source": [
    "### 1.1 Load data and declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion-MNIST dataset training and test sets as numpy arrays\n",
    "(X_train_original, y_train_original), (X_test_original, y_test_original) = keras.datasets.fashion_mnist.load_data()\n",
    "assert X_train_original.shape == (60000, 28, 28)\n",
    "assert X_test_original.shape == (10000, 28, 28)\n",
    "assert y_train_original.shape == (60000,)\n",
    "assert y_test_original.shape == (10000,)\n",
    "\n",
    "# An ordered list of the class names\n",
    "class_names = [\"T-shirt/top\",\n",
    "               \"Trouser\",\n",
    "               \"Pullover\",\n",
    "               \"Dress\",\n",
    "               \"Coat\",\n",
    "               \"Sandal\",\n",
    "               \"Shirt\",\n",
    "               \"Sneaker\",\n",
    "               \"Bag\",\n",
    "               \"Ankle boot\"\n",
    "              ]\n",
    "\n",
    "# Declare size of the image\n",
    "IMAGE_SIZE = X_train_original[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed0594",
   "metadata": {},
   "source": [
    "### 1.2 Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a23dbff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise data\n",
    "X_train_full = X_train_original.reshape(X_train_original.shape[0], -1) # Flatten data from 3D to 2D\n",
    "y_train_full = y_train_original.copy()\n",
    "X_test = X_test_original.reshape(X_test_original.shape[0], -1) # Flatten data from 3D to 2D\n",
    "y_test = y_test_original.copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_full)\n",
    "X_train_full = scaler.transform(X_train_full) # apply normalisation to the training set\n",
    "X_test = scaler.transform(X_test) # apply normalisation to the test set\n",
    "\n",
    "X_train_full = X_train_full.reshape(X_train_original.shape[0], *IMAGE_SIZE) # restore the dimention from 2D to 3D\n",
    "X_test = X_test.reshape(X_test_original.shape[0], *IMAGE_SIZE) # restore the dimention from 2D to 3D\n",
    "\n",
    "# Create validation set from the training set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, train_size=0.9, stratify=y_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b1fe80",
   "metadata": {},
   "source": [
    "### 1.3 Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccfef24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original training set is 60000 images with (28, 28) pixels, without normalization (uint8).\n",
      "The original test set is 10000 images with (28, 28) pixels, without normalization (uint8).\n",
      "\n",
      "The size of training set is 54000 (float64), the size of validation set is 6000 (float64), andthe size of test set is 10000 (float64)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The original training set is {X_train_original.shape[0]} images with {X_train_original[0].shape} pixels, \\\n",
    "without normalization ({X_train_original.dtype}).\")\n",
    "print(f\"The original test set is {X_test_original.shape[0]} images with {X_test_original[0].shape} pixels, \\\n",
    "without normalization ({X_test_original.dtype}).\\n\")\n",
    "\n",
    "print(f\"The size of training set is {X_train.shape[0]} ({X_train.dtype}), \\\n",
    "the size of validation set is {X_valid.shape[0]} ({X_valid.dtype}), and\\\n",
    "the size of test set is {X_test.shape[0]} ({X_test.dtype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c836c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 different classes: [0 1 2 3 4 5 6 7 8 9]\n",
      "The label distribution of training set is [5400, 5400, 5400, 5400, 5400, 5400, 5400, 5400, 5400, 5400]\n",
      "The label distribution of validation set is [600, 600, 600, 600, 600, 600, 600, 600, 600, 600]\n",
      "The label distribution of test set is [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n"
     ]
    }
   ],
   "source": [
    "def show_distribution(y):\n",
    "    \"\"\"Simple way to show a label distribution.\"\"\"\n",
    "    result = []\n",
    "    for i in range(len(class_names)):\n",
    "        result.append((y == i).sum())\n",
    "    return result\n",
    "\n",
    "print(f\"There are {len(set(y_train_original))} different classes: {np.unique(y_train_original)}\")\n",
    "print(f\"The label distribution of training set is {show_distribution(y_train)}\")\n",
    "print(f\"The label distribution of validation set is {show_distribution(y_valid)}\")\n",
    "print(f\"The label distribution of test set is {show_distribution(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44772aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAACiCAYAAAAZQyvMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABH3UlEQVR4nO3deZgdVbU28Hc5MJMAGYgJGYCEySSECIEgkCAoEmZFBgEBkQsiIlf0w+uVQVTgwr0KgqAiKoKAgEwqAQxhCJAEIoQAgUDIPCdkYh5kf39U9c67V05VTjo9nO7z/p4nT/bpqlOnuqp2VZ3qvdayEAJERERERERERKT1fay1V0BERERERERERDJ6UCMiIiIiIiIiUiP0oEZEREREREREpEboQY2IiIiIiIiISI3QgxoRERERERERkRqhBzUiIiIiIiIiIjVCD2pERETWkZmdZGah4N/y1l6/dWFmffLf46QW+ryGbdmniZZ3uJl9tymWVbD8PmZ2oZlt01yfUfC5J5nZ11vyM0VERKRlfKK1V0BERKQd+QqAOe5nH7bGirRh/wAwFMD8Jlre4QD2B/DzJlqe1wfABQAeBzCtmT6jkpOQ3cf9vgU/U0RERFqAHtSIiIg0nYkhhKmtvRJtWQhhMYDFrb0eIiIiIq1FoU8iIiItwMw+ZmaPmNkMM+tIPx9gZu+Y2eX0s2PMbLSZLTazN83sWTM7scIyg5n91MzOMbOZZva2mf3DzLrm/24zsxVmNtvMznXvbQgx2sfM7s4/53Uz+5WZbVjF7zPMzB4yszfM7C0ze8DM+rt5DjCzJ/N1eNPMppjZ+WtY7mqhT/k2uynfLi/lnzfBzPZaw7L+COBEAD0oFG0GTe9iZr82s7lm9p6ZvWxm/+GW0c3MbjCzefk8883s7/n2HQ7g4XzWf9JnDC9ZpzVuEzPb2czuNbNl+bHxhJntTdMfATAMwGfpMx8p2xYiIiLSdmhEjYiISNP5uJn5a+tHIYSPQggfmdnxAJ4D8BsAx+QPRG4F8CKA/6b3bAPgDgCXAvgIwD4AfmdmG4YQfu2WfwKAFwCcAWBLAFcA+BOATQGMBPBbZCFZl5rZ8yGE+9z7bwJwG4BrAAwBcD6AjZGF1lRkZgcBuAdZmNLx+Y/PBTDGzAaGEGbnOVvuzX+PiwC8D6Bf/rs1xt4AtgdwHoB3AfwEwN/NrE8IYXnBe34CoAuA3QAcmv/svfx36IAsXGlDABcCmA7gAADXmtn6IYSr8vlvBNAbwPcBzEa2jfcDsBGAZwB8C8CvAJwF4On8PZMrrUw128TMBgMYA+BZAKcCeBvA6QBGmdmeIYR/IdvXNwH4OIDT8reuLNgGIiIi0sboQY2IiEjTebnCz/4B4GAACCHMMbNvALjTzB5AloulF4DBIYT3G94QQri4oW1mHwPwCIBPAfgmAP+g5j0Ah4UQPszn7w/gPwGcF0L4af6zRwAcgeyBjX9Qc18I4Xt5+0EzCwAuMrOLQwivFPyeVwJ4NIRwGK3nw8hytJwD4GwAgwGsB+CbIYSGhwijC5ZXjQ4ABoUQluWftwDZg5ERAG6u9IYQwmtmthjA+yGEcW7yd5A9gBkQQng1/9koM9sMwAVmdm2+TYcC+GEI4c/03tsbGmbW8FDmpQqf4VWzTS4HMAvA5xqOifxYeQHZQ6rDQwiTzWwlgE9U8ZkiIiLSxij0SUREpOkcgWz0Bv87m2cIIdyFbETNtchGTJxFDwoAAGbWz8xuMbO5AD7I/30D2YgS758ND2lyDQ+LHqDP/BDAVAA9K7z/Nvf6VmT3B0Mq/YJm1g/AtgD+bGafaPiHbOTHWGSjfwBgYr7et5rZkWbWtdLy1sLYhoc0uefz/3s1cnlfBDAewHT3ezwAoBOAnfL5ngbwfTP7jmVhatbIzwPWsE3yEVbDkD0I+ojWyQCMwqptKyIiIu2YHtSIiIg0nRdCCBPcv0rJhW8AsD6ARXCjQcxsEwD/BLAzgB8gC/nZDVl1n/UrLGuZe/1+yc83qPD+hQWve1SYFwAaHi5cj1UPkRr+HYzsIQfy3/sAZPcaNwJYYGbjzGxYwXLXZCm/CCG8lzcr/U7V6IrswYf/HRpGy3TK/z8aWbjS/wMwCcBcMzs/H+m0VqrYJlsgC2c6r8J6nQlg88Z8roiIiLQtCn0SERFpQWa2EbKHLi8gy09yKbJQpQZDkYXk7B1CeJze11zX7C2R5cjh1wAwt2D+1/P//wvZKA+PQ7geBvCwma0P4LPI8rL8I88rs2Sd1nrdvY7sQdl3CqZPAYAQwiJkeWi+ZWbbI0tO/GNklamuXdsPLdsmAJYjy0n0K2R5hiq9/6O1/UwRERFpW/SgRkREpGVdiWy0yiBkI1CuMLP7QwgNoUob5f9/0PAGM9scwGFoHkchzZNyDLKHBeML5p8CYAaAT4cQLq3mA/LRL6Pz0UL3ANgaQEs9qHkPWcJg734A3wYwK38Ys0YhhCkAfmhmpwNoqHDVMLJnjZWy3LJW2yYhhKfNbAyy0VTPrOGhzHvIEkaLiIhIO6MHNSIiIk1nkJl1rvDzCSGED83sy8hyzZwQQpgG4Jdm9gUAN+TVkhYBeBJZBZ9fmdkFyCow/QjZg42OFZa9rkZYVhr8QWR5aS4A8CefN6dBCCGY2bcA3GNm6yHLcbME2UicPZE9+Ph5/jBjH2TJi2cD6IxsFM48ZKOJWspkAFuY2TcBTADwbgjheQC/QBbWNMbMfoHsAdTGAHZANprpMMvKqI8C8GdkuX8+QPbAbHNk2wsAXgHwIYCvm9lSZA9QpoQQ3vArUuU2+S6AxwA8YGbXA5ifzzcYwMdDCD+g3+sMMzsawGsA3sgfJImIiEgbpwc1IiIiTef2gp93yRPFXgfgzyGEm2jaychyn/zRzA4KISw2syMA/B+yMs7zkI3C2QLZQ5SmdjyySk3fRBa2dB2A75W9IYRwn5ntg6yk+O+QjSZZAGAcgL/ksz0H4EAAlyDLB7MUWTns40II7zT9r1HodwD2AHAxgM0AzATQJ4Swwsz2RFaO/Fxko5yWI3tg89f8ve8iK8F9KrJwtI/y6ceFEO4BgBDC62Z2Zr6MR5HlmNkXWaUub43bJITwjJnthmxf/xLZw7nF+Xpwxa//QZZc+ncANsk/e3hjNpCIiIjUFgshtPY6iIiISAszs5MA/AFAv4KExyIiIiLSClQ5QERERERERESkRuhBjYiIiIiIiIhIjVDok4iIiIiIiIhIjdCIGhERERERERGRGqEHNSIiIiIiIiIiNUIPakREREREREREaoQe1IiIiIiIiIiI1Ag9qBERERERERERqRF6UCMiIiIiIiIiUiP0oEZEREREREREpEboQY2IiIiIiIiISI3QgxoRERERERERkRqhBzUiIiIiIiIiIjVCD2pERERERERERGqEHtSIiIiIiIiIiNQIPagREREREREREakRelAjIiIiIiIiIlIj9KBGRERERERERKRG6EGNiIiIiIiIiEiN0IMaEREREREREZEaoQc1IiIiIiIiIiI1Qg9qRERERERERERqhB7UiIiIiIiIiIjUCD2oERERERERERGpEW3mQY2ZzTCz/Qum7W1mU1p6nUTqnZmdZGaP0+tgZn1bc51EROpVtedgM+uTz/uJlliveuWvkRWmjzSzE1tynaT5lX1nEZHKyq5fjf1+saZzcK1r9gc1ZvYm/fvIzN6h18c1xWeEEMaEELZfw3pUPGma2bFmdrNuWtZNS+xnaV55H2nYbwvN7I9mtklrr5c0Ddq/b5jZcjN70sxON7M288BeVmdmXzWzCXm/nZ9/8dtrHZf5iJl9o6nWUQAz2yvvcyvMbKmZPWFmu7X2eknTaOz+DSEcGEK4oWS5bfpLRi1Q36tP7p52mZn9w8x6tvZ61YP8HmKZma3f2uvSXMxsuJnNae7PafYb9BDCJg3/AMwCcAj97M/N/flVPHg5CMB9zb0e7V21+7kWHoTVwjrUsEPyfTgYwK4AftTK61NK+3KtHRJC2BRAbwCXAjgXwPWVZjSzj7fkisnaM7PvArgCwMUAtgTQC8A1AA5rxdUSx8w6APg7gKsAbAGgB4AfA3ivNddLmkZz7V9d39ZdW+572v9NouGe9lMAFiI7DqQZmVkfAHsDCAAObd21aftq6i+pZtbZzP6e/7V3qZmNcX/tHWRmk/Kn4n8xsw3y9yVPtfKnqOea2SQAb5nZLchuYP+WP1n9f/l8HwPweQD3A3gsf/vyfJ6hZvYxM/uRmc00s0Vm9icz65i/t2EEzn+Y2bz8L5nfa/6t1LY07Jt8fywA8AczW9/Mrsi327y8vX4+/2p/PTIa7mZmI8xscj4qYC5vczM72Mwm0miBgTTNHxO6AJYIIcwFMBJAf3Mjzar9a7uZdcz7zOK8D/0o71Pr5/uoP83bJf/LR9f8tfZlMwohrAgh3AvgaAAnmll/y0ZQXWtm95nZWwD2NbPuZvbXfB9ON7OzGpZhZkMsG8mx0rIRWD/Pf76Bmd1kZq/n++9pM9uylX7Vdiu/Fl0E4FshhDtDCG+FED4IIfwthPD9NZxnN8+vtYst+6vX381sq3zaz5DdZF2dXwuvbr3fst3YDgBCCLeEEP4dQngnhPBgCGGSmW1rZqPz/rLEzP5sZps1vDE/333PKtz75NO/n99/zDOzr/OHmtlBZvZs3kdnm9mFLfUL15nC/dswg5n9b97XppvZgfTzeD3N73+eMLNfmNnrAP4C4NcAhuZ9cXnL/lrtQlnfO8nMHi/ZNx3N7Pq8f801s59a/geMNfVbZmY75ss+Nn+t+5sWFkJ4F8AdAHYC1nxuNLOvWXbf+rqZnWcKZVsbXwMwDsAfASRhnfl95q8sG930hpmNN7NtKy3EspFws81seIVp6+f9dlZ+//lrM9uwZJ3MzK7Or6Evm9l+NKG7md1r2XOHqWZ2qvuc1e6jzGxjZN+Rutuq6JHua7GNqlZTD2oAnANgDoAuyP46+ENkT+QaHAXgiwC2BjAQwEklyzoW2WiZzUIIxyId5XFZPs8QANNCCEsA7JP/bLN8nrH58k8CsC+AbQBsAsDftO4LoB+ALwA4Vx25om7I/pLRG8B/APhvAHsAGARgZ2T7odqRG9cDOC0fFdAfwGgAMLNdAPwewGkAOgH4DYB7LR12x8fEh+v2K7Vvlg0PHQFg2Tos5ioAHZH1nWHITt4nhxDeA3Ansv3R4CgAj4YQFmlftpwQwlPIzrl75z/6KoCfAdgUwJMA/gbgOWR/hdwPwNlmdkA+75UArgwhdACwLYDb8p+fiGy/90S2/04H8E6z/zL1ZyiADQDcVTC97Dz7MQB/QHZO7oVs/1wNACGE/wYwBsCZ+bXwzGZa/3ryCoB/m9kNZnagmW1O0wzAJQC6A9gRWb+50L2/4r2PmX0RwPeQ/cGpHwB///EWsvPuZsjOl980s8Ob6HeSVcr2LwDsDmAKgM4ALgNwvZlZwbJ2BzAN2T3w8cjOn2PzvrhZs6x9+7Yu++aPAD4E0BfALsju8xv+SFVNv4WZDQbwAIBvhxBu0f1N6zCzjZD9YWpc/qPCc6OZ7YRsZOpxyEbidER2DyTV+RqAP+f/DrDV/1B3DLJRbZsDmIrsnjORX9tuAfDlEMIjFT7jUmQPYQch6589AJxfsk67A3gNWT+/AMCdZrZFPu1WZPfB3QEcCeBiM/tcPq3ifVQI4S0ABwKYR9Ej80o+v9Fq7UHNB8g6Re/8L4NjQgj8oOaXIYR5IYSlyL5ADCpZ1i9DCLNDCGVfENYU9nQcgJ+HEKaFEN4E8F8AjnFPuH+c/yXzeWQ3vsdWWlCd+wjABSGE9/L9cRyAi0IIi0IIi5F12BOqXNYHAHYysw4hhGUhhGfyn/8HgN+EEMbnfzW5AdnQ1j3ovdUcE/Xu7vyvdo8DeBRZSMVay//qdAyA/wohvBFCmAHg/7BqP9+cT2/w1fxngPZlS5uH7EEqANwTQngihPARgAEAuoQQLgohvB9CmAbgOqzabx8A6GtmnUMIb4YQxtHPOwHom++/f4UQVrbg71MvOgFYUnIjX3ieDSG8HkL4awjh7RDCG8hulIa1yFrXofz43wvZH56uA7A4/wveliGEqSGEf+bXx8UAfo7V90XRvc9RAP4QQnghv3G80H3uIyGE50MIH+WjO26psGxZR2X7N59lZgjhuhDCvwHcgOw+t2iU4bwQwlUhhA91fVt3jd03+fQRAM7O7/EXAfgF8utflf12bwD3AvhaCOHv+c90f9OyGu5pVyB7oH05sMZz45EA/hZCeDyE8D6yBwBh9UWLZ1l+vN4Abgsh/AvZw5GvutnuCiE8ld+7/Bmrf5f/CrIHmAfmf0z0n2HI+tF/hhCW5vcwFyP9TuEtAnBF/mzhL8gezh6U/1H6swDODSG8G0KYCOB3yB42Aev2fbVJtNqDGjPrRcOF3sx/fDmyp2sPmtk0M/uBe9sCar+NbIRLkdlVrMYIlD+o6Q5gJr2eCeATSC+ws930Zhn61MYtDtmwwwaVtmu12+3LyPbbTDN71MyG5j/vDeCcfCjp8vzE3NMtt5pjot4dHkLYLITQO4RwBho/EqIzgE9i9f3c8FeJhwFsZGa7WxbPOgirRgZoX7asHgCW5m3err2RDevk/fBDrDr/nYLsLxovWxbedHD+8xuR/QXx1nyo6GVm9slm/y3qz+sAOpcMjS88z5rZRmb2m3xo90pkob+bmfISNZsQwkshhJNCCFshGw3aHcAVZralmd1qWWjFSgA3ITt/sqJ7n+5Y/R4kys+vD1sW4rYC2egMv2xpAkX7N5+8gOZ7O28W3b/q2tbEGrlveiO7h5lP17/fAGgIz66m354O4Ek3IkD3Ny3r8JCNRNsAwJkAHjWzbms4Nybn1fy4eL2F17utOhHAgyGLVAGyP8D6qnZr+i5/NrIHPS8UfEYXABsB+Bf1ofvznxeZ6wZ+NNwPdQfQ8LCHpzV8V1mX76tNotUe1IQQZoU0AS3yv7yfE0LYBlkCou9yHNnafkTZazPrhuzJ+TMF8wPZX5p70+teyIZBLqSf9XTTm2XoUxvnt22l7dqw3d5C1gEBxP20akEhPB1COAzZxfJurAq3mA3gZ/lDhoZ/G4UQbilZD1mzt/L/N6Kfdas0o7ME2cgKv5/nAkD+16vbkI1AOxbA3+lEqX3ZQiyrfNED2QgqIN2uswFMd/th0xDCCAAIIbwasrDSrgD+B8AdZrZx/heLH4cQdgKwJ4CDseqvE9J0xiL7S+zhBdPLzrPnANgewO4hC11rCP1tGPKv/tWMQggvIwur6I/sL4EBwIB8XxyPVfthTeZj9XsQdjOyv+j3DCF0RJbvpNplSyO5/bvWb1/Da1kHa7FvZiM7v3am61+HEMKn8+nV9NvTAfQys1+45er+poXlo5fuBPBvZCOsys6N8wFs1fBey3KfdGrZNW578u10FIBhZrbAsryk/wlgZzPbeS0W9RUAh5vZdwqmL0H2R+RPUx/q2PAsoUAPF27acD80D8AWZrapmzY3b5fdR7VIP62p0CfLEmz1zTfmCmQd6qMmWvxCZLkyGhwI4H56wrY4/yye5xYA/2lmW1tWpvhiAH8J6VDz8/K/Tn4awMnIkr9JuVsA/MiyBLKdkQ0rvCmf9hyAT5vZIMsSJl7Y8CYzW8/MjjOzjiGEDwCsxKrj4zoAp+dPyc3MNrYsWRh3PllL+VC/uQCON7OPW5assmLiL/e+hgcxPzOzTc2sN4DvYtV+BrIL5dHIhhbeTD/XvmxmZtYhHwFzK4CbQha66T0F4A3LEhtumO///vnDHZjZ8WbWJWRhUsvz93xkZvua2YB8dMZKZA/smuo8LrkQwgpk585fmdnh+XXok5blYbgM5efZTZHd6Cy3LE77Ard4f72UdWBmO5jZObYqYXNPZA+oxyHbF28CWGFmPQB8fy0WfRuAk8xsJ8tyMPj9uCmyvxa+a2ZDsPoQdGkCa9i/62ohgK3MbL0mWFbdaey+CSHMB/AggP/Lr5cfsyyBcEN4TDX99g1kuaX2MbNL85/p/qYV5Nv6MGR5UV5C+bnxDgCHmNmeeb+7EHrAXY3DkX1v3wnZKPlByPI3jcHa/bFuHrKciN8xs2/6ifk953UAfmGrCpD0sFX5EyvpCuCs/B7pK/l63RdCmI0sH+MllhXCGIhstHjDvVLZfdRCAJ0sLzLUXGrqQQ2yZHijkJ38xgK4JoTwcBMt+xJkG3u5ZZWCkvw0+dC2nwF4Ip9nD2QJv25ENix8OoB3AXzbLfdRZOFaDwH43xDCg020vu3ZTwFMADAJwPPIRjX9FABCCK8gq2QyCsCrWPWX/gYnAJhh2VDT05F9yUcIYQKAU5ElxFyGbJ+c1My/R704FdlNyOsAPo3spFaNbyMbkTMN2X68GVmfAgCEEMbn07sjy57e8HPty+bzNzN7A9lf9f4bWVz9yZVmzB+2HYzsYjsd2V8xfocssR6Q3YC+aFno6pUAjslj6rshu9FZieyG6FFk51FpYiGE/0P2APRHyP7YMBvZ8O67UXKeRTbsf0Nk+3QcsmHD7EoAR1pWCeWXzfpL1Ic3kCUzHG9ZRbVxAF5ANrLpxwAGI/vj1D+QJVqvSghhJLJ9ORrZeXK0m+UMABflff58rBqBKk2rbP+uq9EAXgSwwMyWrGlmWc267JuvAVgPwGRk9yJ3IBuJD1TZb0MIy5HlRjnQzH6i+5sW97f8HmUlsu94J4YQXkTJuTGf/m1kf8iaj+w76SK0gZLurexEZDnTZoUQFjT8Q3asH2drUcEshDAL2cOaH1jlKrPnIus74/Lvg6OQjRIuMh7ZM4YlyI6DI0MIDeFsxwLog+wB0V3IcqqOyqeVfV99GdmDnGn5c4NmCYmyNGSrPuQHywIA24RGJrm0LK/GdACfDMrKLiIiIiIi0m7kERXLAfQLIUxv5dWROlNrI2payhYAzmvsQxoRERERERFpX8zskDyceGMA/4tsNMWM1l0rqUd1+aAmL7N1bWuvh4iIiIiIiNSMw7Aq2Ww/ZKHd9ReCIq2uLkOfRERERERERERqUV2OqBERERERERERqUV6UCMiIiIiIiIiUiPWVCqr1eKiJkyYENvf+973YvuQQw5J5jvmmGNiu3v34spYs2bNiu0rrrgimTZ16tTYvuqqq2K7T58+Va9vM7AmXFaL7UcfSme26td46KGHYvuXv0wrvg4aNCi2FyxYENt9+/ZN5nvzzTdje9myZcm0T3xi1eE8ffqqxOx33XVXNaveXJpqP9ZEjOIbb7wR20899VQybb/99lvr5T3zzDPJ60022SS2t9tuu7VeXjNpk32xzL///e/YnjFjRjJt2223XetlfPzjH0+mPf/887Hdv3//2ObzQStoc32x7HzK/Llw8803j+3XXnsttpcsSav78n5bf/31k2kDBgxYu5VtGe2uL9apFu2LfK4C0uO+LPy/MeersWPHJq/ffvvt2H7//fcL14m9915aBbhLly6xvc8++6z1OjUT9cX2oc1dF2U16ovtQ8X9qBE1IiIiIiIiIiI1Qg9qRERERERERERqxJqqPjXpEKgPPvggeX311VfH9v33359MmzlzZmxzSNPLL7+czPfuu+/GNg/35jAYAJg/f35s77jjjsm0jTbaKLY5DGCPPfZI5jv88MNj++ijj0Yza5ND2T766KPk9cc+tupZ4F577RXbTzzxRFXL69ChQ/KahxF/+OGHybQNN9wwtt95553Y/tvf/pbMd/DBB1f12U2kTQwr5X7kQwNvueWW2OYQi8WLFyfz8fb3oRhFNthgg8LX3If9cO9TTz01tr/4xS9W9VnroE32xTK8v8eNG5dMGz58eMX3VBuGAwAPPPBAbB9wwAGNWMNm0Sb6YllIGe8DDlXy19aic+Fmm22WzMfv++QnP5lM4z522WWXVbPqLaHd9cU61aJ9sdrqpmXnNA77BYDRo0fHNofwjhw5Mplv++23r7h8DuMGgNdffz22O3XqlEzj8zWfH3wqgEMPPTS2e/XqVeG3aFLqi+1Dm7guSin1xfZBoU8iIiIiIiIiIrVMD2pERERERERERGqEHtSIiIiIiIiIiNSIZs9Rw7G7P/nJT5JpHJPrc8rwa47r9TlQOM6Xc834eHuO0/d5MTjXyXrrrVfx50BaWtHn4BgzZkxsd+zYEU2g3cUcbrrpprHt9w+Xn3zrrbdi25ew5H3nl8HHMpdcv/zyy5P5uNx7C6jJ+N9zzz03ef3b3/42tleuXJlM437FuS98n+U+xrkv/D7k3Bq8PCDdh1yilJftlzl06NBk2mOPPYYm1u76Ijv++OOT19w/Bg0aVNUy7rnnnuT1lVdeGducy6GV1WRfXBu33nprbPM5btKkScl8t99+e2x///vfj+1nn302mW/UqFGxvf/++yfTrrvuutjeaqutYtv3e+6zLVB+vV33xTrSqn2x2mOWr4tTpkxJpvG96A477BDbPvfMxIkTY5vvX/g+BwA22WST2OZ7JQDYeOONY5tzw/F7AGDatGkV3wMAl156aWxz3sd1oL7YPrT566KoL7YTylEjIiIiIiIiIlLL9KBGRERERERERKRGNHvoE4cl+FAVHrbJYQ5AcTlFP0yVQyB4GX4+LnPqS57yvFxO2q8vh2z4Uo38u9x7770V130ttbuhbLydO3funEzj4fQ8pJi3uV+GL0nL83KZ9bPOOiuZj8MyWkDNDCvlYdynnXZaMq1bt26xXdY/uF/67c99p+j9fj4fXlj0Ph9uwes4e/bsZNqIESNi25dmb6R21xd53/Xt2zeZxsPzBw4cGNsnn3xyMt9FF10U21w+FgD69+8f2zfddNO6rWzTqZm+2Fh33HFHbHOI4te//vVkPu4Dc+bMie3p06cn83GYxiuvvJJM69ev37qtbPNod32xTrVqee6icKdrrrkmeb106dLY9qXt+f6Q70N92BLfl955552xzddcoDjEGACGDBkS21z+m8+zAPD222/H9syZM5NpvI6///3v0QTUF9uHNn9dFPXFdkKhTyIiIiIiIiIitUwPakREREREREREaoQe1IiIiIiIiIiI1IhPrHmWtcflCDmO1+eZ4LwGvoSvz5NRhOfzZbcZxyT7zyrKreFLgXNJbo4nBtJY/8mTJ8f2TjvtVLhO9WDhwoUVf+6PhaKYcZ/DhI+nslwqHTp0iO1FixZVt7Lt3HnnnRfbvH2A8v6xYMGCisvzMfvc/3j/+nKl3O87deqUTOPP5mWU5bDacsstk2lcnnvJkiWx7fMi1TPuRz5XAve5l19+Oba/9a1vJfPx/t58882TaV26dGmS9WxPikoCv//++8l8zzzzTGwvX748mcZ9h8tzv/DCC8l89913X2xzP/3Upz6VzOfz0jAuR8z9z5f25XxHvi8WXVtFWlJZjhrOcebznW2zzTax7a9jjEth+3uebbfdtmL71VdfTebbYostYnv33XdPpvE1jfufzw3GOWp8nhu+jt94442xfcIJJyTzVVu6XEREmp/uokREREREREREaoQe1IiIiIiIiIiI1IhmCX3icsjTpk2L7T59+iTzrbfeerHtw1h4+KUP02A87JrbZSWBfdgNhzjxEG8/JL2oHKNfXy6hev755xeuez3wQ/Ib8L4HgHfeeSe2+Vgo21d+OHNRWW8Of6lnK1asiG1f9py3pQ91+uY3vxnbXNZ78ODByXw8/JtLAvtypb17945tP0yc14uX0aNHj8L53njjjWQaH0t8/lHoU2V+iPzcuXNjm/edD3XjfeCH4POxIJmiMAIOlQWAp59+Ora33377ZNp2220X24MGDYpt3mdAGqZx9913x/Yuu+ySzMfnRu43QLoPX3/99dj2IRt8LudrJKA+J7WhLASPQwj9fSiHgW6yySbJNL5X5PtBPx+HLx544IGx/fjjjyfz8XnYh3zzaw5Vfeutt5L5+Fro71/5fP3ss8/Gtg99UriT1KJrrrkmts8444xWXJPmUxaiKfVLI2pERERERERERGqEHtSIiIiIiIiIiNSIZgl9mj9/fmzz0EzOSA+sHv7CePgoh2z4rPxF8/kqMVylyQ8J5eHZXLXCD0Njfpg4L/+RRx6J7XoPfXruuedim/e3r9DFxwaHUfA+BdIqQX5YIO8v3v8Kw8jwNvHbv+xYv+SSS2K7Y8eOse2rovE+HD58eGw//PDDhcvecccdk9dcZWjlypWxfeWVVybzcQUrX2GIh6Hz8PIhQ4YUrkc985XpXnvttdj2oSyMp/nQJx+q1kBDe1fH1QQBoG/fvrHtK83wsc79w1dP42vahAkTYvupp55K5uvfv39sL168OJnGYRRc1ct/FoeV+OuiSK178cUXY9tfF/ma6c9dfM/H10IfPsX9lKuufeELX0jm4/f5ZfA5oSxMmUOk/DmZ+fOASGvhY5ZTHfj7xqVLlxZO4/7Sr1+/2OZKasDqIf+1RvdDUolG1IiIiIiIiIiI1Ag9qBERERERERERqRF6UCMiIiIiIiIiUiOaJEfN6NGjk9dcjpBLa/tSyxyP5+N/OW6Y4wq5tC8A9OzZM7Y5JnfRokXJfJxbY+bMmck0LkE7fvz42Obyp0Cak8HnwOE4fc6bwzlaAGDnnXdGPeFSs7yNfL4iPjY4L40vAT1x4sTY5rwJQHqc8PL5GKk3Ph9TAx8L649n9rWvfS2277nnnsL5ONcGxxD7PE18Trj11luTaRyHzP306KOPTubjHDWckwZIjyU+Xuodn2N5//scTrz9it4DpPm8uHyzf5+sjnPP+PLynMfi3nvvTaYNGDAgtstyUPA1iM8BPocMX9N8P+L9zfk4uA2k51p/XhepdXPmzIltvjYB5ddFPv/xce9La3Mf4/ta7stAev3s3r17Mm3evHmxzffXCxcuTObj0t38WQCw9dZbxzbnmfL3CGW5I0XWlc9tyPcbPO1Pf/pTMh9fj1544YVkGt/DcJ/w1yO+LjJ/v1J231O0Tv77Lb/mPFV++ZzPz68fn0s+85nPJNNOOeWUwvWSxvP3Qfy9texY4GuFz4X06quvxjbnUKqWRtSIiIiIiIiIiNQIPagREREREREREakRTRL65Eul8ZBsLo/mSw7ykCI/TJOHeXHJxLJysR988EFsb7bZZsk0DpPxw5f22GOP2OYQKS7tCwBDhw6NbT/MjYeP8rD2UaNGJfPVW+jTSy+9FNu873jfA+k246H/48aNS+bjfeeHUPJrHjLoS/TVEx4yzfz2Lyury0PDy9x+++0Vf37CCSckrzfccMPY9sMMuX/Mnz8/touGrK4JDzmsd0XDNqdOnZq89sdGAx8GwCE7vmSzDy9d0zrUGw5f8NuVwxd8aAOX0Obh3v7aytdMDufw108egu33IV8L+dzqz7t87eNrMFA+HFiktfh+1YDvQ4C0nw4cODCZVhY2yLhvch/gZQNpP/KhGNyv+Lro+xQvwy+fcR+eNGlSMm3XXXctfF97x9vd74Oi6+LaeOyxx2J7n332WeflVeutt95KXvtw55ZUth1PPPHE2Pbh1Pydzt9H8L0Ih+6VhfiXhWdzfy6bz193GfdFf83s3LlzbM+dOze2/fWTr+P+Xl6hT5lq+yxvZwAYO3ZsbB944IGx3di+UXZ/c+edd8b2ueeeu9bL1ogaEREREREREZEaoQc1IiIiIiIiIiI1Qg9qRERERERERERqRJPkqPnJT36SvOayhZdffnls+3wjn/vc52Lbx8dz/C/nrbj++uuT+TgGsShPjF8nX36Qc6JMnjw5tjlXAJDGLfoYZ17mZZddFtsHHHAA6hmX2uZ4zrIcNV/60peqWrYvg+nLxjYoKlFdDzinRRnelj6PBcfG+lhbNmzYsIo/931g+vTpse3zB40cOTK2hw8fHts+txPnrPHrxMfZggULCtdXMj7HWK9evWKbj4Wyfe9jwV9++eUmWrv2ifNH+OsRx+JzbjUgjbHnaWWx2ZwzjvNDAWkMvC/3zXmr+Nzqr3187vC54bgsaZcuXSBSC6ZNmxbbfC3xfYBze/hz3NKlS2Obc0v4ZTA+h/r8Frz8RYsWFU7j5fucFlyO2Od143MH51jk6zFQ3zlqeDtXm0/trLPOSl7PmjUrtvfee+9k2kMPPRTbXC69Z8+eVa8jn299rkzG3718/sDRo0fHdmPz/5Upy+vit+uMGTNim0sX+xxL/Lv6nIr8mvuV72N8/eN19P2obLtWW7qbvy/69eB+yjlRfC4hvj77+68xY8bEtj/O6lVZ/iPeXgAwfvz42ObvOL4/V4vP2Q888EAybdNNN23UMhtoRI2IiIiIiIiISI3QgxoRERERERERkRrRJKFP3plnnlmx7cOWLr300tjmYYAAMGHChNjmIUt+6HaPHj0qTvND13ioGYfjAMB9990X2zw8lIfhAWlZxG984xvJtHPOOQeyOh4mX23Zs2OPPbZwGpdA46HHQFryjvG+rze+JF2DsqGpPoSMw4d4aKFfxpQpU2KbS9DxMHNvxx13TF5zyAwPIb7mmmuS+TiM0oeH8DFS9PvXOy5b7kNSisoMcrlmoHyYOJ8rZXU8VNuHPvEwaT/Ee8mSJbHN+80P+S0aku2vizw82w+t5uOA38fDsSstk/l5RWrB7NmzY7ssvIDNnDkzed2nT5/Y5j7sS3VzqAoPgfchxrx8vx5F/c9/Fl+r/f0Wfx63+brdXvH29OfGakOc+D5mt912i+2vfvWryXyDBw+ObR/ywikevv3tb8f23XffXdU6AMXn2xtvvDF5feutt8a2TwXB91nNEepWForLpbSBNPSJU1T40CfuYz6dAS+fw5jKymdzv/T9iN9Xdrzw5/prMF8//TJ4Hf1nM/49faoHvx3rFW8/3zeefvrp2H7ppZeSaXys8f3wEUcckczH3y98WGvv3r1jm8vJc8g3kD6naAyNqBERERERERERqRF6UCMiIiIiIiIiUiOaJPTJD3Pj1zwc7JRTTknm49AnP4yLQ6F42J4fps9Dw3h4ox8yzsPQ/PpWmw2/a9eusV0W6lSW7bve8H7gYb9lw/323XffwmlDhw6N7bFjxybT/NDABr6iWD0pqvrkj0vedn47clWAH/7wh4XzPfjgg7H93HPPxfaLL76YzMfDAn11IA6ZOvroo2N74sSJq/8SOT9MnIeZ+mz+knnmmWdi2w8jLtp+fqg+DwP1w37nzJnTJOvZXvG2832RQxt8haVly5bFNldY4mG3QLpP+ZrmP4uHk/t9yP2qQ4cOsf3oo48m8+2yyy6x7Yd4l4VYtne8T3z4J4fjcohn//79k/l++9vfxvYJJ5wQ2927d0/m47BEHwrKeJ+WVcjwqq100lZwlQ/eDnycA+m9ob9H5ffxtbDs2srv4WUD6b7xVUK43/P5wd/n8vr7az8fI/xZfK2udf58wsdi2TFadqzzuZJDx/i8BgBnn312bPN9ysCBA5P5OJTHV/HhUO9Ro0bFtu+zfJ91+OGHJ9P4Ovz444/Htg8PL6qcC6x7KMaa+Hsy3v5PPfVUMq1v376xzdcWv6/5O4PvO3yN47Y/Dvh9vE4+ZKasQm21YUvML4PTdPA6llWAnDRpUjKt6PtOPeDji/ed72933HFHbPuQfr4H43N72fMMP42/22y11Vax7ftztcdJEY2oERERERERERGpEXpQIyIiIiIiIiJSI/SgRkRERERERESkRjRJjprGxqWXxdhxTBeXC/Zlr4riEcvybPiybxyfxrFljY3Frve8NNXwuUN4PxaVBwbSkpgcnwsUH3e+rHA9KSqT7GNmedv5vsPb75JLLin8LJ6PS99Nnjy58D3dunVLXnP54WpL+/r4z6LylWUlGOvN+PHjY9sfC7ydivKN+Wn+XPmpT30qtqdOnRrbHI9ezzgvAl/fgHS7+usd9xe+jpXlZOD+UJbPyecgKro+c9w3AGy33Xax7XOn+HKWtaTsPqXs2l+Ug2706NHJfFdddVVsv/baa8k0jqXnvATbbrttMh/nKBo2bFhsX3311cl8nO/i3nvvTabtsccesV1trg6fK6E95KVhnPeQf1efW4BLZh922GGFyyjrR5wXg9tl90B+Gvcj/ix/jdxhhx1i+5577kmm8b7ndfR5bmpZ2XFYNm3MmDGF0y644ILY5twt119/fTIfnzs5B5vPucL8tuVzzkEHHRTb/h712muvje3f//73yTTOX8T3S7169Urm43yOfL0HgBUrVsQ2X6ubStm51d+T7rfffrHNx72/j+Pt74/7ohxR/pjgZXJ/KMt9VLaMovcA6e/ir7tFOXD8+ZmX6XMJtpUcNWXbtux+hNvV3rv/+te/Tl7z9xB/zPC5nc+v/B4g3c5+H2+88caxzd9buX8B6Xnf59HhZRTRiBoRERERERERkRqhBzUiIiIiIiIiIjWiSUKfvGqHyfKQ07L3lA09KgrZ8MPTeLiRL01XNITMh+Bw+FSZ9lbKsqmUDePzQ76LcAk0PxxO23p1ReW5Pe6Ln/vc55JpPGyYt78ffsjD+3jflPUb3/d42CEvzy+Dyxb60t1bbLFFxc/isplA9cdce8RlBf1wWz4Wiob3A+XnZR72y2WKFfqU4WPbh1vw9cOXr+fhu3x98kPsi8pB+p/zvvfDkIv67V133ZW8Puecc2Lbh8zw8VPLysrJenze4zL3V1xxRTLf9ttvH9tHH310Mu0zn/lMbPO57L777kvmGzt2bGxfd911se3LN/N584gjjkimbb311rH9gx/8ILYPPfTQZD6/79ozHurOpXKLwhoAYKeddkpe83Wx7BrHxxKHK/p+XxSa5NerLKyEwxDLwm7Khum3VRxi69Mb3HLLLbHtz6nnnXdebPP3BC7V7afxtc+HVPA51p9X+Ljja8BXvvKVZD7um1OmTEmmcRhlz549Y3v//fdP5uNwqr/85S/JtLL0Ak2hsffiZeE83D/K5uNpZedx3jd+P5WdB8rCwRlPK/veWlRiHkj7sO/P/hzR2op+p7JjoWz7FYUYe9y3fZ/dZZddYtsfM3yO4O8MnTp1Subj8EJ/P1N0HPr9+Pbbb8f2q6++mkwbNGhQxWUwjagREREREREREakRelAjIiIiIiIiIlIjmiX0qSlCf/xQtErLBqofesTDDH2WZR6izJ/rh4lXW81KITiV8XBCn/n605/+dFXLGDFiRGxfdtllybSiY6ae+SHADbjSGZCGA5500knJtJEjR8a2r1DDyoaSFikLp+E+64eifulLX4ptH/pUhIcwAvUd+sRhYD7kgfdBWTipD18sWgYP9dx9993Xel3bIz7uO3TokEzj496H63HIC8/nqyvxsOiyqhJlQ4q5r/O1z1dqmzt3bmwPHDgwmVbL52TeB42tAMchTBziBxSHYJY58cQTS183mD59evL6pz/9aWz78yEP1+aqfX4ZXP1l6dKlyTTe/2WhHTyfr1w0fPjw2N51113Rkvx9Iq9bUZU7IO1HvqJZ0f0gD3MHiu97/BD7snCBorA0f4/KoaV+PXhe/v39cH5+XW24/7ri8J7bbrstmda1a9fY9uc5vo/h38n3vX333Te2d9ttt2QaV23i86M/L/M5grel7yscfuHvszh8hc/fPqyFtzt/PwGAvfbaK7Y5fM6vx9133x3b/libNGlSbHNoZEvgUE8g3W+8r3n7AGlf9McBhy9yvyyrOMT8vQ2fL8oqEzUWr1dZVUae5iuDlYVntYai7eJ/J37tr7vVXpO5Etorr7wS2xwKCKTX5LKwMv7+4/ssr5P//sPHYbXPPR544IHktUKfRERERERERETaED2oERERERERERGpEXpQIyIiIiIiIiJSI1o1yI3jQavN/+I1JhdGWewjx6CtXLmycBllVJ67srJY8G222aaqZey8886x7XNkFOUr8jmJ6onPm9DAx0RzbLAvG8q4r/jtzcd6Wdm9ovcA6TFS1mfLcp3wZ3PpzFrOl9HSZs2aFds+Bt7nlmjg91VZbDXvg+eff77R69mecDwzH+e+VKq/7jDOO8HnNR+vzv20aH8C5SXWOS8G56GZN29eMt+cOXMKl1/LfY7Xm+PcgfQ+gPMfAOmxffbZZ8e2P0c9+eSTse1zhfGxUJQvBUjzZ3DuC58/Y4cddojtz3/+88m0fv36xfZWW20V25zDAkjLTfvy33w88T711wCe5n9nnxukJfn8ZEX3aH77l13vivJ3+fsSzjfCx5Xvl3xN5usxkOZr4HX3eRw4j07ZNZiPad/v+TjjnDfN6Te/+U1sP/fcc8m0slLSRTk8Fi9enMzHJcj9tuXzKOdteuGFF5L5+HzBx7a/lyo6z3v8e/lzDOdwevrpp5NpV199dWzzPvZ5Hsvux/w1vylUW7Z62LBhyevbb789tvm844/LsvyhfO4tK4vNynK3leW54T7Hx19ZPhw/jT+PzzH+WCrr681dYr2Ssut50e/rt2213w34PuPOO+9MpvF24uubz7fFx4X/LsTbndfd5/Zift15H/A0/52Tl//EE08ULr/wc9f6HSIiIiIiIiIi0iz0oEZEREREREREpEa0auhTY0pilpWDZH7IW1mYFQ9VbWyZTqmMh1pzaUo/hMyXvixSVpJOoU+r4yG6HAbkyxvyNnrppZcKl8fbvyykojFhgv593PbD6MuWz+cIXr4f/l5veLgw73/fF4vKbpeVqfRhH3we5aH09axoGLwPY/FhOIyHyPOwW79sHgLM+6IsRM1P4/Mpl6/ccsstk/m4/LpXFFZSC9dZDknx50MODfShXbzNBgwYENvXX3994Wf5/sFhZXzu9WEZRx11VGxzGV0upd1Yp512WvKaQ+7KytUyf/4uC2H3ZXlbkr9+8O/D10U/X69evWLbh4Px/Qz3ibJwMD7u/THHw/l9/yi67vpSstzvfT8tKi/t99miRYtiu6VCn4444ojY9qW1Z8+eHdvLli1LpvHvz6ESvpTxjBkzCqdxuBPvU98HuN/zMnzJXj4ncFlwIA2/4HCOBx98ENXi37ksTIPPK768uw8RaQpl5eX5ePP9iMM2jzvuuNj2ZcN5Gf6ayfh3rTb0yeM+5u+Hyn7PxuDzjw9nKrreA813DS27Tjc2pQHjsETulwAwZcqU2J4/f35s++OX9z+fs33YOO/HsntUXg9//ubrll+PovOoD2Xk+fi+A0hDLPv3749KNKJGRERERERERKRG6EGNiIiIiIiIiEiN0IMaEREREREREZEa0ao5ajhuqyx2vjF83C0v3y+7rIxa0TKkOhxzP23atNj2cYBleRmYjxFkRflryuJ427tqj1ku1/jaa68Vzsd9pSxnSVk/KnoPkB4XHJPrl+dzObCiHDW+ZGe9mTlzZsWf+xh7jtMvKssLlJew5LhrzvchGb4G+e3/zDPPFL6PY5/LclrwfiuLZedzZlmJUuZjrDmu3Csq5VwLOWo49nzEiBGttyI1oizvQ1vnrzN8DPN1xh/LnD/DH/dF9xs+XxTnSeD1KMtH4fPX8DJ5GUuXLk3m47wknCsFSHObbL755hWXBzRP/pI1GThwYGxvs802ybSyfExFedf4XhNI8+6MHDkymXbSSSdVXI9OnTol85XdezbGIYccEtv3339/Mm3nnXeObX9e5nMnH5P+fM3Xcc73AaT3xD4nUGMVlbwHys/3gwcPju2DDz44tp966qlkPr5++Ht6znvD06rNIePXr9rvpo39Tsh5b3h9fT4cPuZ8rr/muoaWLXfhwoWx7e8n+Xjjti85zjmh/H7kcyrvU7+dV6xYUXH5/pzMy/d5Y/j8y9vdn284741fXz6P8nnTn5e5n/r96OetRCNqRERERERERERqhB7UiIiIiIiIiIjUiFYNfeKhctUOL/PD+5q65GdRKUWguGytFBsyZEhsc9lnP+x34sSJ6/xZvvxa0WfVEx52XVbanEOfHn300cL5isq0AsX9uSyMsay8a1l/5rLv3AaKy3C3xpDuWvLyyy9X/LnfP3zM8NBbP19ZiVc+1ubOnbv2K9sO8Xbl/uGHZ5cNheXhwDy82F+beN/wPizrv76/8ZBiDqXyIQH8e3l8zFQbDinS1LgsMpCen/g45yH1QBqC4kNneUg892HfH/i+hMMcy8o/+3ABPl+UhTVymOm2226bTHvyyScrLp/Du4DVS9y2BC537T//oYceim1//uJtwaGMvswthyyceeaZyTQOteLzqL+P8KXQG/h9xa/Lros9evSIbX+POmbMmNjmYxBIj6+ikutA+jv7ex9/Dm8KTVGqmkvK++Ng2LBhsc3hMwAwZ86c2O7SpUtsl13TeJv7bcf9rSzku0xZGe+iMDq/vvzany9a4n521KhRyet58+YVrg+fH8u+lxeFNwHp78QhQn6b8zmVw4/8vuLl+X3MYaIcmsTnEaD6lAm8Hv5emY87f69W9r0sLq+qNRARERERERERkWanBzUiIiIiIiIiIjWiZkKf/NCwouoyZdnEy4bil1WrKQqz8p9Vz9WDGmufffaJ7T/84Q+x7Yf+Pfvss2u9bD+8rGhY/7pWEGvLONN5WSgRbyMfIsPDi/3wwcYoqmDhX5cNCZw6dWpsd+vWLZnGQyZ53eu9/1YbglTUj8qGAPvh/rwfi4aM1xse8srb0h/nfig94xABPs79OY6H/PIw4bKKh34f8jpyRSBeB6C8qhf/bqqaKK3FV1LjPsZtrmoCpMPZJ0yYkEzjMCbuV2UhomVhiDyfv85ymAa3/bnjueeei21fxauoYpy/LvLveeSRR6Kl9ezZs/Q14/sA/p1effXVZL7ly5fHtt8/XOmLz5V++/G9BJ83fTUwrqLkz6l8zeTzN4fr+M8qu+4uW7YMRTisxFd28mFxTc0fv2X3nhyWyMczbwMA+Ne//hXbu+22WzKNj5FJkybFtv8eyOEuZd8X+RjxfawoxL/sdyybxutRlqahT58+yetevXoVzrsuHnzwwdi+/vrrk2kcJumrIxVVafLf9cq2Oy+D75d8n+V7Sl6Gv3fifuqPSe5/fN6fPHlyMh+vR9n3Hz4PcFg6kFZB9ecL3/crqd9vsCIiIiIiIiIiNUYPakREREREREREaoQe1IiIiIiIiIiI1IhmyVHTmDKc1cbwVVuCu6xUXGPLhJaVNpXK9txzz9jmOD2/H7t27brWy/YxxEX7tZ5zI3B8bVl8JcdS+/LAHIvfmG25NmUb+bgo62/33HNPbPvYXc5HwLGtZfHc9YDjevm4KMtNwvvA7/uy/crbXefNTFlONjZ//vzY7tevXzKN31dWupNfl50Dyq6FRbm9dtxxx+R1Udl3/9n1fB6W1sW5KQDg3XffjW3O3eXzaXFpZM7/AqRlXMvyn3Ef4xwoZXk8fB4DzvPA8/nzyIwZM2L70EMPTaadcsopsX3UUUfFNl/fgdVzT9Syvn37VjXfgAEDmnlNhJV9T+PcIEB6vB133HGxzaW6gbRc+l133ZVM437KxwT3ByDNh8P5UHw+nLIcNTyt2jyrXrX3AkWfW2m9msqQIUNie9y4ccm0559/PrYff/zxwmXw9vQluDlfks+dxPnvODeMv0/h/cg5pvx5mEu8++3M5/OBAwfGtv8+8c9//jO2+fwNFN8j+X3TvXv32PbfWzl/VhGNqBERERERERERqRF6UCMiIiIiIiIiUiOaZexUtUO5/JB7VjQ0rNrh9n6+ohLcZfxwK19mTNasd+/esc1DvvwQMh6KPG3atNjeZpttCpfthysWHU9NUVK6reLjnrexx+ELft9wyUDexn54a1EYhf95tcM+y/opD2nlYYsAcMcdd1Rcftn5ph7wEP+y8p9l5dNZWUlaXiYfT34f+D7cnlV73HO566222qpwGWVhFFymkveF/9yyaUVlwn1YRllZ4WpDGUWa08knn1w4jUvZ870HkJYxvvPOO5NpXLqbl+HPpxwixeEbPLQfSK/Pvq/waz7X+pBxDlU47bTTkmmLFy+Obe7DHJIu0hQ4fBcA7r///tj2x/1rr70W2+eee25sc8lkIA1f9NdF/jwOi/GhNdyPOATL36PytbAs9GltwvpZUdiSv6fmkJlnn302mebv05sKn6/OP//8wvn4nAcA48ePj20OR3ryySeT+fjenUupA2lZ67L7Jd4HvI99iOP+++8f2yNGjEimVXve4xBSvjcDgE6dOsU2f7/14V68v30J9p122mmN66ARNSIiIiIiIiIiNUIPakREREREREREaoQe1IiIiIiIiIiI1Ijmqe9VpbIcFDyN47uKymEB1eciKVtGmXrKp9AcOKbSx2Bz3Gq1OWp8GUmOfeT48XrOUcPxkGX9jUtX+1w2vIyysotFOUvKyhb6aRyXym0u2wcAY8eOje3tttuucJ14+UU5N+oFlyrkfVpWopn3d1kctz+nFp1jfel3X4KzXpTlS+Lj1Jef5f3B+9D3o6ISvj4/QNF7yvhSx7y+vjwmX7vLPluktXC+Fp/vjMt1c+4LIM2NwP25W7duyXzcJ3gZZTkQ/fmT753KciXyZ02cODGZ5nM0iDQlzqHC+UoA4Itf/GJs+/LH7MQTT4ztOXPmJNP4/oXv7wFg+vTpsc35Frt06ZLM95nPfCa2zzzzzNjecMMNk/n4mlbtPeraKFqGvwbzecVfW7feeutGfXZT8bnq9ttvv4rtM844o8XWqTnce++9rb0KGlEjIiIiIiIiIlIr9KBGRERERERERKRGtGjoE5feAtKhW2XlRctClXhYWrVDt8uUlRBtbCm2elIWHnHEEUfE9s0335zMx2E5jz/+eGxzeTVvo402qmo9/DDJesLhery9fGm97373u7E9atSoZFq1fZFVG97k8XHAn7VixYpkvuHDh8f2wQcfnEz78Y9/HNt8TmiucoZtBe/zasM4y4b58rYtWx4P312+fHkyrZ5CnziksOxaxSGce+65ZzKNh3hzSVI/dJvPebz9/XWWr3c+HKsoPMt/FvdNv/yiMqQirYnPZXzN8f2S70XKjmW+tvoQ46lTp8Z2WSg3lwv2y+BzB4ce+r7Yo0eP2H7ssceSaRz6VFb6VqQxdtlll4rtteHv5VqL71cirUkjakREREREREREaoQe1IiIiIiIiIiI1Iiaqfrkh3py2ENjqi355ZWFW/DQz7L5fNZtWV1Z6NNhhx0W2zfccEMyH1cy+Otf/xrbF154YeFn+WH2ReE2XB2l3nC4YVmoCoc5+Ez5r776amzz0O2yKlLVKjteeB25KhUAdO3aNbY7d+5cuHw+j8ycObPR69keFA2f9/2IX/M+9mGhZdXAuMIPh7j6imL1hLffBhtsENt++/M22nXXXZNp3F/4nOmXwdW1uDqD77N8fuAKN0Dad3idBg8enMzHVW5mz56dTNt+++1ju6zSlUhLqjZkfsqUKbG92WabJdM4lJbDovg9QHr+4/Pu3LlzC5fn+ymHHxedA/xrDqXyyu55FQolIlI7NKJGRERERERERKRG6EGNiIiIiIiIiEiN0IMaEREREREREZEa0aI5anzsK8fA+xh7juFnPp6W4/451tjHHfvlF60Ht31OBh8PLKsryzV04IEHxrYvmc3x2dWWgO7fv3/y+vnnn49tPn64jG29+exnPxvbY8eOjW3fv7bbbrvYfuWVV5p/xZrQtGnTktebbrppbPNxNWTIkBZbp1r0xBNPxDZvI49LU3Lbn/84h1BZCXbOb+LzN+y8885rWu12oyiH1rx585L5OL/PkUce2fwrluvUqVNV8/m8OVz2ffTo0cm0AQMGxDbnwxGpFXxv6O8bOa8ZX0uA9JrJ79thhx2S+bbYYovYnjx5cmz7cybncPKlwPl83bFjx9jmc4VfR59TkadxfjHlqBERqV0aUSMiIiIiIiIiUiP0oEZEREREREREpEY0S+hTUYlr/3MeEu+HcPK8PMTeD03l+YpCmDwf0sSfzcvz8y1fvrxwmZIpK3XJevfunbweN25cbPOQ3SeffDKZb88994ztsrK2vE+XLFlS1Tq1RxzuwyU+fRhLteFmtciX/eUh3nwccGnUenT66afH9iWXXBLb/tzLZZo5bJCH8APpdvfHEw/V5/7sQx7rCYdRrFixIrb9deW8885rqVVqEt/5zndim0sRA2mJYA6LrefjQGpLWajPxRdfHNuXX355Mm3kyJGxzX3Y9wEOY+JrcNeuXZP5li1bFtsrV64snMZ9ypcM79y5c2yfeeaZyTQOd2Jt+dovItLe6QwtIiIiIiIiIlIj9KBGRERERERERKRG6EGNiIiIiIiIiEiNaJYcNUUxvz5HRLdu3WLb55nYZJNNYpvzJHCMr1eWX4bzVpR9Fpct9iVsu3fvXvjZkqm2tOOpp56avOaSlsccc0xsc04a74QTTkhec94H3qd77713VevUHvExu8suu8S2L89dlr+F+xLnICrKRdUc/GfxevTt2zeZdtBBB8U25w4YOnRo86xcG3HRRRfFNpdN5pKxQHqO5RK0gwYNSubj3DMbbbRRMo3LcB977LGNW+F2hvsYX4M6dOiQzDd8+PCqlsd9ojVL6n75y1+ObZ+ryOcRE6k1ZTlaNtxww9g+//zzC+ebNWtWbPvz6cKFC2Obc89wziaP8zL617169Yrtz372s8l8fN8jIiJtn0bUiIiIiIiIiIjUCD2oERERERERERGpEdaS4QsiIiIiIiIiIlJMI2pERERERERERGqEHtSIiIiIiIiIiNQIPagREREREREREakRelAjIiIiIiIiIlIj9KBGRERERERERKRG6EGNiIiIiIiIiEiN+P9+WR3X0E935AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_examples(X, y, title=\"\"):\n",
    "    \"\"\"Plot a grid of images from different classes.\"\"\"\n",
    "    # Size figure depending on the size of the grid\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.suptitle(title, fontsize=16,x=0.5,y=1.2,)\n",
    "\n",
    "    index = []\n",
    "    # search index\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(y)):\n",
    "            if i == y[j]:\n",
    "                index.append(j)\n",
    "                break\n",
    "\n",
    "    # Plot the image at appropriate place in grid\n",
    "    for i in range(len(index)):\n",
    "        plt.subplot(1, len(index), i + 1)\n",
    "        plt.imshow(X[index[i]], cmap=\"binary\")\n",
    "        plt.title(class_names[y[index[i]]])\n",
    "        plt.axis('off')\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "print(\"Some examples:\")\n",
    "\n",
    "plot_examples(X_train, y_train, \"Examples in training set\")\n",
    "plot_examples(X_valid, y_valid, \"Examples in validation set\")\n",
    "plot_examples(X_test, y_test, \"Examples in test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm design and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4636e21",
   "metadata": {},
   "source": [
    "First, A group of simple algorithms from the first 6 weeks are compared. They are: K-nearest neighbors, Naive Bayes, Decision tree, and Random forest. We simply train the model with default/simple parameters on the full training set, and test their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f11273",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=10) # k shoule be less than sqrt(#training_examples), commercial packages typically use k=10\n",
    "\n",
    "# Training and timer\n",
    "time_stamp = time.time()\n",
    "neigh.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "neigh_training_time = time.time() - time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0ec567",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "\n",
    "# Training and timer\n",
    "time_stamp = time.time()\n",
    "nb.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "nb_training_time = time.time() - time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a297035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running in arround 30s\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', random_state=0) # without setting max_depth results in overfitting.\n",
    "\n",
    "# Training and timer\n",
    "time_stamp = time.time()\n",
    "tree.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "tree_training_time = time.time() - time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4811836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running in arround 90s\n",
    "\n",
    "rnd = RandomForestClassifier(criterion='entropy', random_state=0) # n_estimators=100 by default\n",
    "\n",
    "# Training and timer\n",
    "time_stamp = time.time()\n",
    "rnd.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "rnd_training_time = time.time() - time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd9994d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of KNN is 0.8519, training time is 0.06 s.\n",
      "The accuracy of NB is 0.5838, training time is 0.65 s.\n",
      "The accuracy of DT is 0.8001, training time is 34.46 s.\n",
      "The accuracy of RF is 0.8760, training time is 85.67 s.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The accuracy of KNN is {neigh.score(X_test.reshape(X_test.shape[0], -1), y_test):.4f}, training time is {neigh_training_time:.2f} s.\")\n",
    "print(f\"The accuracy of NB is {nb.score(X_test.reshape(X_test.shape[0], -1), y_test):.4f}, training time is {nb_training_time:.2f} s.\")\n",
    "print(f\"The accuracy of DT is {tree.score(X_test.reshape(X_test.shape[0], -1), y_test):.4f}, training time is {tree_training_time:.2f} s.\")\n",
    "print(f\"The accuracy of RF is {rnd.score(X_test.reshape(X_test.shape[0], -1), y_test):.4f}, training time is {rnd_training_time:.2f} s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0cf2aa",
   "metadata": {},
   "source": [
    "Althouth Random Forest performs best, the training time is relatively long. Noticing that KNN is simple but with a content accuracy among them, the training time is  also tiny. Therefore, KNN is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fully connected neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6912a6",
   "metadata": {},
   "source": [
    "First, the numbers of layers need to be settled. Apart from the **input layer** and **output layer**, the numebr of hidden layer can be a variable. According to Cybenko(1998), any function (including discontinuous) can be approximated to arbitrary small error by a network with two hidden layers. To make the model small, we choose **two hidden layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba5c8d",
   "metadata": {},
   "source": [
    "Number of neurons in the input layer: 784   \n",
    "For numerical attributes, basically 1 neuron per attribute, in this dataset, we have 28 * 28 = 784 atttributes each example. Thus, the number of neurons of input layer should be 784. Simply, we just use **keras.layers.Flatten(input_shape)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3925d",
   "metadata": {},
   "source": [
    "Number of neurons in the output layer: 10   \n",
    "1 for each class. Therefore, the number of the output layers should be 10. The **softmax** function ($\\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}}$) converts the raw outputs of this layer into a probability distribution over the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c2c8c",
   "metadata": {},
   "source": [
    "Now we choose the hidden layers.   \n",
    "**Sigmoid** is the most widely used transfer function.\n",
    "We simply set most paras by default as well as the basic **SGD** learning algorithm. Since our labels are in index form rather than encoded as one-hot vectors, as we discussed earlier, we utilise the **sparse_categorical_crossentropy** loss. Then we observe the trend of the numbers of neurons with respect to MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8076c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_build_mlp(num1=50, num2=50):\n",
    "    \"\"\"Build the MLP model with the specified number of neurons.\"\"\"\n",
    "    # Define a test MLP model\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=IMAGE_SIZE),\n",
    "        keras.layers.Dense(num1, activation=\"sigmoid\"),\n",
    "        keras.layers.Dense(num2, activation=\"sigmoid\"),\n",
    "        keras.layers.Dense(len(class_names), activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    opt = keras.optimizers.SGD() # default learning_rate=0.01\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # model.summary()\n",
    "    return model\n",
    "\n",
    "def test_train_mlp(num1, num2, max_epochs=50, criterion=0.02):\n",
    "    \"\"\"Training the model.\n",
    "    max_epochs: the maximum number of epochs to terminate.\n",
    "    criterion: stop when the difference between the loss of the last 5 epoch is less than.\"\"\"\n",
    "    # Train the classifier.\n",
    "    mlp = test_build_mlp(num1, num2)\n",
    "    loss_list = []\n",
    "    for i in range(max_epochs):\n",
    "        history = mlp.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=1)\n",
    "        loss_list.append(history.history[\"loss\"][0])\n",
    "\n",
    "        # Stop condition\n",
    "        if len(loss_list) > 5 and loss_list[-6] - loss_list[-1] < criterion:\n",
    "            print(len(loss_list))\n",
    "            break\n",
    "    \n",
    "    return loss_list[-1], len(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6466b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.1037 - accuracy: 0.4263 - val_loss: 1.7923 - val_accuracy: 0.5368\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4612 - accuracy: 0.5885 - val_loss: 1.2179 - val_accuracy: 0.6322\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.0852 - accuracy: 0.6626 - val_loss: 0.9803 - val_accuracy: 0.6810\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.9097 - accuracy: 0.6979 - val_loss: 0.8525 - val_accuracy: 0.7093\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8078 - accuracy: 0.7238 - val_loss: 0.7724 - val_accuracy: 0.7292\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7400 - accuracy: 0.7426 - val_loss: 0.7161 - val_accuracy: 0.7463\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6904 - accuracy: 0.7559 - val_loss: 0.6740 - val_accuracy: 0.7567\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6524 - accuracy: 0.7656 - val_loss: 0.6415 - val_accuracy: 0.7682\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6226 - accuracy: 0.7758 - val_loss: 0.6155 - val_accuracy: 0.7745\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5982 - accuracy: 0.7848 - val_loss: 0.5937 - val_accuracy: 0.7838\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5777 - accuracy: 0.7934 - val_loss: 0.5750 - val_accuracy: 0.7907\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5600 - accuracy: 0.8008 - val_loss: 0.5588 - val_accuracy: 0.8003\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5446 - accuracy: 0.8065 - val_loss: 0.5445 - val_accuracy: 0.8043\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5310 - accuracy: 0.8121 - val_loss: 0.5319 - val_accuracy: 0.8095\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5190 - accuracy: 0.8168 - val_loss: 0.5207 - val_accuracy: 0.8133\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5083 - accuracy: 0.8212 - val_loss: 0.5107 - val_accuracy: 0.8158\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4987 - accuracy: 0.8246 - val_loss: 0.5017 - val_accuracy: 0.8193\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4900 - accuracy: 0.8276 - val_loss: 0.4936 - val_accuracy: 0.8227\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4822 - accuracy: 0.8307 - val_loss: 0.4861 - val_accuracy: 0.8263\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4751 - accuracy: 0.8329 - val_loss: 0.4793 - val_accuracy: 0.8293\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4686 - accuracy: 0.8352 - val_loss: 0.4730 - val_accuracy: 0.8315\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4625 - accuracy: 0.8376 - val_loss: 0.4671 - val_accuracy: 0.8327\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4569 - accuracy: 0.8395 - val_loss: 0.4616 - val_accuracy: 0.8343\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4516 - accuracy: 0.8410 - val_loss: 0.4564 - val_accuracy: 0.8355\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4467 - accuracy: 0.8428 - val_loss: 0.4516 - val_accuracy: 0.8372\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4421 - accuracy: 0.8443 - val_loss: 0.4470 - val_accuracy: 0.8395\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4377 - accuracy: 0.8459 - val_loss: 0.4427 - val_accuracy: 0.8413\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4336 - accuracy: 0.8472 - val_loss: 0.4387 - val_accuracy: 0.8423\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4297 - accuracy: 0.8483 - val_loss: 0.4349 - val_accuracy: 0.8437\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4259 - accuracy: 0.8497 - val_loss: 0.4312 - val_accuracy: 0.8455\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4224 - accuracy: 0.8507 - val_loss: 0.4278 - val_accuracy: 0.8457\n",
      "31\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 2.0525 - accuracy: 0.4094 - val_loss: 1.6879 - val_accuracy: 0.5487\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4032 - accuracy: 0.6255 - val_loss: 1.1924 - val_accuracy: 0.6540\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.0702 - accuracy: 0.6905 - val_loss: 0.9676 - val_accuracy: 0.7092\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8942 - accuracy: 0.7212 - val_loss: 0.8329 - val_accuracy: 0.7280\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7843 - accuracy: 0.7380 - val_loss: 0.7473 - val_accuracy: 0.7423\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7138 - accuracy: 0.7494 - val_loss: 0.6919 - val_accuracy: 0.7505\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6670 - accuracy: 0.7595 - val_loss: 0.6537 - val_accuracy: 0.7593\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6334 - accuracy: 0.7697 - val_loss: 0.6249 - val_accuracy: 0.7708\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6072 - accuracy: 0.7794 - val_loss: 0.6014 - val_accuracy: 0.7795\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.5854 - accuracy: 0.7897 - val_loss: 0.5815 - val_accuracy: 0.7882\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5669 - accuracy: 0.7983 - val_loss: 0.5643 - val_accuracy: 0.7962\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5508 - accuracy: 0.8051 - val_loss: 0.5493 - val_accuracy: 0.8040\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5367 - accuracy: 0.8115 - val_loss: 0.5360 - val_accuracy: 0.8073\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5242 - accuracy: 0.8164 - val_loss: 0.5242 - val_accuracy: 0.8132\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5130 - accuracy: 0.8199 - val_loss: 0.5136 - val_accuracy: 0.8167\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5030 - accuracy: 0.8237 - val_loss: 0.5041 - val_accuracy: 0.8195\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4941 - accuracy: 0.8261 - val_loss: 0.4955 - val_accuracy: 0.8225\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4860 - accuracy: 0.8288 - val_loss: 0.4877 - val_accuracy: 0.8243\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4788 - accuracy: 0.8318 - val_loss: 0.4807 - val_accuracy: 0.8248\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4722 - accuracy: 0.8342 - val_loss: 0.4743 - val_accuracy: 0.8267\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4662 - accuracy: 0.8360 - val_loss: 0.4685 - val_accuracy: 0.8305\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4607 - accuracy: 0.8381 - val_loss: 0.4631 - val_accuracy: 0.8320\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4557 - accuracy: 0.8396 - val_loss: 0.4582 - val_accuracy: 0.8337\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4510 - accuracy: 0.8414 - val_loss: 0.4536 - val_accuracy: 0.8347\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.4467 - accuracy: 0.8426 - val_loss: 0.4493 - val_accuracy: 0.8360\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4426 - accuracy: 0.8436 - val_loss: 0.4453 - val_accuracy: 0.8373\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.4388 - accuracy: 0.8456 - val_loss: 0.4415 - val_accuracy: 0.8390\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4352 - accuracy: 0.8466 - val_loss: 0.4379 - val_accuracy: 0.8408\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4317 - accuracy: 0.8477 - val_loss: 0.4345 - val_accuracy: 0.8420\n",
      "29\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 1.9774 - accuracy: 0.4624 - val_loss: 1.5652 - val_accuracy: 0.5695\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 1.2907 - accuracy: 0.6319 - val_loss: 1.0863 - val_accuracy: 0.6747\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.9647 - accuracy: 0.7042 - val_loss: 0.8735 - val_accuracy: 0.7193\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.8076 - accuracy: 0.7326 - val_loss: 0.7646 - val_accuracy: 0.7350\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.7240 - accuracy: 0.7460 - val_loss: 0.7040 - val_accuracy: 0.7445\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6750 - accuracy: 0.7562 - val_loss: 0.6655 - val_accuracy: 0.7527\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6419 - accuracy: 0.7662 - val_loss: 0.6372 - val_accuracy: 0.7658\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6166 - accuracy: 0.7755 - val_loss: 0.6144 - val_accuracy: 0.7755\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5958 - accuracy: 0.7845 - val_loss: 0.5951 - val_accuracy: 0.7845\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5780 - accuracy: 0.7930 - val_loss: 0.5783 - val_accuracy: 0.7913\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5624 - accuracy: 0.7986 - val_loss: 0.5633 - val_accuracy: 0.7958\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5485 - accuracy: 0.8039 - val_loss: 0.5500 - val_accuracy: 0.8003\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5362 - accuracy: 0.8089 - val_loss: 0.5381 - val_accuracy: 0.8058\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5250 - accuracy: 0.8131 - val_loss: 0.5273 - val_accuracy: 0.8090\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5150 - accuracy: 0.8171 - val_loss: 0.5177 - val_accuracy: 0.8130\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5060 - accuracy: 0.8199 - val_loss: 0.5089 - val_accuracy: 0.8163\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4979 - accuracy: 0.8227 - val_loss: 0.5010 - val_accuracy: 0.8192\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4905 - accuracy: 0.8251 - val_loss: 0.4938 - val_accuracy: 0.8213\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4837 - accuracy: 0.8272 - val_loss: 0.4872 - val_accuracy: 0.8238\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4775 - accuracy: 0.8294 - val_loss: 0.4811 - val_accuracy: 0.8257\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4718 - accuracy: 0.8317 - val_loss: 0.4754 - val_accuracy: 0.8275\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4664 - accuracy: 0.8336 - val_loss: 0.4701 - val_accuracy: 0.8300\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4614 - accuracy: 0.8355 - val_loss: 0.4652 - val_accuracy: 0.8325\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4567 - accuracy: 0.8366 - val_loss: 0.4605 - val_accuracy: 0.8347\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4523 - accuracy: 0.8381 - val_loss: 0.4561 - val_accuracy: 0.8363\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4481 - accuracy: 0.8400 - val_loss: 0.4519 - val_accuracy: 0.8377\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4441 - accuracy: 0.8415 - val_loss: 0.4480 - val_accuracy: 0.8383\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4403 - accuracy: 0.8432 - val_loss: 0.4442 - val_accuracy: 0.8412\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4367 - accuracy: 0.8441 - val_loss: 0.4407 - val_accuracy: 0.8435\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4333 - accuracy: 0.8455 - val_loss: 0.4373 - val_accuracy: 0.8457\n",
      "30\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 1.9281 - accuracy: 0.4664 - val_loss: 1.5100 - val_accuracy: 0.5860\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 1.2636 - accuracy: 0.6341 - val_loss: 1.0840 - val_accuracy: 0.6643\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.9792 - accuracy: 0.6924 - val_loss: 0.8971 - val_accuracy: 0.7053\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.8346 - accuracy: 0.7221 - val_loss: 0.7900 - val_accuracy: 0.7305\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.7471 - accuracy: 0.7397 - val_loss: 0.7224 - val_accuracy: 0.7432\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.6903 - accuracy: 0.7517 - val_loss: 0.6774 - val_accuracy: 0.7543\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.6514 - accuracy: 0.7622 - val_loss: 0.6451 - val_accuracy: 0.7633\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.6225 - accuracy: 0.7718 - val_loss: 0.6198 - val_accuracy: 0.7722\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.5992 - accuracy: 0.7809 - val_loss: 0.5984 - val_accuracy: 0.7807\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.5794 - accuracy: 0.7890 - val_loss: 0.5797 - val_accuracy: 0.7880\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.5621 - accuracy: 0.7961 - val_loss: 0.5631 - val_accuracy: 0.7952\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.5469 - accuracy: 0.8030 - val_loss: 0.5485 - val_accuracy: 0.7997\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.5335 - accuracy: 0.8083 - val_loss: 0.5356 - val_accuracy: 0.8060\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.5218 - accuracy: 0.8131 - val_loss: 0.5242 - val_accuracy: 0.8095\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.5115 - accuracy: 0.8176 - val_loss: 0.5142 - val_accuracy: 0.8142\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.5023 - accuracy: 0.8210 - val_loss: 0.5054 - val_accuracy: 0.8162\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.4942 - accuracy: 0.8243 - val_loss: 0.4975 - val_accuracy: 0.8218\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4868 - accuracy: 0.8270 - val_loss: 0.4903 - val_accuracy: 0.8247\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.4802 - accuracy: 0.8303 - val_loss: 0.4838 - val_accuracy: 0.8260\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.4741 - accuracy: 0.8329 - val_loss: 0.4779 - val_accuracy: 0.8285\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.4685 - accuracy: 0.8352 - val_loss: 0.4724 - val_accuracy: 0.8300\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.4633 - accuracy: 0.8370 - val_loss: 0.4673 - val_accuracy: 0.8307\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.4585 - accuracy: 0.8386 - val_loss: 0.4626 - val_accuracy: 0.8327\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.4540 - accuracy: 0.8397 - val_loss: 0.4582 - val_accuracy: 0.8337\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4497 - accuracy: 0.8409 - val_loss: 0.4540 - val_accuracy: 0.8357\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.4458 - accuracy: 0.8427 - val_loss: 0.4502 - val_accuracy: 0.8378\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.4420 - accuracy: 0.8441 - val_loss: 0.4465 - val_accuracy: 0.8387\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.4385 - accuracy: 0.8451 - val_loss: 0.4431 - val_accuracy: 0.8403\n",
      "28\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 1.9717 - accuracy: 0.5007 - val_loss: 1.5339 - val_accuracy: 0.6148\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 1.2558 - accuracy: 0.6619 - val_loss: 1.0629 - val_accuracy: 0.6808\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.9534 - accuracy: 0.7084 - val_loss: 0.8699 - val_accuracy: 0.7160\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.8055 - accuracy: 0.7320 - val_loss: 0.7640 - val_accuracy: 0.7345\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.7219 - accuracy: 0.7469 - val_loss: 0.7023 - val_accuracy: 0.7460\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.6712 - accuracy: 0.7587 - val_loss: 0.6618 - val_accuracy: 0.7590\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.6362 - accuracy: 0.7690 - val_loss: 0.6316 - val_accuracy: 0.7707\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.6090 - accuracy: 0.7785 - val_loss: 0.6068 - val_accuracy: 0.7777\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.5865 - accuracy: 0.7869 - val_loss: 0.5856 - val_accuracy: 0.7863\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.5671 - accuracy: 0.7949 - val_loss: 0.5672 - val_accuracy: 0.7945\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.5504 - accuracy: 0.8015 - val_loss: 0.5511 - val_accuracy: 0.7995\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.5360 - accuracy: 0.8078 - val_loss: 0.5372 - val_accuracy: 0.8050\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.5235 - accuracy: 0.8130 - val_loss: 0.5252 - val_accuracy: 0.8097\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.5128 - accuracy: 0.8164 - val_loss: 0.5147 - val_accuracy: 0.8145\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.5034 - accuracy: 0.8200 - val_loss: 0.5057 - val_accuracy: 0.8162\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4952 - accuracy: 0.8228 - val_loss: 0.4977 - val_accuracy: 0.8205\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4880 - accuracy: 0.8252 - val_loss: 0.4906 - val_accuracy: 0.8222\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4815 - accuracy: 0.8278 - val_loss: 0.4843 - val_accuracy: 0.8243\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4756 - accuracy: 0.8304 - val_loss: 0.4785 - val_accuracy: 0.8257\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4702 - accuracy: 0.8328 - val_loss: 0.4732 - val_accuracy: 0.8285\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4652 - accuracy: 0.8349 - val_loss: 0.4683 - val_accuracy: 0.8303\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4606 - accuracy: 0.8369 - val_loss: 0.4637 - val_accuracy: 0.8323\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4563 - accuracy: 0.8384 - val_loss: 0.4594 - val_accuracy: 0.8342\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4522 - accuracy: 0.8398 - val_loss: 0.4554 - val_accuracy: 0.8352\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4483 - accuracy: 0.8412 - val_loss: 0.4517 - val_accuracy: 0.8352\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4447 - accuracy: 0.8424 - val_loss: 0.4481 - val_accuracy: 0.8370\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4412 - accuracy: 0.8437 - val_loss: 0.4447 - val_accuracy: 0.8380\n",
      "27\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 1.9267 - accuracy: 0.4810 - val_loss: 1.4817 - val_accuracy: 0.6058\n",
      "1688/1688 [==============================] - 8s 5ms/step - loss: 1.2466 - accuracy: 0.6458 - val_loss: 1.0782 - val_accuracy: 0.6727\n",
      "1688/1688 [==============================] - 8s 5ms/step - loss: 0.9810 - accuracy: 0.6969 - val_loss: 0.9003 - val_accuracy: 0.7093\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.8370 - accuracy: 0.7252 - val_loss: 0.7881 - val_accuracy: 0.7323\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.7440 - accuracy: 0.7432 - val_loss: 0.7155 - val_accuracy: 0.7462\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.6840 - accuracy: 0.7561 - val_loss: 0.6677 - val_accuracy: 0.7572\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.6433 - accuracy: 0.7670 - val_loss: 0.6333 - val_accuracy: 0.7698\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.6130 - accuracy: 0.7774 - val_loss: 0.6062 - val_accuracy: 0.7783\n",
      "1688/1688 [==============================] - 8s 5ms/step - loss: 0.5888 - accuracy: 0.7870 - val_loss: 0.5836 - val_accuracy: 0.7867\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.5684 - accuracy: 0.7956 - val_loss: 0.5644 - val_accuracy: 0.7947\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.5510 - accuracy: 0.8035 - val_loss: 0.5478 - val_accuracy: 0.8005\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.5360 - accuracy: 0.8102 - val_loss: 0.5334 - val_accuracy: 0.8037\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.5231 - accuracy: 0.8154 - val_loss: 0.5209 - val_accuracy: 0.8102\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.5119 - accuracy: 0.8206 - val_loss: 0.5101 - val_accuracy: 0.8153\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.5022 - accuracy: 0.8239 - val_loss: 0.5007 - val_accuracy: 0.8188\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.4937 - accuracy: 0.8264 - val_loss: 0.4924 - val_accuracy: 0.8233\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.4862 - accuracy: 0.8293 - val_loss: 0.4850 - val_accuracy: 0.8250\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 0.4795 - accuracy: 0.8315 - val_loss: 0.4784 - val_accuracy: 0.8273\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.4735 - accuracy: 0.8337 - val_loss: 0.4724 - val_accuracy: 0.8297\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4680 - accuracy: 0.8355 - val_loss: 0.4670 - val_accuracy: 0.8318\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4629 - accuracy: 0.8375 - val_loss: 0.4620 - val_accuracy: 0.8328\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4583 - accuracy: 0.8391 - val_loss: 0.4573 - val_accuracy: 0.8345\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4539 - accuracy: 0.8408 - val_loss: 0.4531 - val_accuracy: 0.8363\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4499 - accuracy: 0.8422 - val_loss: 0.4491 - val_accuracy: 0.8370\n",
      "1688/1688 [==============================] - 26s 16ms/step - loss: 0.4461 - accuracy: 0.8431 - val_loss: 0.4453 - val_accuracy: 0.8378\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.4425 - accuracy: 0.8446 - val_loss: 0.4418 - val_accuracy: 0.8393\n",
      "1688/1688 [==============================] - 6s 3ms/step - loss: 0.4392 - accuracy: 0.8460 - val_loss: 0.4385 - val_accuracy: 0.8405\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Running in arround 960s\n",
    "\n",
    "# some possible numbers to choose\n",
    "hidden_layer_1 = [100, 200, 300, 400, 500, 600]\n",
    "\n",
    "# results\n",
    "loss_history_1 = []\n",
    "epoch_history_1 = []\n",
    "\n",
    "# for the first hidden layer\n",
    "for i in hidden_layer_1:\n",
    "    loss, epoch = test_train_mlp(i, 50)\n",
    "    loss_history_1.append(loss)\n",
    "    epoch_history_1.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "938aefdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The min loss is 0.4224 when neurons is equal to 100\n",
      "Loss history: [0.4224, 0.4317, 0.4333, 0.4385, 0.4412, 0.4392]\n",
      "Epoch history: [31, 29, 30, 28, 27, 27]\n"
     ]
    }
   ],
   "source": [
    "loss_history_1 = [round(i, 4) for i in loss_history_1]\n",
    "print(f\"The min loss is {min(loss_history_1)} when neurons is equal to {hidden_layer_1[loss_history_1.index(min(loss_history_1))]}\")\n",
    "\n",
    "print(f\"Loss history: {loss_history_1}\")\n",
    "print(f\"Epoch history: {epoch_history_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ede2ba3",
   "metadata": {},
   "source": [
    "It shows that when the number of neurons of the first layer is 100, the loss is minimal. Although the epochs is slightly larger, which may enhance the performance, the training time is much faster because of the lesser neurons. To keep the model small, we choose the 100 as the first number of neurons of the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5e83f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 3s 1ms/step - loss: 2.0871 - accuracy: 0.4057 - val_loss: 1.7920 - val_accuracy: 0.5297\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5618 - accuracy: 0.5764 - val_loss: 1.3705 - val_accuracy: 0.6178\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2350 - accuracy: 0.6409 - val_loss: 1.1207 - val_accuracy: 0.6568\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.0368 - accuracy: 0.6730 - val_loss: 0.9675 - val_accuracy: 0.6868\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.9106 - accuracy: 0.7007 - val_loss: 0.8658 - val_accuracy: 0.7157\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8246 - accuracy: 0.7223 - val_loss: 0.7942 - val_accuracy: 0.7325\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7629 - accuracy: 0.7376 - val_loss: 0.7417 - val_accuracy: 0.7425\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.7165 - accuracy: 0.7494 - val_loss: 0.7015 - val_accuracy: 0.7508\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6801 - accuracy: 0.7589 - val_loss: 0.6693 - val_accuracy: 0.7598\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6504 - accuracy: 0.7681 - val_loss: 0.6425 - val_accuracy: 0.7675\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6252 - accuracy: 0.7768 - val_loss: 0.6195 - val_accuracy: 0.7765\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6031 - accuracy: 0.7850 - val_loss: 0.5991 - val_accuracy: 0.7842\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.5836 - accuracy: 0.7930 - val_loss: 0.5809 - val_accuracy: 0.7892\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5660 - accuracy: 0.8004 - val_loss: 0.5644 - val_accuracy: 0.7958\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.5501 - accuracy: 0.8068 - val_loss: 0.5495 - val_accuracy: 0.8020\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5359 - accuracy: 0.8126 - val_loss: 0.5361 - val_accuracy: 0.8088\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5231 - accuracy: 0.8169 - val_loss: 0.5242 - val_accuracy: 0.8123\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5118 - accuracy: 0.8212 - val_loss: 0.5136 - val_accuracy: 0.8158\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5018 - accuracy: 0.8240 - val_loss: 0.5041 - val_accuracy: 0.8195\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4928 - accuracy: 0.8272 - val_loss: 0.4956 - val_accuracy: 0.8225\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4848 - accuracy: 0.8295 - val_loss: 0.4879 - val_accuracy: 0.8250\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4775 - accuracy: 0.8321 - val_loss: 0.4809 - val_accuracy: 0.8273\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4709 - accuracy: 0.8344 - val_loss: 0.4745 - val_accuracy: 0.8305\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4647 - accuracy: 0.8363 - val_loss: 0.4686 - val_accuracy: 0.8325\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4591 - accuracy: 0.8384 - val_loss: 0.4631 - val_accuracy: 0.8333\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4538 - accuracy: 0.8403 - val_loss: 0.4579 - val_accuracy: 0.8363\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4488 - accuracy: 0.8418 - val_loss: 0.4531 - val_accuracy: 0.8378\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4441 - accuracy: 0.8436 - val_loss: 0.4485 - val_accuracy: 0.8383\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4397 - accuracy: 0.8452 - val_loss: 0.4442 - val_accuracy: 0.8393\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4355 - accuracy: 0.8468 - val_loss: 0.4401 - val_accuracy: 0.8397\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4315 - accuracy: 0.8481 - val_loss: 0.4363 - val_accuracy: 0.8410\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4277 - accuracy: 0.8494 - val_loss: 0.4326 - val_accuracy: 0.8432\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4241 - accuracy: 0.8507 - val_loss: 0.4292 - val_accuracy: 0.8448\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4206 - accuracy: 0.8520 - val_loss: 0.4259 - val_accuracy: 0.8465\n",
      "34\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 2.0995 - accuracy: 0.3984 - val_loss: 1.7970 - val_accuracy: 0.5143\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5080 - accuracy: 0.5655 - val_loss: 1.2942 - val_accuracy: 0.5982\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1716 - accuracy: 0.6271 - val_loss: 1.0748 - val_accuracy: 0.6368\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.0034 - accuracy: 0.6637 - val_loss: 0.9453 - val_accuracy: 0.6703\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8933 - accuracy: 0.6912 - val_loss: 0.8521 - val_accuracy: 0.6965\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8107 - accuracy: 0.7154 - val_loss: 0.7801 - val_accuracy: 0.7172\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7473 - accuracy: 0.7352 - val_loss: 0.7248 - val_accuracy: 0.7390\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6985 - accuracy: 0.7501 - val_loss: 0.6821 - val_accuracy: 0.7517\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6601 - accuracy: 0.7597 - val_loss: 0.6483 - val_accuracy: 0.7610\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6289 - accuracy: 0.7703 - val_loss: 0.6207 - val_accuracy: 0.7723\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6030 - accuracy: 0.7805 - val_loss: 0.5976 - val_accuracy: 0.7815\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5810 - accuracy: 0.7905 - val_loss: 0.5778 - val_accuracy: 0.7917\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5619 - accuracy: 0.7994 - val_loss: 0.5605 - val_accuracy: 0.7987\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5453 - accuracy: 0.8071 - val_loss: 0.5452 - val_accuracy: 0.8037\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5308 - accuracy: 0.8131 - val_loss: 0.5318 - val_accuracy: 0.8093\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5181 - accuracy: 0.8183 - val_loss: 0.5200 - val_accuracy: 0.8130\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5070 - accuracy: 0.8218 - val_loss: 0.5096 - val_accuracy: 0.8165\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4973 - accuracy: 0.8253 - val_loss: 0.5004 - val_accuracy: 0.8195\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4887 - accuracy: 0.8282 - val_loss: 0.4921 - val_accuracy: 0.8230\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4810 - accuracy: 0.8315 - val_loss: 0.4847 - val_accuracy: 0.8248\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4740 - accuracy: 0.8339 - val_loss: 0.4779 - val_accuracy: 0.8288\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4677 - accuracy: 0.8364 - val_loss: 0.4717 - val_accuracy: 0.8305\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4619 - accuracy: 0.8380 - val_loss: 0.4660 - val_accuracy: 0.8330\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4565 - accuracy: 0.8393 - val_loss: 0.4606 - val_accuracy: 0.8352\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4515 - accuracy: 0.8413 - val_loss: 0.4556 - val_accuracy: 0.8368\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4468 - accuracy: 0.8430 - val_loss: 0.4509 - val_accuracy: 0.8388\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4423 - accuracy: 0.8443 - val_loss: 0.4464 - val_accuracy: 0.8407\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4381 - accuracy: 0.8460 - val_loss: 0.4422 - val_accuracy: 0.8410\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4342 - accuracy: 0.8473 - val_loss: 0.4382 - val_accuracy: 0.8432\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4304 - accuracy: 0.8488 - val_loss: 0.4344 - val_accuracy: 0.8453\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4268 - accuracy: 0.8500 - val_loss: 0.4308 - val_accuracy: 0.8462\n",
      "31\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 2.0829 - accuracy: 0.3955 - val_loss: 1.7425 - val_accuracy: 0.5367\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 1.4488 - accuracy: 0.5871 - val_loss: 1.2333 - val_accuracy: 0.6203\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 1.1090 - accuracy: 0.6535 - val_loss: 1.0094 - val_accuracy: 0.6720\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.9342 - accuracy: 0.6981 - val_loss: 0.8746 - val_accuracy: 0.7113\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8195 - accuracy: 0.7245 - val_loss: 0.7823 - val_accuracy: 0.7313\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7419 - accuracy: 0.7409 - val_loss: 0.7201 - val_accuracy: 0.7460\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6892 - accuracy: 0.7528 - val_loss: 0.6765 - val_accuracy: 0.7547\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6509 - accuracy: 0.7633 - val_loss: 0.6434 - val_accuracy: 0.7645\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6211 - accuracy: 0.7724 - val_loss: 0.6167 - val_accuracy: 0.7763\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5965 - accuracy: 0.7826 - val_loss: 0.5941 - val_accuracy: 0.7835\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5754 - accuracy: 0.7912 - val_loss: 0.5746 - val_accuracy: 0.7900\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5571 - accuracy: 0.8002 - val_loss: 0.5575 - val_accuracy: 0.7968\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5411 - accuracy: 0.8072 - val_loss: 0.5425 - val_accuracy: 0.8040\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5272 - accuracy: 0.8128 - val_loss: 0.5295 - val_accuracy: 0.8082\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5151 - accuracy: 0.8174 - val_loss: 0.5182 - val_accuracy: 0.8117\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5046 - accuracy: 0.8214 - val_loss: 0.5083 - val_accuracy: 0.8158\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4954 - accuracy: 0.8246 - val_loss: 0.4995 - val_accuracy: 0.8188\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4873 - accuracy: 0.8276 - val_loss: 0.4917 - val_accuracy: 0.8210\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4800 - accuracy: 0.8302 - val_loss: 0.4847 - val_accuracy: 0.8230\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4734 - accuracy: 0.8331 - val_loss: 0.4782 - val_accuracy: 0.8255\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4674 - accuracy: 0.8355 - val_loss: 0.4722 - val_accuracy: 0.8293\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4618 - accuracy: 0.8373 - val_loss: 0.4667 - val_accuracy: 0.8310\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4567 - accuracy: 0.8386 - val_loss: 0.4615 - val_accuracy: 0.8328\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4518 - accuracy: 0.8408 - val_loss: 0.4566 - val_accuracy: 0.8357\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4472 - accuracy: 0.8419 - val_loss: 0.4520 - val_accuracy: 0.8382\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4429 - accuracy: 0.8433 - val_loss: 0.4476 - val_accuracy: 0.8392\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4387 - accuracy: 0.8448 - val_loss: 0.4434 - val_accuracy: 0.8410\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4348 - accuracy: 0.8460 - val_loss: 0.4395 - val_accuracy: 0.8422\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4311 - accuracy: 0.8474 - val_loss: 0.4357 - val_accuracy: 0.8422\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4275 - accuracy: 0.8489 - val_loss: 0.4322 - val_accuracy: 0.8450\n",
      "30\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 2.0918 - accuracy: 0.4046 - val_loss: 1.7554 - val_accuracy: 0.5035\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 1.4473 - accuracy: 0.5924 - val_loss: 1.2254 - val_accuracy: 0.6318\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 1.0961 - accuracy: 0.6680 - val_loss: 0.9935 - val_accuracy: 0.6812\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.9170 - accuracy: 0.7072 - val_loss: 0.8591 - val_accuracy: 0.7158\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.8051 - accuracy: 0.7338 - val_loss: 0.7704 - val_accuracy: 0.7393\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.7303 - accuracy: 0.7486 - val_loss: 0.7103 - val_accuracy: 0.7533\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6793 - accuracy: 0.7605 - val_loss: 0.6682 - val_accuracy: 0.7617\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6428 - accuracy: 0.7703 - val_loss: 0.6366 - val_accuracy: 0.7705\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6148 - accuracy: 0.7785 - val_loss: 0.6114 - val_accuracy: 0.7783\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5921 - accuracy: 0.7864 - val_loss: 0.5903 - val_accuracy: 0.7855\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5729 - accuracy: 0.7941 - val_loss: 0.5723 - val_accuracy: 0.7900\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5563 - accuracy: 0.8004 - val_loss: 0.5566 - val_accuracy: 0.7975\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5419 - accuracy: 0.8061 - val_loss: 0.5428 - val_accuracy: 0.8022\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5291 - accuracy: 0.8116 - val_loss: 0.5306 - val_accuracy: 0.8082\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5176 - accuracy: 0.8159 - val_loss: 0.5196 - val_accuracy: 0.8135\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5074 - accuracy: 0.8193 - val_loss: 0.5097 - val_accuracy: 0.8180\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4981 - accuracy: 0.8228 - val_loss: 0.5007 - val_accuracy: 0.8225\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4897 - accuracy: 0.8261 - val_loss: 0.4925 - val_accuracy: 0.8255\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4820 - accuracy: 0.8290 - val_loss: 0.4849 - val_accuracy: 0.8267\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4750 - accuracy: 0.8311 - val_loss: 0.4780 - val_accuracy: 0.8280\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4686 - accuracy: 0.8336 - val_loss: 0.4716 - val_accuracy: 0.8313\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4626 - accuracy: 0.8354 - val_loss: 0.4657 - val_accuracy: 0.8337\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4572 - accuracy: 0.8382 - val_loss: 0.4602 - val_accuracy: 0.8353\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4520 - accuracy: 0.8402 - val_loss: 0.4551 - val_accuracy: 0.8358\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4472 - accuracy: 0.8418 - val_loss: 0.4503 - val_accuracy: 0.8377\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4427 - accuracy: 0.8434 - val_loss: 0.4457 - val_accuracy: 0.8397\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4385 - accuracy: 0.8448 - val_loss: 0.4415 - val_accuracy: 0.8420\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4345 - accuracy: 0.8459 - val_loss: 0.4375 - val_accuracy: 0.8427\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4307 - accuracy: 0.8474 - val_loss: 0.4337 - val_accuracy: 0.8438\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4271 - accuracy: 0.8483 - val_loss: 0.4302 - val_accuracy: 0.8448\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4236 - accuracy: 0.8495 - val_loss: 0.4268 - val_accuracy: 0.8465\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# Running in arround 340s\n",
    "\n",
    "# some possible numbers to choose\n",
    "hidden_layer_2 = [20, 40, 60, 80]\n",
    "\n",
    "# results\n",
    "loss_history_2 = []\n",
    "epoch_history_2 = []\n",
    "\n",
    "# for the first hidden layer\n",
    "for i in hidden_layer_2:\n",
    "    loss, epoch = test_train_mlp(100, i)\n",
    "    loss_history_2.append(loss)\n",
    "    epoch_history_2.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5ec56b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The min loss is 0.4206 when neurons is equal to 20\n",
      "Loss history: [0.4206, 0.4268, 0.4275, 0.4236]\n",
      "Epoch history: [34, 31, 30, 31]\n"
     ]
    }
   ],
   "source": [
    "loss_history_2 = [round(i, 4) for i in loss_history_2]\n",
    "print(f\"The min loss is {min(loss_history_2)} when neurons is equal to {hidden_layer_2[loss_history_2.index(min(loss_history_2))]}\")\n",
    "\n",
    "print(f\"Loss history: {loss_history_2}\")\n",
    "print(f\"Epoch history: {epoch_history_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f06bf4",
   "metadata": {},
   "source": [
    "We select 20 as the neurons of the second hidden layer using the same method as before consistently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd29f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method in this section.\n",
    "def get_result(estimator, paras, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, epochs=1):\n",
    "    \"\"\"get grid search result.\n",
    "    estimator: model to be tuned.\n",
    "    paras: an instance of ParameterGrid.\"\"\"\n",
    "    # Return a dict\n",
    "    result = {\n",
    "        \"best_paras\": None,\n",
    "        \"best_score\": 0,\n",
    "        \"best_estimator\": None,\n",
    "        \"results\": [],\n",
    "    }\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    # Grid search for each combination.\n",
    "    for para in paras:\n",
    "        # Set para\n",
    "        current_estimator = clone(estimator)\n",
    "        current_estimator.set_params(**para)\n",
    "\n",
    "        # Training and timer\n",
    "        t1 = time.time()\n",
    "        if epochs == 1:\n",
    "            current_estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            current_estimator.fit(X_train, y_train, epochs=epochs)\n",
    "        t2 = time.time()\n",
    "\n",
    "        # Score on validation set\n",
    "        score = current_estimator.score(X_valid, y_valid)\n",
    "        t3 = time.time()\n",
    "\n",
    "        # result for each combination\n",
    "        temp = {}\n",
    "        temp[\"paras\"] = para\n",
    "        temp[\"training_time\"] = t2 - t1\n",
    "        temp[\"validation_time\"] = t3 - t2\n",
    "        temp[\"score\"] =score\n",
    "        \n",
    "        # Update the best result\n",
    "        result[\"results\"].append(temp)\n",
    "        if score > result[\"best_score\"]:\n",
    "            result[\"best_paras\"] = para\n",
    "            result[\"best_score\"] = score\n",
    "            result[\"best_estimator\"] = current_estimator\n",
    "        \n",
    "        i += 1\n",
    "        print(f\"{i} out of {len(list(paras))} finished: {para}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def show_results(name, result, X_test=X_test, y_test=y_test):\n",
    "    \"\"\"Show the results.\"\"\"\n",
    "    print(f\"Results for {name}:\")\n",
    "    print(\"Best parameters: {}\".format(result[\"best_paras\"]))\n",
    "    print(\"Best validation score: {:.4f}\".format(result[\"best_score\"]))\n",
    "    print(\"Test set score: {:.4f}\".format(result[\"best_estimator\"].score(X_test, y_test)))\n",
    "\n",
    "    # table of results\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"Score\", \"training_time\", \"validation_time\"], \n",
    "        index=[str(result[\"results\"][i][\"paras\"]) for i in range(len(result[\"results\"]))]\n",
    "        )\n",
    "\n",
    "    for i in range(len(result[\"results\"])):\n",
    "        df.loc[str(result[\"results\"][i][\"paras\"])] = [\n",
    "            round(result[\"results\"][i][\"score\"], 4),\n",
    "            round(result[\"results\"][i][\"training_time\"], 2),\n",
    "            round(result[\"results\"][i][\"validation_time\"], 2)\n",
    "            ]\n",
    "\n",
    "    df.to_csv(f'{name}_results.csv')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5065b2",
   "metadata": {},
   "source": [
    "First, to determine a rough trend of the accuracy with different k, we calculate an accuracy every 10 with different k, until k = 245 (sqrt(#examples)), e.g., k = [1, 11, 21, ..., 241]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7796726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\t finished.\n",
      "k = 11\t finished.\n",
      "k = 21\t finished.\n",
      "k = 31\t finished.\n",
      "k = 41\t finished.\n",
      "k = 51\t finished.\n",
      "k = 61\t finished.\n",
      "k = 71\t finished.\n",
      "k = 81\t finished.\n",
      "k = 91\t finished.\n",
      "k = 101\t finished.\n",
      "k = 111\t finished.\n",
      "k = 121\t finished.\n",
      "k = 131\t finished.\n",
      "k = 141\t finished.\n",
      "k = 151\t finished.\n",
      "k = 161\t finished.\n",
      "k = 171\t finished.\n",
      "k = 181\t finished.\n",
      "k = 191\t finished.\n",
      "k = 201\t finished.\n",
      "k = 211\t finished.\n",
      "k = 221\t finished.\n",
      "k = 231\t finished.\n",
      "k = 241\t finished.\n"
     ]
    }
   ],
   "source": [
    "# Running in arroung 190s\n",
    "\n",
    "k_value = [1 + i for i in range(245)]\n",
    "k_acc = []\n",
    "\n",
    "for i in k_value:\n",
    "    if i % 10 != 1:\n",
    "        continue\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "    k_acc.append(knn.score(X_test.reshape(X_test.shape[0], -1), y_test))\n",
    "    print(f\"k = {i}\\t finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8feae03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 144x144 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2, 2))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot([i*10 + 1 for i in range(25)], k_acc, marker=\"o\")\n",
    "ax.set(xlabel=\"k\", ylabel=\"ACC\", xticks=range(1, 250, 40))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c385ef",
   "metadata": {},
   "source": [
    "From the figure, the trend is roughly decending. Therefore, we can choose a range of k in [1, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa801bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\t finished.\n",
      "k = 2\t finished.\n",
      "k = 3\t finished.\n",
      "k = 4\t finished.\n",
      "k = 5\t finished.\n",
      "k = 6\t finished.\n",
      "k = 7\t finished.\n",
      "k = 8\t finished.\n",
      "k = 9\t finished.\n",
      "k = 10\t finished.\n",
      "k = 11\t finished.\n",
      "k = 12\t finished.\n",
      "k = 13\t finished.\n",
      "k = 14\t finished.\n",
      "k = 15\t finished.\n",
      "k = 16\t finished.\n",
      "k = 17\t finished.\n",
      "k = 18\t finished.\n",
      "k = 19\t finished.\n",
      "k = 20\t finished.\n"
     ]
    }
   ],
   "source": [
    "# Running in arroung 140s\n",
    "\n",
    "k_value = [i for i in range(1,21)]\n",
    "k_acc = []\n",
    "\n",
    "for i in k_value:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "    k_acc.append(knn.score(X_test.reshape(X_test.shape[0], -1), y_test))\n",
    "    print(f\"k = {i}\\t finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4272973c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 144x144 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2, 2))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(k_value, k_acc, marker=\"o\")\n",
    "ax.set(xlabel=\"k\", ylabel=\"ACC\", xticks=k_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089faa21",
   "metadata": {},
   "source": [
    "From the figure above, we chose k = [3, 5, 9]. For p, we chose p [1, 2], it represents manhattan_distance and euclidean_distance respectively. We also consider the weight (uniform, distance) each point contribute to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca9e1e",
   "metadata": {},
   "source": [
    "If we use CV with 10 folds, totally we need 3 * 2 * 2 * cv = 120 on 90% training set. Considering the running time, we use standard grid search instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfadede2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 combinations.\n",
      "Parameter grid:\n",
      "{'n_neighbors': [3, 5, 9], 'p': [1, 2], 'weights': ['uniform', 'distance']}\n"
     ]
    }
   ],
   "source": [
    "# Setting parameters\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 9],\n",
    "    'p': [1, 2],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    }\n",
    "\n",
    "knn_paras = ParameterGrid(param_grid)\n",
    "\n",
    "print(f\"There are {len(list(knn_paras))} combinations.\")\n",
    "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CV for tuning\n",
    "# Setting the 10 fold stratified cross-validation\n",
    "\n",
    "# cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "# grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=cvKFold, return_train_score=True, verbose=3)\n",
    "# grid_search.fit(X_train_full.reshape(60000, -1), y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d58ecbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 12 finished.\n",
      "2 out of 12 finished.\n",
      "3 out of 12 finished.\n",
      "4 out of 12 finished.\n",
      "5 out of 12 finished.\n",
      "6 out of 12 finished.\n",
      "7 out of 12 finished.\n",
      "8 out of 12 finished.\n",
      "9 out of 12 finished.\n",
      "10 out of 12 finished.\n",
      "11 out of 12 finished.\n",
      "12 out of 12 finished.\n"
     ]
    }
   ],
   "source": [
    "# Running in around 410s\n",
    "\n",
    "# Tuning KNN paras, 12 combination in total.\n",
    "knn_result = get_result(\n",
    "    KNeighborsClassifier(), \n",
    "    knn_paras, \n",
    "    X_train=X_train.reshape(X_train.shape[0], -1), \n",
    "    X_valid=X_valid.reshape(X_valid.shape[0], -1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e925ac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for KNN:\n",
      "Best parameters: {'n_neighbors': 3, 'p': 1, 'weights': 'distance'}\n",
      "Best validation score: 0.8653\n",
      "Test set score: 0.8567\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>training_time</th>\n",
       "      <th>validation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}</th>\n",
       "      <td>0.8622</td>\n",
       "      <td>0.08</td>\n",
       "      <td>55.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 3, 'p': 1, 'weights': 'distance'}</th>\n",
       "      <td>0.8653</td>\n",
       "      <td>0.07</td>\n",
       "      <td>56.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}</th>\n",
       "      <td>0.8583</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 3, 'p': 2, 'weights': 'distance'}</th>\n",
       "      <td>0.8602</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}</th>\n",
       "      <td>0.8603</td>\n",
       "      <td>0.09</td>\n",
       "      <td>54.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 5, 'p': 1, 'weights': 'distance'}</th>\n",
       "      <td>0.8622</td>\n",
       "      <td>0.06</td>\n",
       "      <td>54.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}</th>\n",
       "      <td>0.855</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 5, 'p': 2, 'weights': 'distance'}</th>\n",
       "      <td>0.8577</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}</th>\n",
       "      <td>0.8628</td>\n",
       "      <td>0.09</td>\n",
       "      <td>54.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 9, 'p': 1, 'weights': 'distance'}</th>\n",
       "      <td>0.8645</td>\n",
       "      <td>0.06</td>\n",
       "      <td>54.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 9, 'p': 2, 'weights': 'uniform'}</th>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 9, 'p': 2, 'weights': 'distance'}</th>\n",
       "      <td>0.8533</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Score training_time  \\\n",
       "{'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}   0.8622          0.08   \n",
       "{'n_neighbors': 3, 'p': 1, 'weights': 'distance'}  0.8653          0.07   \n",
       "{'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}   0.8583          0.08   \n",
       "{'n_neighbors': 3, 'p': 2, 'weights': 'distance'}  0.8602          0.06   \n",
       "{'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}   0.8603          0.09   \n",
       "{'n_neighbors': 5, 'p': 1, 'weights': 'distance'}  0.8622          0.06   \n",
       "{'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}    0.855          0.08   \n",
       "{'n_neighbors': 5, 'p': 2, 'weights': 'distance'}  0.8577          0.06   \n",
       "{'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}   0.8628          0.09   \n",
       "{'n_neighbors': 9, 'p': 1, 'weights': 'distance'}  0.8645          0.06   \n",
       "{'n_neighbors': 9, 'p': 2, 'weights': 'uniform'}   0.8512          0.06   \n",
       "{'n_neighbors': 9, 'p': 2, 'weights': 'distance'}  0.8533          0.05   \n",
       "\n",
       "                                                  validation_time  \n",
       "{'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}            55.33  \n",
       "{'n_neighbors': 3, 'p': 1, 'weights': 'distance'}           56.74  \n",
       "{'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}             3.49  \n",
       "{'n_neighbors': 3, 'p': 2, 'weights': 'distance'}            3.26  \n",
       "{'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}            54.26  \n",
       "{'n_neighbors': 5, 'p': 1, 'weights': 'distance'}           54.26  \n",
       "{'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}             3.46  \n",
       "{'n_neighbors': 5, 'p': 2, 'weights': 'distance'}            3.27  \n",
       "{'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}            54.43  \n",
       "{'n_neighbors': 9, 'p': 1, 'weights': 'distance'}           54.62  \n",
       "{'n_neighbors': 9, 'p': 2, 'weights': 'uniform'}              3.4  \n",
       "{'n_neighbors': 9, 'p': 2, 'weights': 'distance'}            3.24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running in around 110s\n",
    "\n",
    "show_results(\"KNN\", knn_result, X_test=X_test.reshape(X_test.shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fully connected neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd231b38",
   "metadata": {},
   "source": [
    "From section 2.2, we settled the numbers of neurons (100, 20) in hidden layers. Although the number of hidden layers as well as number of neurons are also hyperparameter, to avoid a great running time due to a number of combination of paras, we design the structure first and tune the other paras in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce23462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24 combinations.\n",
      "Parameter grid:\n",
      "{'optimizer__lr': [0.1, 0.01, 0.001], 'activation_function': ['relu', 'sigmoid', 'tanh', None], 'optimizer': ['sgd', 'Adam']}\n"
     ]
    }
   ],
   "source": [
    "def build_mlp(activation_function=\"relu\"):\n",
    "    \"\"\"Build a Keras MLP for 10 class classification with desired parameters.\"\"\"\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Add the input layer\n",
    "    model.add(keras.layers.Flatten(input_shape=IMAGE_SIZE))\n",
    "    \n",
    "    # Add the hidden layers with activation function\n",
    "    model.add(keras.layers.Dense(100, activation=activation_function))\n",
    "    model.add(keras.layers.Dense(20, activation=activation_function))\n",
    "        \n",
    "    # Add the output layer for 10 class classification\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier object which works with sklearn grid searches\n",
    "# We need to pass default values of arguments in build_mlp if we wish to tune them\n",
    "keras_classifier = KerasClassifier(build_mlp,\n",
    "                                   activation_function=\"relu\",\n",
    "                                   loss=\"sparse_categorical_crossentropy\",\n",
    "                                   optimizer=\"sgd\",\n",
    "                                   optimizer__lr=0.01,\n",
    "                                   metrics=[\"accuracy\"]\n",
    "                                  )\n",
    "\n",
    "param_grid = {\n",
    "    \"optimizer__lr\": [0.1, 0.01, 0.001],\n",
    "    \"activation_function\": [\"relu\", \"sigmoid\", \"tanh\", None],\n",
    "    \"optimizer\": [\"sgd\", \"Adam\"]\n",
    "}\n",
    "\n",
    "mlp_paras = ParameterGrid(param_grid)\n",
    "\n",
    "print(f\"There are {len(list(mlp_paras))} combinations.\")\n",
    "print(\"Parameter grid:\\n{}\".format(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cd83cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5828 - accuracy: 0.7866\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4142 - accuracy: 0.8485\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3736 - accuracy: 0.8630\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3474 - accuracy: 0.8719\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3293 - accuracy: 0.8783\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3143 - accuracy: 0.8838\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3031 - accuracy: 0.8888\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2929 - accuracy: 0.8901\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2844 - accuracy: 0.8929\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2797 - accuracy: 0.8950\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2698 - accuracy: 0.8999\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2637 - accuracy: 0.9013\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2578 - accuracy: 0.9028\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2500 - accuracy: 0.9061\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2449 - accuracy: 0.9071\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2413 - accuracy: 0.9083\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2358 - accuracy: 0.9116\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2331 - accuracy: 0.9113\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2284 - accuracy: 0.9134\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2244 - accuracy: 0.9152\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2178 - accuracy: 0.9178\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2132 - accuracy: 0.9189\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2117 - accuracy: 0.9191\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2069 - accuracy: 0.9203\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2054 - accuracy: 0.9226\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2015 - accuracy: 0.9229\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1994 - accuracy: 0.9244\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1956 - accuracy: 0.9258\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1930 - accuracy: 0.9267\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1902 - accuracy: 0.9278\n",
      "188/188 [==============================] - 0s 901us/step\n",
      "1 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7796 - accuracy: 0.7406\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5088 - accuracy: 0.8235\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4597 - accuracy: 0.8393\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4328 - accuracy: 0.8474\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4132 - accuracy: 0.8544\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3973 - accuracy: 0.8621\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3839 - accuracy: 0.8651\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3732 - accuracy: 0.8683\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3630 - accuracy: 0.8719\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3545 - accuracy: 0.8743\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3460 - accuracy: 0.8770\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3388 - accuracy: 0.8787\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3326 - accuracy: 0.8802\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3256 - accuracy: 0.8846\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3198 - accuracy: 0.8847\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3146 - accuracy: 0.8863\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3085 - accuracy: 0.8894\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3043 - accuracy: 0.8895\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2993 - accuracy: 0.8919\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2939 - accuracy: 0.8943\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2906 - accuracy: 0.8944\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2873 - accuracy: 0.8957\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2832 - accuracy: 0.8979\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2790 - accuracy: 0.8994\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2747 - accuracy: 0.9005\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2719 - accuracy: 0.9018\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2678 - accuracy: 0.9024\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2636 - accuracy: 0.9040\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2614 - accuracy: 0.9058\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2577 - accuracy: 0.9063\n",
      "188/188 [==============================] - 0s 887us/step\n",
      "2 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7175 - accuracy: 0.4389\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1015 - accuracy: 0.6524\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8764 - accuracy: 0.6871\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7801 - accuracy: 0.7178\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7221 - accuracy: 0.7439\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6799 - accuracy: 0.7638\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6461 - accuracy: 0.7801\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6173 - accuracy: 0.7913\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5928 - accuracy: 0.8004\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5723 - accuracy: 0.8066\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5548 - accuracy: 0.8119\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5397 - accuracy: 0.8172\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5273 - accuracy: 0.8197\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5164 - accuracy: 0.8237\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5069 - accuracy: 0.8259\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4984 - accuracy: 0.8280\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4907 - accuracy: 0.8314\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4838 - accuracy: 0.8331\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4775 - accuracy: 0.8358\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4718 - accuracy: 0.8363\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4666 - accuracy: 0.8384\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4617 - accuracy: 0.8402\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4573 - accuracy: 0.8416\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4532 - accuracy: 0.8426\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4490 - accuracy: 0.8437\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4454 - accuracy: 0.8461\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4418 - accuracy: 0.8469\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4385 - accuracy: 0.8482\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4355 - accuracy: 0.8490\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4324 - accuracy: 0.8495\n",
      "188/188 [==============================] - 0s 871us/step\n",
      "3 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5463 - accuracy: 0.3626\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4111 - accuracy: 0.3500\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4844 - accuracy: 0.3378\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7644 - accuracy: 0.2027\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7721 - accuracy: 0.1980\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7944 - accuracy: 0.1972\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7579 - accuracy: 0.1971\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7597 - accuracy: 0.1955\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7586 - accuracy: 0.1962\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7598 - accuracy: 0.1941\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7607 - accuracy: 0.1949\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7590 - accuracy: 0.1951\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7604 - accuracy: 0.1982\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7589 - accuracy: 0.1972\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7607 - accuracy: 0.1967\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7572 - accuracy: 0.1971\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7596 - accuracy: 0.1986\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7581 - accuracy: 0.1969\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7604 - accuracy: 0.1938\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7595 - accuracy: 0.1999\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7621 - accuracy: 0.1978\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7587 - accuracy: 0.1967\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7583 - accuracy: 0.1966\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7575 - accuracy: 0.2015\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7605 - accuracy: 0.1968\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7597 - accuracy: 0.1977\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7587 - accuracy: 0.1957\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7579 - accuracy: 0.1999\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7588 - accuracy: 0.1982\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7599 - accuracy: 0.1973\n",
      "188/188 [==============================] - 0s 862us/step\n",
      "4 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5713 - accuracy: 0.7924\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4413 - accuracy: 0.8406\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4134 - accuracy: 0.8517\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4001 - accuracy: 0.8565\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3873 - accuracy: 0.8620\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3802 - accuracy: 0.8634\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3745 - accuracy: 0.8667\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3611 - accuracy: 0.8721\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3581 - accuracy: 0.8730\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3555 - accuracy: 0.8738\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3531 - accuracy: 0.8744\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 20s 12ms/step - loss: 0.3479 - accuracy: 0.8758\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 20s 12ms/step - loss: 0.3400 - accuracy: 0.8780\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 20s 12ms/step - loss: 0.3404 - accuracy: 0.8789\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 20s 12ms/step - loss: 0.3352 - accuracy: 0.8808\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3390 - accuracy: 0.8798\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3366 - accuracy: 0.8791\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3218 - accuracy: 0.8853\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3248 - accuracy: 0.8854\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3246 - accuracy: 0.8856\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3257 - accuracy: 0.8867\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3193 - accuracy: 0.8866\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3237 - accuracy: 0.8859\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3202 - accuracy: 0.8882\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3154 - accuracy: 0.8885\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3159 - accuracy: 0.8891\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3124 - accuracy: 0.8884\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3144 - accuracy: 0.8902\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3111 - accuracy: 0.8889\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3075 - accuracy: 0.8921\n",
      "188/188 [==============================] - 0s 953us/step\n",
      "5 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.5382 - accuracy: 0.8097\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3896 - accuracy: 0.8610\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3496 - accuracy: 0.8723\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3272 - accuracy: 0.8796\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3081 - accuracy: 0.8872\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2947 - accuracy: 0.8913\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2821 - accuracy: 0.8958\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2681 - accuracy: 0.9004\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2612 - accuracy: 0.9018\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2521 - accuracy: 0.9060\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2429 - accuracy: 0.9095\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2389 - accuracy: 0.9107\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2290 - accuracy: 0.9143\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2239 - accuracy: 0.9153\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2168 - accuracy: 0.9176\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2118 - accuracy: 0.9194\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2073 - accuracy: 0.9223\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2015 - accuracy: 0.9235\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1988 - accuracy: 0.9257\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1952 - accuracy: 0.9269\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1876 - accuracy: 0.9286\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1857 - accuracy: 0.9304\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1809 - accuracy: 0.9320\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1768 - accuracy: 0.9329\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1746 - accuracy: 0.9356\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1699 - accuracy: 0.9373\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1690 - accuracy: 0.9361\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1641 - accuracy: 0.9395\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1599 - accuracy: 0.9399\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1590 - accuracy: 0.9408\n",
      "188/188 [==============================] - 0s 978us/step\n",
      "6 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.0409 - accuracy: 0.6619\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5602 - accuracy: 0.8033\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4838 - accuracy: 0.8286\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4451 - accuracy: 0.8401\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4190 - accuracy: 0.8509\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3993 - accuracy: 0.8575\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3844 - accuracy: 0.8624\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3723 - accuracy: 0.8662\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3619 - accuracy: 0.8696\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3533 - accuracy: 0.8725\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3445 - accuracy: 0.8757\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3372 - accuracy: 0.8784\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3301 - accuracy: 0.8803\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3233 - accuracy: 0.8835\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3166 - accuracy: 0.8852\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3117 - accuracy: 0.8878\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3048 - accuracy: 0.8903\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3010 - accuracy: 0.8901\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2959 - accuracy: 0.8916\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2900 - accuracy: 0.8943\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2867 - accuracy: 0.8949\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2834 - accuracy: 0.8976\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2790 - accuracy: 0.8982\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2747 - accuracy: 0.9000\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2703 - accuracy: 0.9016\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2676 - accuracy: 0.9020\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2636 - accuracy: 0.9036\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2598 - accuracy: 0.9041\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2575 - accuracy: 0.9063\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2533 - accuracy: 0.9076\n",
      "188/188 [==============================] - 0s 943us/step\n",
      "7 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.1202 - accuracy: 0.4008\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5601 - accuracy: 0.5391\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2323 - accuracy: 0.6225\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.0571 - accuracy: 0.6536\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.9375 - accuracy: 0.6792\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8504 - accuracy: 0.7066\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7833 - accuracy: 0.7283\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7294 - accuracy: 0.7457\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6861 - accuracy: 0.7586\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6514 - accuracy: 0.7689\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6226 - accuracy: 0.7781\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5979 - accuracy: 0.7886\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5767 - accuracy: 0.7968\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5583 - accuracy: 0.8051\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5426 - accuracy: 0.8109\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5289 - accuracy: 0.8162\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5172 - accuracy: 0.8196\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5072 - accuracy: 0.8238\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4984 - accuracy: 0.8266\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4907 - accuracy: 0.8294\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4836 - accuracy: 0.8307\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4773 - accuracy: 0.8344\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4714 - accuracy: 0.8354\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4660 - accuracy: 0.8364\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4608 - accuracy: 0.8385\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4561 - accuracy: 0.8403\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4517 - accuracy: 0.8421\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4474 - accuracy: 0.8429\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4434 - accuracy: 0.8455\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4398 - accuracy: 0.8464\n",
      "188/188 [==============================] - 0s 959us/step\n",
      "8 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.3663 - accuracy: 0.1022\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.2771 - accuracy: 0.1301\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.2490 - accuracy: 0.3064\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.2235 - accuracy: 0.4730\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.1958 - accuracy: 0.5137\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.1642 - accuracy: 0.5341\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.1276 - accuracy: 0.5460\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.0850 - accuracy: 0.5400\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.0364 - accuracy: 0.5472\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.9822 - accuracy: 0.5672\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.9244 - accuracy: 0.5654\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.8649 - accuracy: 0.5717\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.8057 - accuracy: 0.5724\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7484 - accuracy: 0.5751\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.6938 - accuracy: 0.5842\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.6422 - accuracy: 0.5884\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5936 - accuracy: 0.5956\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5477 - accuracy: 0.6068\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5043 - accuracy: 0.6155\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4631 - accuracy: 0.6211\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4239 - accuracy: 0.6274\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.3866 - accuracy: 0.6339\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.3511 - accuracy: 0.6360\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.3173 - accuracy: 0.6430\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2852 - accuracy: 0.6471\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2546 - accuracy: 0.6504\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2256 - accuracy: 0.6534\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1981 - accuracy: 0.6599\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1721 - accuracy: 0.6605\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1475 - accuracy: 0.6666\n",
      "188/188 [==============================] - 0s 951us/step\n",
      "9 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.3249 - accuracy: 0.4316\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2869 - accuracy: 0.4299\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2529 - accuracy: 0.4292\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2044 - accuracy: 0.4476\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1946 - accuracy: 0.4515\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1834 - accuracy: 0.4534\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2194 - accuracy: 0.4469\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2136 - accuracy: 0.4518\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2269 - accuracy: 0.4390\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2254 - accuracy: 0.4404\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2310 - accuracy: 0.4414\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1918 - accuracy: 0.4516\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1821 - accuracy: 0.4559\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2210 - accuracy: 0.4426\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2232 - accuracy: 0.4305\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4527 - accuracy: 0.2885\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4635 - accuracy: 0.2901\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4752 - accuracy: 0.2880\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5167 - accuracy: 0.2883\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4828 - accuracy: 0.3245\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 1.4660 - accuracy: 0.3348\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4293 - accuracy: 0.3539\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4283 - accuracy: 0.3470\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4109 - accuracy: 0.3543\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4467 - accuracy: 0.3472\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.3717 - accuracy: 0.3804\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4254 - accuracy: 0.3582\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4026 - accuracy: 0.3586\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4172 - accuracy: 0.3491\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.3917 - accuracy: 0.3575\n",
      "188/188 [==============================] - 0s 948us/step\n",
      "10 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5861 - accuracy: 0.7915\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4712 - accuracy: 0.8331\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4539 - accuracy: 0.8376\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4466 - accuracy: 0.8425\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4259 - accuracy: 0.8487\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4188 - accuracy: 0.8502\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4138 - accuracy: 0.8517\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4052 - accuracy: 0.8549\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4008 - accuracy: 0.8569\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3955 - accuracy: 0.8581\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3875 - accuracy: 0.8605\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3926 - accuracy: 0.8589\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3926 - accuracy: 0.8585\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3862 - accuracy: 0.8615\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3864 - accuracy: 0.8613\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3898 - accuracy: 0.8599\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3839 - accuracy: 0.8626\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3816 - accuracy: 0.8639\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3774 - accuracy: 0.8647\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3754 - accuracy: 0.8655\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.3714 - accuracy: 0.8648\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3703 - accuracy: 0.8667\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3745 - accuracy: 0.8662\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3736 - accuracy: 0.8644\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3679 - accuracy: 0.8681\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3662 - accuracy: 0.8688\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3693 - accuracy: 0.8679\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3620 - accuracy: 0.8696\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3663 - accuracy: 0.8694\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3624 - accuracy: 0.8692\n",
      "188/188 [==============================] - 0s 1ms/step\n",
      "11 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7804 - accuracy: 0.7599\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4225 - accuracy: 0.8533\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3700 - accuracy: 0.8684\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3429 - accuracy: 0.8769\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3233 - accuracy: 0.8839\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3081 - accuracy: 0.8887\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2947 - accuracy: 0.8944\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2819 - accuracy: 0.8981\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2734 - accuracy: 0.9007\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2653 - accuracy: 0.9027\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.2556 - accuracy: 0.9063\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2495 - accuracy: 0.9088\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2413 - accuracy: 0.9110\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2348 - accuracy: 0.9139\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2286 - accuracy: 0.9165\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.2222 - accuracy: 0.9185\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2158 - accuracy: 0.9211\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2107 - accuracy: 0.9226\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2057 - accuracy: 0.9249\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2029 - accuracy: 0.9264\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.1963 - accuracy: 0.9283\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1923 - accuracy: 0.9298\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1876 - accuracy: 0.9311\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1846 - accuracy: 0.9316\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1792 - accuracy: 0.9351\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1742 - accuracy: 0.9375\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1705 - accuracy: 0.9376\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1675 - accuracy: 0.9391\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1632 - accuracy: 0.9410\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1609 - accuracy: 0.9413\n",
      "188/188 [==============================] - 0s 950us/step\n",
      "12 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5473 - accuracy: 0.8043\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4170 - accuracy: 0.8497\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3803 - accuracy: 0.8620\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3568 - accuracy: 0.8708\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3381 - accuracy: 0.8774\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3234 - accuracy: 0.8821\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3111 - accuracy: 0.8863\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3007 - accuracy: 0.8899\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2910 - accuracy: 0.8924\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2830 - accuracy: 0.8956\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2759 - accuracy: 0.8975\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2687 - accuracy: 0.9018\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2626 - accuracy: 0.9028\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2550 - accuracy: 0.9061\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2506 - accuracy: 0.9079\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2445 - accuracy: 0.9093\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2387 - accuracy: 0.9122\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2345 - accuracy: 0.9132\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2286 - accuracy: 0.9148\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2238 - accuracy: 0.9169\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2190 - accuracy: 0.9176\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2163 - accuracy: 0.9204\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2134 - accuracy: 0.9202\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2081 - accuracy: 0.9234\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2046 - accuracy: 0.9239\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2029 - accuracy: 0.9240\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2009 - accuracy: 0.9257\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1978 - accuracy: 0.9265\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1950 - accuracy: 0.9276\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1908 - accuracy: 0.9293\n",
      "188/188 [==============================] - 0s 935us/step\n",
      "13 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8310 - accuracy: 0.7396\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5300 - accuracy: 0.8211\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4678 - accuracy: 0.8367\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4356 - accuracy: 0.8454\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4147 - accuracy: 0.8535\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3987 - accuracy: 0.8592\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3860 - accuracy: 0.8627\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3757 - accuracy: 0.8671\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3668 - accuracy: 0.8689\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3593 - accuracy: 0.8708\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3516 - accuracy: 0.8751\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3456 - accuracy: 0.8764\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3393 - accuracy: 0.8794\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3336 - accuracy: 0.8813\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3284 - accuracy: 0.8826\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3235 - accuracy: 0.8849\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3182 - accuracy: 0.8863\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3146 - accuracy: 0.8859\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3101 - accuracy: 0.8886\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3057 - accuracy: 0.8895\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3019 - accuracy: 0.8901\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2985 - accuracy: 0.8927\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2950 - accuracy: 0.8934\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2915 - accuracy: 0.8947\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2877 - accuracy: 0.8965\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2851 - accuracy: 0.8967\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2816 - accuracy: 0.8979\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2783 - accuracy: 0.9005\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2758 - accuracy: 0.9001\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2726 - accuracy: 0.9014\n",
      "188/188 [==============================] - 0s 912us/step\n",
      "14 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 3s 1ms/step - loss: 1.4639 - accuracy: 0.5648\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.0372 - accuracy: 0.6951\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8926 - accuracy: 0.7314\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8055 - accuracy: 0.7538\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7444 - accuracy: 0.7710\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6973 - accuracy: 0.7837\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6599 - accuracy: 0.7934\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6302 - accuracy: 0.8010\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6061 - accuracy: 0.8073\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5862 - accuracy: 0.8123\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5695 - accuracy: 0.8158\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5550 - accuracy: 0.8196\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5427 - accuracy: 0.8219\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5318 - accuracy: 0.8241\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5221 - accuracy: 0.8264\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5134 - accuracy: 0.8285\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5056 - accuracy: 0.8301\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4984 - accuracy: 0.8314\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4920 - accuracy: 0.8333\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4860 - accuracy: 0.8342\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4804 - accuracy: 0.8361\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4751 - accuracy: 0.8378\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4702 - accuracy: 0.8390\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4656 - accuracy: 0.8401\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4611 - accuracy: 0.8412\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4571 - accuracy: 0.8433\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4531 - accuracy: 0.8445\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4494 - accuracy: 0.8455\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4457 - accuracy: 0.8469\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4424 - accuracy: 0.8479\n",
      "188/188 [==============================] - 0s 984us/step\n",
      "15 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.3502 - accuracy: 0.1391\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5129 - accuracy: 0.0997\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5171 - accuracy: 0.1019\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5099 - accuracy: 0.0981\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5084 - accuracy: 0.1015\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5137 - accuracy: 0.0989\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5011 - accuracy: 0.1010\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5144 - accuracy: 0.1001\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5148 - accuracy: 0.1006\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5081 - accuracy: 0.0981\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5218 - accuracy: 0.1013\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5015 - accuracy: 0.1020\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5115 - accuracy: 0.0990\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5134 - accuracy: 0.1006\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5203 - accuracy: 0.0975\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5128 - accuracy: 0.0996\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5135 - accuracy: 0.0995\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5150 - accuracy: 0.1005\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5141 - accuracy: 0.1006\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5112 - accuracy: 0.1005\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5285 - accuracy: 0.0987\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5025 - accuracy: 0.0986\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5011 - accuracy: 0.1012\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5043 - accuracy: 0.0987\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5241 - accuracy: 0.0996\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5128 - accuracy: 0.0995\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5107 - accuracy: 0.1012\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5042 - accuracy: 0.1014\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5100 - accuracy: 0.1001\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.5251 - accuracy: 0.1016\n",
      "188/188 [==============================] - 0s 998us/step\n",
      "16 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6944 - accuracy: 0.7453\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6334 - accuracy: 0.7732\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6133 - accuracy: 0.7851\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6378 - accuracy: 0.7724\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6177 - accuracy: 0.7778\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6392 - accuracy: 0.7663\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6181 - accuracy: 0.7823\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6050 - accuracy: 0.7881\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5973 - accuracy: 0.7845\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6037 - accuracy: 0.7841\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6034 - accuracy: 0.7781\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6513 - accuracy: 0.7596\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6311 - accuracy: 0.7699\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6193 - accuracy: 0.7764\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6030 - accuracy: 0.7763\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5966 - accuracy: 0.7856\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6013 - accuracy: 0.7900\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5952 - accuracy: 0.7908\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6102 - accuracy: 0.7910\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6078 - accuracy: 0.7866\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5842 - accuracy: 0.7964\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6276 - accuracy: 0.7660\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5865 - accuracy: 0.7914\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6005 - accuracy: 0.7880\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5913 - accuracy: 0.7913\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5950 - accuracy: 0.7867\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5553 - accuracy: 0.8072\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5675 - accuracy: 0.7970\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6199 - accuracy: 0.7720\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5995 - accuracy: 0.7707\n",
      "188/188 [==============================] - 0s 1ms/step\n",
      "17 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5129 - accuracy: 0.8242\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3812 - accuracy: 0.8625\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3430 - accuracy: 0.8759\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3236 - accuracy: 0.8812\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3066 - accuracy: 0.8875\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2949 - accuracy: 0.8904\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2807 - accuracy: 0.8961\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2717 - accuracy: 0.8991\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2652 - accuracy: 0.9009\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2583 - accuracy: 0.9044\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2494 - accuracy: 0.9068\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2449 - accuracy: 0.9091\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2364 - accuracy: 0.9114\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2310 - accuracy: 0.9137\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2276 - accuracy: 0.9162\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2223 - accuracy: 0.9171\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2177 - accuracy: 0.9186\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2129 - accuracy: 0.9220\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2071 - accuracy: 0.9232\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2063 - accuracy: 0.9224\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2008 - accuracy: 0.9254\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1951 - accuracy: 0.9273\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1922 - accuracy: 0.9289\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1887 - accuracy: 0.9299\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1850 - accuracy: 0.9326\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1836 - accuracy: 0.9318\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1788 - accuracy: 0.9327\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1760 - accuracy: 0.9349\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1719 - accuracy: 0.9358\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1729 - accuracy: 0.9359\n",
      "188/188 [==============================] - 0s 1ms/step\n",
      "18 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6106 - accuracy: 0.7877\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4882 - accuracy: 0.8289\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4664 - accuracy: 0.8378\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4559 - accuracy: 0.8395\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4473 - accuracy: 0.8438\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4409 - accuracy: 0.8477\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4353 - accuracy: 0.8481\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4338 - accuracy: 0.8482\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4286 - accuracy: 0.8504\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4273 - accuracy: 0.8505\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4254 - accuracy: 0.8508\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4236 - accuracy: 0.8525\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4214 - accuracy: 0.8526\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4209 - accuracy: 0.8517\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4184 - accuracy: 0.8534\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4174 - accuracy: 0.8541\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4141 - accuracy: 0.8544\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4137 - accuracy: 0.8526\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4132 - accuracy: 0.8547\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4101 - accuracy: 0.8557\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4114 - accuracy: 0.8561\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4090 - accuracy: 0.8565\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4088 - accuracy: 0.8554\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4079 - accuracy: 0.8574\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4065 - accuracy: 0.8577\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4066 - accuracy: 0.8566\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4072 - accuracy: 0.8561\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4051 - accuracy: 0.8563\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4046 - accuracy: 0.8574\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4052 - accuracy: 0.8577\n",
      "188/188 [==============================] - 0s 1ms/step\n",
      "19 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6849 - accuracy: 0.7664\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5021 - accuracy: 0.8273\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4682 - accuracy: 0.8374\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4515 - accuracy: 0.8408\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4401 - accuracy: 0.8474\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4318 - accuracy: 0.8506\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4257 - accuracy: 0.8522\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4208 - accuracy: 0.8540\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4163 - accuracy: 0.8548\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4133 - accuracy: 0.8555\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4102 - accuracy: 0.8571\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4072 - accuracy: 0.8580\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4052 - accuracy: 0.8586\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4025 - accuracy: 0.8600\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4004 - accuracy: 0.8600\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3989 - accuracy: 0.8609\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3961 - accuracy: 0.8618\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3957 - accuracy: 0.8597\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3932 - accuracy: 0.8622\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3911 - accuracy: 0.8627\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3907 - accuracy: 0.8624\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3897 - accuracy: 0.8631\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3884 - accuracy: 0.8636\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3872 - accuracy: 0.8652\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3858 - accuracy: 0.8658\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3855 - accuracy: 0.8651\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3841 - accuracy: 0.8652\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3827 - accuracy: 0.8658\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3825 - accuracy: 0.8655\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.3815 - accuracy: 0.8655\n",
      "188/188 [==============================] - 0s 1ms/step\n",
      "20 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 3s 2ms/step - loss: 1.2039 - accuracy: 0.6052\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.7999 - accuracy: 0.7228\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.7025 - accuracy: 0.7583\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6475 - accuracy: 0.7784\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6109 - accuracy: 0.7930\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5838 - accuracy: 0.8023\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5632 - accuracy: 0.8100\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5464 - accuracy: 0.8150\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5328 - accuracy: 0.8193\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5216 - accuracy: 0.8221\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5118 - accuracy: 0.8259\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5031 - accuracy: 0.8284\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4960 - accuracy: 0.8307\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4895 - accuracy: 0.8328\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4838 - accuracy: 0.8339\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4784 - accuracy: 0.8358\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4737 - accuracy: 0.8376\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4694 - accuracy: 0.8382\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4655 - accuracy: 0.8401\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4620 - accuracy: 0.8396\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4587 - accuracy: 0.8418\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4556 - accuracy: 0.8421\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4528 - accuracy: 0.8438\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4503 - accuracy: 0.8441\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4476 - accuracy: 0.8458\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4454 - accuracy: 0.8461\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4431 - accuracy: 0.8461\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4411 - accuracy: 0.8471\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4392 - accuracy: 0.8488\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4374 - accuracy: 0.8493\n",
      "188/188 [==============================] - 0s 1ms/step\n",
      "21 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 3s 2ms/step - loss: 229.7746 - accuracy: 0.7259\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 33.0447 - accuracy: 0.7429\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 6.8981 - accuracy: 0.7438\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 320.1916 - accuracy: 0.7269\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 38.8116 - accuracy: 0.7514\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 12.9391 - accuracy: 0.7413\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 369.7435 - accuracy: 0.7158\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 27.4467 - accuracy: 0.7638\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 16.3582 - accuracy: 0.7440\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 395.8207 - accuracy: 0.7239\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 23.5214 - accuracy: 0.7606\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 17.7860 - accuracy: 0.7466\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 444.9901 - accuracy: 0.7321\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 22.2031 - accuracy: 0.7655\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 17.2978 - accuracy: 0.7457\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 353.9337 - accuracy: 0.7415\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 18.7875 - accuracy: 0.7639\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 332.0373 - accuracy: 0.7252\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 51.0065 - accuracy: 0.7675\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 20.5281 - accuracy: 0.7513\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 381.1170 - accuracy: 0.7370\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 26.1784 - accuracy: 0.7693\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 26.4121 - accuracy: 0.7452\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 292.9041 - accuracy: 0.7532\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 23.6111 - accuracy: 0.7558\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 217.9507 - accuracy: 0.7392\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 20.2442 - accuracy: 0.7553\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 225.6747 - accuracy: 0.7485\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 18.0728 - accuracy: 0.7597\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 382.3825 - accuracy: 0.7448\n",
      "188/188 [==============================] - 0s 1ms/step\n",
      "22 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6639 - accuracy: 0.7843\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5769 - accuracy: 0.8064\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5539 - accuracy: 0.8126\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5460 - accuracy: 0.8163\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5544 - accuracy: 0.8166\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5438 - accuracy: 0.8208\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5364 - accuracy: 0.8216\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5471 - accuracy: 0.8224\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5088 - accuracy: 0.8287\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5667 - accuracy: 0.8250\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5215 - accuracy: 0.8247\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5256 - accuracy: 0.8248\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5340 - accuracy: 0.8219\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5201 - accuracy: 0.8284\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5427 - accuracy: 0.8229\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5198 - accuracy: 0.8269\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5286 - accuracy: 0.8236\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5247 - accuracy: 0.8265\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5203 - accuracy: 0.8263\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5270 - accuracy: 0.8273\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5236 - accuracy: 0.8252\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5200 - accuracy: 0.8281\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5156 - accuracy: 0.8301\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5232 - accuracy: 0.8288\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5193 - accuracy: 0.8283\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5254 - accuracy: 0.8268\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5281 - accuracy: 0.8271\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5077 - accuracy: 0.8313\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5261 - accuracy: 0.8272\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5164 - accuracy: 0.8302\n",
      "188/188 [==============================] - 0s 1ms/step\n",
      "23 out of 24 finished.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5570 - accuracy: 0.8055\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4662 - accuracy: 0.8362\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4464 - accuracy: 0.8443\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4363 - accuracy: 0.8466\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4297 - accuracy: 0.8497\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4222 - accuracy: 0.8523\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4182 - accuracy: 0.8526\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4111 - accuracy: 0.8558\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4085 - accuracy: 0.8555\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4090 - accuracy: 0.8559\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4043 - accuracy: 0.8581\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4013 - accuracy: 0.8595\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3983 - accuracy: 0.8601\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3961 - accuracy: 0.8603\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3944 - accuracy: 0.8609\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3935 - accuracy: 0.8616\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3910 - accuracy: 0.8617\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3895 - accuracy: 0.8615\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3882 - accuracy: 0.8641\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3879 - accuracy: 0.8629\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3870 - accuracy: 0.8628\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3857 - accuracy: 0.8631\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3843 - accuracy: 0.8649\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3852 - accuracy: 0.8639\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3831 - accuracy: 0.8646\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3815 - accuracy: 0.8646\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3800 - accuracy: 0.8650\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3795 - accuracy: 0.8658\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3780 - accuracy: 0.8666\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3783 - accuracy: 0.8662\n",
      "188/188 [==============================] - 0s 892us/step\n",
      "24 out of 24 finished.\n"
     ]
    }
   ],
   "source": [
    "# Running in arround 1800s\n",
    "\n",
    "mlp_result = get_result(keras_classifier, mlp_paras, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c82c50e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for MLP:\n",
      "Best parameters: {'activation_function': 'sigmoid', 'optimizer': 'Adam', 'optimizer__lr': 0.001}\n",
      "Best validation score: 0.8933\n",
      "313/313 [==============================] - 0s 869us/step\n",
      "Test set score: 0.8850\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>training_time</th>\n",
       "      <th>validation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'relu', 'optimizer': 'sgd', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.8733</td>\n",
       "      <td>60.99</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'relu', 'optimizer': 'sgd', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.8835</td>\n",
       "      <td>59.75</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'relu', 'optimizer': 'sgd', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.8468</td>\n",
       "      <td>58.45</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'relu', 'optimizer': 'Adam', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.197</td>\n",
       "      <td>60.95</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'relu', 'optimizer': 'Adam', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.8617</td>\n",
       "      <td>145.03</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'relu', 'optimizer': 'Adam', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.8928</td>\n",
       "      <td>69.17</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'sigmoid', 'optimizer': 'sgd', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.8803</td>\n",
       "      <td>60.62</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'sigmoid', 'optimizer': 'sgd', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.8418</td>\n",
       "      <td>61.28</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'sigmoid', 'optimizer': 'sgd', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.6603</td>\n",
       "      <td>62.96</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'sigmoid', 'optimizer': 'Adam', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.3617</td>\n",
       "      <td>67.84</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'sigmoid', 'optimizer': 'Adam', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.85</td>\n",
       "      <td>67.23</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'sigmoid', 'optimizer': 'Adam', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.8933</td>\n",
       "      <td>67.8</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'tanh', 'optimizer': 'sgd', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.891</td>\n",
       "      <td>60.8</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'tanh', 'optimizer': 'sgd', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.8807</td>\n",
       "      <td>60.11</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'tanh', 'optimizer': 'sgd', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.8457</td>\n",
       "      <td>64.16</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'tanh', 'optimizer': 'Adam', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.1</td>\n",
       "      <td>67.04</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'tanh', 'optimizer': 'Adam', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.8133</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'tanh', 'optimizer': 'Adam', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.888</td>\n",
       "      <td>94.75</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': None, 'optimizer': 'sgd', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.8407</td>\n",
       "      <td>86.99</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': None, 'optimizer': 'sgd', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.8532</td>\n",
       "      <td>86.68</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': None, 'optimizer': 'sgd', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.8473</td>\n",
       "      <td>86.12</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': None, 'optimizer': 'Adam', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.7423</td>\n",
       "      <td>90.82</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': None, 'optimizer': 'Adam', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.8363</td>\n",
       "      <td>90.99</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': None, 'optimizer': 'Adam', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.848</td>\n",
       "      <td>69.84</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Score training_time  \\\n",
       "{'activation_function': 'relu', 'optimizer': 's...  0.8733         60.99   \n",
       "{'activation_function': 'relu', 'optimizer': 's...  0.8835         59.75   \n",
       "{'activation_function': 'relu', 'optimizer': 's...  0.8468         58.45   \n",
       "{'activation_function': 'relu', 'optimizer': 'A...   0.197         60.95   \n",
       "{'activation_function': 'relu', 'optimizer': 'A...  0.8617        145.03   \n",
       "{'activation_function': 'relu', 'optimizer': 'A...  0.8928         69.17   \n",
       "{'activation_function': 'sigmoid', 'optimizer':...  0.8803         60.62   \n",
       "{'activation_function': 'sigmoid', 'optimizer':...  0.8418         61.28   \n",
       "{'activation_function': 'sigmoid', 'optimizer':...  0.6603         62.96   \n",
       "{'activation_function': 'sigmoid', 'optimizer':...  0.3617         67.84   \n",
       "{'activation_function': 'sigmoid', 'optimizer':...    0.85         67.23   \n",
       "{'activation_function': 'sigmoid', 'optimizer':...  0.8933          67.8   \n",
       "{'activation_function': 'tanh', 'optimizer': 's...   0.891          60.8   \n",
       "{'activation_function': 'tanh', 'optimizer': 's...  0.8807         60.11   \n",
       "{'activation_function': 'tanh', 'optimizer': 's...  0.8457         64.16   \n",
       "{'activation_function': 'tanh', 'optimizer': 'A...     0.1         67.04   \n",
       "{'activation_function': 'tanh', 'optimizer': 'A...  0.8133          82.2   \n",
       "{'activation_function': 'tanh', 'optimizer': 'A...   0.888         94.75   \n",
       "{'activation_function': None, 'optimizer': 'sgd...  0.8407         86.99   \n",
       "{'activation_function': None, 'optimizer': 'sgd...  0.8532         86.68   \n",
       "{'activation_function': None, 'optimizer': 'sgd...  0.8473         86.12   \n",
       "{'activation_function': None, 'optimizer': 'Ada...  0.7423         90.82   \n",
       "{'activation_function': None, 'optimizer': 'Ada...  0.8363         90.99   \n",
       "{'activation_function': None, 'optimizer': 'Ada...   0.848         69.84   \n",
       "\n",
       "                                                   validation_time  \n",
       "{'activation_function': 'relu', 'optimizer': 's...            0.34  \n",
       "{'activation_function': 'relu', 'optimizer': 's...            0.29  \n",
       "{'activation_function': 'relu', 'optimizer': 's...            0.29  \n",
       "{'activation_function': 'relu', 'optimizer': 'A...            0.28  \n",
       "{'activation_function': 'relu', 'optimizer': 'A...            0.31  \n",
       "{'activation_function': 'relu', 'optimizer': 'A...            0.32  \n",
       "{'activation_function': 'sigmoid', 'optimizer':...            0.31  \n",
       "{'activation_function': 'sigmoid', 'optimizer':...            0.31  \n",
       "{'activation_function': 'sigmoid', 'optimizer':...            0.32  \n",
       "{'activation_function': 'sigmoid', 'optimizer':...            0.31  \n",
       "{'activation_function': 'sigmoid', 'optimizer':...            0.33  \n",
       "{'activation_function': 'sigmoid', 'optimizer':...            0.31  \n",
       "{'activation_function': 'tanh', 'optimizer': 's...            0.31  \n",
       "{'activation_function': 'tanh', 'optimizer': 's...             0.3  \n",
       "{'activation_function': 'tanh', 'optimizer': 's...            0.32  \n",
       "{'activation_function': 'tanh', 'optimizer': 'A...            0.31  \n",
       "{'activation_function': 'tanh', 'optimizer': 'A...            0.43  \n",
       "{'activation_function': 'tanh', 'optimizer': 'A...            0.42  \n",
       "{'activation_function': None, 'optimizer': 'sgd...            0.45  \n",
       "{'activation_function': None, 'optimizer': 'sgd...            0.41  \n",
       "{'activation_function': None, 'optimizer': 'sgd...             0.4  \n",
       "{'activation_function': None, 'optimizer': 'Ada...            0.41  \n",
       "{'activation_function': None, 'optimizer': 'Ada...            0.43  \n",
       "{'activation_function': None, 'optimizer': 'Ada...            0.31  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_results(\"MLP\", mlp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098737b",
   "metadata": {},
   "source": [
    "From the table above, both {'activation_function': 'relu', 'optimizer': 'Adam', 'optimizer__lr': 0.001} and {'activation_function': 'sigmoid', 'optimizer': 'Adam', 'optimizer__lr': 0.001} are good. Considering the limitations of Sigmoid activation function, we choose **ReLU** in this study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb5d694",
   "metadata": {},
   "source": [
    "Then we observe the trend of epochs with these paras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "188af150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_8 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 20)                2020      \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 10)                210       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,730\n",
      "Trainable params: 80,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build a model with paras we just chose\n",
    "mlp_model = build_mlp(activation_function=\"relu\")\n",
    "\n",
    "mlp_model.summary()\n",
    "\n",
    "mlp_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc3a90a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0984 - accuracy: 0.9628 - val_loss: 0.4898 - val_accuracy: 0.8860\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.0988 - accuracy: 0.9640 - val_loss: 0.5211 - val_accuracy: 0.8898\n",
      "Epoch 3/30\n",
      " 899/1688 [==============>...............] - ETA: 0s - loss: 0.0904 - accuracy: 0.9667"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\1\\Desktop\\AS2\\a2-490576560-520653377.ipynb Cell 58\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/1/Desktop/AS2/a2-490576560-520653377.ipynb#Y123sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mlp_history \u001b[39m=\u001b[39m mlp_model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_valid, y_valid))\n",
      "File \u001b[1;32mc:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\engine\\training.py:1555\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   1552\u001b[0m     data_handler\u001b[39m.\u001b[39m_initial_step \u001b[39m=\u001b[39m data_handler\u001b[39m.\u001b[39m_initial_step \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   1553\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_load_initial_step_from_ckpt()\n\u001b[0;32m   1554\u001b[0m     )\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   1556\u001b[0m         \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m             epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m             _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m         ):\n\u001b[0;32m   1563\u001b[0m             callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[1;32mc:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:1374\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1374\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1375\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[0;32m   1376\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1377\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1378\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m original_spe\n\u001b[0;32m   1379\u001b[0m )\n\u001b[0;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32mc:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:637\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    636\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 637\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    638\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    639\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:728\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    725\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_variable_op()\n\u001b[0;32m    726\u001b[0m \u001b[39m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[1;32m--> 728\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39;49midentity(value)\n",
      "File \u001b[1;32mc:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:294\u001b[0m, in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39minput\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgraph\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    291\u001b[0m   \u001b[39m# Make sure we get an input with handle data attached from resource\u001b[39;00m\n\u001b[0;32m    292\u001b[0m   \u001b[39m# variables. Variables have correct handle data when graph building.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m   \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\u001b[39minput\u001b[39m)\n\u001b[1;32m--> 294\u001b[0m ret \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49midentity(\u001b[39minput\u001b[39;49m, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    295\u001b[0m \u001b[39m# Propagate handle data for happier shape inference for resource variables.\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39minput\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_handle_data\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\1\\PycharmProjects\\5318\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:4068\u001b[0m, in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   4066\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   4067\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 4068\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   4069\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mIdentity\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39minput\u001b[39;49m)\n\u001b[0;32m   4070\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   4071\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp_history = mlp_model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b64a0961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the history dictionary to a Pandas dataframe and extract the accuracies\n",
    "accuracies = pd.DataFrame(mlp_history.history)[['accuracy', 'val_accuracy']]\n",
    "\n",
    "# Plot the accuracies\n",
    "accuracies.plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0.8, 1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3dd083",
   "metadata": {},
   "source": [
    "The accuracy of validation set fluctuated since epoch = 10, thus, we set the epochs = 10 to keep it small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba1bd8d",
   "metadata": {},
   "source": [
    "Best paras for K-nearest neighbors is:  \n",
    "'n_neighbors'= 3, 'p'= 1, 'weights'= 'distance'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa9ad490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3, p=1, weights=&#x27;distance&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3, p=1, weights=&#x27;distance&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3, p=1, weights='distance')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the best paras\n",
    "knn_best_paras = dict({'n_neighbors': 3, 'p': 1, 'weights': 'distance'})\n",
    "\n",
    "knn = KNeighborsClassifier(**knn_best_paras)\n",
    "knn_runtime = time.time()\n",
    "knn.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "knn_runtime = time.time - knn_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f9cda9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN score on the test set: 0.8597\n"
     ]
    }
   ],
   "source": [
    "# Running in around 120s\n",
    "\n",
    "# Performance on test set.\n",
    "print(f\"KNN training time: {knn_runtime:.2f} s\")\n",
    "print(f\"KNN score on the test set: {knn.score(X_test.reshape(X_test.shape[0], -1), y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Fully connected neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000e22b",
   "metadata": {},
   "source": [
    "We settled the size of layers (784, 100, 20, 10) in section 2.2; and settled the the best paras of our experimental settings:   \n",
    "'activation_function': 'relu', 'optimizer': 'Adam', 'optimizer__lr': 0.001   \n",
    "epochs: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bab3c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5286 - accuracy: 0.8144 - val_loss: 0.3980 - val_accuracy: 0.8595\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3896 - accuracy: 0.8607 - val_loss: 0.3539 - val_accuracy: 0.8732\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3507 - accuracy: 0.8726 - val_loss: 0.3427 - val_accuracy: 0.8762\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3293 - accuracy: 0.8808 - val_loss: 0.3214 - val_accuracy: 0.8792\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3096 - accuracy: 0.8865 - val_loss: 0.3266 - val_accuracy: 0.8782\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2965 - accuracy: 0.8909 - val_loss: 0.3249 - val_accuracy: 0.8803\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2823 - accuracy: 0.8956 - val_loss: 0.3115 - val_accuracy: 0.8852\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.2690 - accuracy: 0.9003 - val_loss: 0.3251 - val_accuracy: 0.8802\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2602 - accuracy: 0.9025 - val_loss: 0.3251 - val_accuracy: 0.8822\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2536 - accuracy: 0.9056 - val_loss: 0.2990 - val_accuracy: 0.8922\n"
     ]
    }
   ],
   "source": [
    "# Running in around 25s\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Build the final model\n",
    "mlp = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=IMAGE_SIZE),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(20, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# mlp.summary()\n",
    "\n",
    "# Complie the model\n",
    "mlp.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "mlp_runtime = time.time()\n",
    "mlp.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), verbose=1)\n",
    "mlp_runtime = time.time() - mlp_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae075211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.8824\n",
      "MLP runtime: 24.77 s\n",
      "MLP score on the test set: 0.8824\n"
     ]
    }
   ],
   "source": [
    "# Performance on test set.\n",
    "loss, accuracy = mlp.evaluate(X_test, y_test)\n",
    "print(f\"MLP training time: {mlp_runtime:.2f} s\")\n",
    "print(f\"MLP score on the test set: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a8c4580c33f1a0ec11d1ec1ec999c7eec26eef61726f4404092dec9f099378c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
