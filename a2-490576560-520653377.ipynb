{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 2: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group number: 100, SID1: 490576560, SID2: 520653377 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and dependencies\n",
    "All the required libraries/dependencies and the plotting environment are listed and set up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Make the notebook's output stable across runs.\n",
    "# Random seed is set to 0 consistently.\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading, preprocessing, and exploration\n",
    "The documentation for the data loading function can be accessed [here](https://keras.io/api/datasets/fashion_mnist/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68db559",
   "metadata": {},
   "source": [
    "### 1.1 Load data and declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion-MNIST dataset training and test sets as numpy arrays\n",
    "(X_train_original, y_train_original), (X_test_original, y_test_original) = keras.datasets.fashion_mnist.load_data()\n",
    "assert X_train_original.shape == (60000, 28, 28)\n",
    "assert X_test_original.shape == (10000, 28, 28)\n",
    "assert y_train_original.shape == (60000,)\n",
    "assert y_test_original.shape == (10000,)\n",
    "\n",
    "# An ordered list of the class names\n",
    "class_names = [\"T-shirt/top\",\n",
    "               \"Trouser\",\n",
    "               \"Pullover\",\n",
    "               \"Dress\",\n",
    "               \"Coat\",\n",
    "               \"Sandal\",\n",
    "               \"Shirt\",\n",
    "               \"Sneaker\",\n",
    "               \"Bag\",\n",
    "               \"Ankle boot\"\n",
    "              ]\n",
    "\n",
    "# Declare size of the image\n",
    "IMAGE_SIZE = X_train_original[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed0594",
   "metadata": {},
   "source": [
    "### 1.2 Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a23dbff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise data\n",
    "X_train_full = X_train_original.reshape(X_train_original.shape[0], -1) # Flatten data from 3D to 2D\n",
    "y_train_full = y_train_original.copy()\n",
    "X_test = X_test_original.reshape(X_test_original.shape[0], -1) # Flatten data from 3D to 2D\n",
    "y_test = y_test_original.copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_full)\n",
    "X_train_full = scaler.transform(X_train_full) # apply normalisation to the training set\n",
    "X_test = scaler.transform(X_test) # apply normalisation to the test set\n",
    "\n",
    "X_train_full = X_train_full.reshape(X_train_original.shape[0], *IMAGE_SIZE) # restore the dimention from 2D to 3D\n",
    "X_test = X_test.reshape(X_test_original.shape[0], *IMAGE_SIZE) # restore the dimention from 2D to 3D\n",
    "\n",
    "# Create validation set from the training set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, train_size=0.9, stratify=y_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b1fe80",
   "metadata": {},
   "source": [
    "### 1.3 Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccfef24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original training set is 60000 images with (28, 28) pixels, without normalization (uint8).\n",
      "The original test set is 10000 images with (28, 28) pixels, without normalization (uint8).\n",
      "\n",
      "The size of training set is 54000 (float64), the size of validation set is 6000 (float64), andthe size of test set is 10000 (float64)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The original training set is {X_train_original.shape[0]} images with {X_train_original[0].shape} pixels, \\\n",
    "without normalization ({X_train_original.dtype}).\")\n",
    "print(f\"The original test set is {X_test_original.shape[0]} images with {X_test_original[0].shape} pixels, \\\n",
    "without normalization ({X_test_original.dtype}).\\n\")\n",
    "\n",
    "print(f\"The size of training set is {X_train.shape[0]} ({X_train.dtype}), \\\n",
    "the size of validation set is {X_valid.shape[0]} ({X_valid.dtype}), and\\\n",
    "the size of test set is {X_test.shape[0]} ({X_test.dtype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c836c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 different classes: [0 1 2 3 4 5 6 7 8 9]\n",
      "The label distribution of training set is [5400, 5400, 5400, 5400, 5400, 5400, 5400, 5400, 5400, 5400]\n",
      "The label distribution of validation set is [600, 600, 600, 600, 600, 600, 600, 600, 600, 600]\n",
      "The label distribution of test set is [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n"
     ]
    }
   ],
   "source": [
    "def show_distribution(y):\n",
    "    \"\"\"Simple way to show a label distribution.\"\"\"\n",
    "    result = []\n",
    "    for i in range(len(class_names)):\n",
    "        result.append((y == i).sum())\n",
    "    return result\n",
    "\n",
    "print(f\"There are {len(set(y_train_original))} different classes: {np.unique(y_train_original)}\")\n",
    "print(f\"The label distribution of training set is {show_distribution(y_train)}\")\n",
    "print(f\"The label distribution of validation set is {show_distribution(y_valid)}\")\n",
    "print(f\"The label distribution of test set is {show_distribution(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44772aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_examples(X, y, title=\"\"):\n",
    "    \"\"\"Plot a grid of images from different classes.\"\"\"\n",
    "    # Size figure depending on the size of the grid\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.suptitle(title, fontsize=16,x=0.5,y=1.2,)\n",
    "\n",
    "    index = []\n",
    "    # search index\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(y)):\n",
    "            if i == y[j]:\n",
    "                index.append(j)\n",
    "                break\n",
    "\n",
    "    # Plot the image at appropriate place in grid\n",
    "    for i in range(len(index)):\n",
    "        plt.subplot(1, len(index), i + 1)\n",
    "        plt.imshow(X[index[i]], cmap=\"binary\")\n",
    "        plt.title(class_names[y[index[i]]])\n",
    "        plt.axis('off')\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "print(\"Some examples:\")\n",
    "\n",
    "plot_examples(X_train, y_train, \"Examples in training set\")\n",
    "plot_examples(X_valid, y_valid, \"Examples in validation set\")\n",
    "plot_examples(X_test, y_test, \"Examples in test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm design and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4636e21",
   "metadata": {},
   "source": [
    "First, A group of simple algorithms from the first 6 weeks are compared. They are: K-nearest neighbors, Naive Bayes, Decision tree, and Random forest. We simply train the model with default/simple parameters on the full training set, and test their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f11273",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=10) # k shoule be less than sqrt(#training_examples), commercial packages typically use k=10\n",
    "\n",
    "# Training and timer\n",
    "time_stamp = time.time()\n",
    "neigh.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "neigh_training_time = time.time() - time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0ec567",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "\n",
    "# Training and timer\n",
    "time_stamp = time.time()\n",
    "nb.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "nb_training_time = time.time() - time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a297035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running in arround 30s\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', random_state=0) # without setting max_depth results in overfitting.\n",
    "\n",
    "# Training and timer\n",
    "time_stamp = time.time()\n",
    "tree.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "tree_training_time = time.time() - time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4811836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running in arround 90s\n",
    "\n",
    "rnd = RandomForestClassifier(criterion='entropy', random_state=0) # n_estimators=100 by default\n",
    "\n",
    "# Training and timer\n",
    "time_stamp = time.time()\n",
    "rnd.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "rnd_training_time = time.time() - time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd9994d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of KNN is 0.8519, training time is 0.05 s.\n",
      "The accuracy of NB is 0.5838, training time is 0.54 s.\n",
      "The accuracy of DT is 0.8001, training time is 28.93 s.\n",
      "The accuracy of RF is 0.8760, training time is 78.00 s.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The accuracy of KNN is {neigh.score(X_test.reshape(X_test.shape[0], -1), y_test):.4f}, training time is {neigh_training_time:.2f} s.\")\n",
    "print(f\"The accuracy of NB is {nb.score(X_test.reshape(X_test.shape[0], -1), y_test):.4f}, training time is {nb_training_time:.2f} s.\")\n",
    "print(f\"The accuracy of DT is {tree.score(X_test.reshape(X_test.shape[0], -1), y_test):.4f}, training time is {tree_training_time:.2f} s.\")\n",
    "print(f\"The accuracy of RF is {rnd.score(X_test.reshape(X_test.shape[0], -1), y_test):.4f}, training time is {rnd_training_time:.2f} s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0cf2aa",
   "metadata": {},
   "source": [
    "Althouth Random Forest performs best, the training time is relatively long. Noticing that KNN is simple but with a content accuracy among them, the training time is  also tiny. Therefore, KNN is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fully connected neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6912a6",
   "metadata": {},
   "source": [
    "First, the numbers of layers need to be settled. Apart from the **input layer** and **output layer**, the numebr of hidden layer can be a variable. According to Cybenko(1998), any function (including discontinuous) can be approximated to arbitrary small error by a network with two hidden layers. To make the model small, we choose **two hidden layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba5c8d",
   "metadata": {},
   "source": [
    "Number of neurons in the input layer: 784   \n",
    "For numerical attributes, basically 1 neuron per attribute, in this dataset, we have 28 * 28 = 784 atttributes each example. Thus, the number of neurons of input layer should be 784. Simply, we just use **keras.layers.Flatten(input_shape)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3925d",
   "metadata": {},
   "source": [
    "Number of neurons in the output layer: 10   \n",
    "1 for each class. Therefore, the number of the output layers should be 10. The **softmax** function ($\\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}}$) converts the raw outputs of this layer into a probability distribution over the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c2c8c",
   "metadata": {},
   "source": [
    "Now we choose the hidden layers.   \n",
    "**Sigmoid** is the most widely used transfer function.\n",
    "We simply set most paras by default as well as the basic **SGD** learning algorithm. Since our labels are in index form rather than encoded as one-hot vectors, as we discussed earlier, we utilise the **sparse_categorical_crossentropy** loss. Then we observe the trend of the numbers of neurons with respect to MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8076c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_build_mlp(num1=50, num2=50):\n",
    "    \"\"\"Build the MLP model with the specified number of neurons.\"\"\"\n",
    "    # Set Random seed to 0\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "    # Define a test MLP model\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=IMAGE_SIZE),\n",
    "        keras.layers.Dense(num1, activation=\"sigmoid\", kernel_initializer=initializer),\n",
    "        keras.layers.Dense(num2, activation=\"sigmoid\", kernel_initializer=initializer),\n",
    "        keras.layers.Dense(len(class_names), activation=\"softmax\", kernel_initializer=initializer)\n",
    "    ])\n",
    "\n",
    "    opt = keras.optimizers.SGD() # default learning_rate=0.01\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # model.summary()\n",
    "    return model\n",
    "\n",
    "def test_train_mlp(num1, num2, max_epochs=50, criterion=0.02):\n",
    "    \"\"\"Training the model.\n",
    "    max_epochs: the maximum number of epochs to terminate.\n",
    "    criterion: stop when the difference between the loss of the last 5 epoch is less than.\"\"\"\n",
    "    # Train the classifier.\n",
    "    mlp = test_build_mlp(num1, num2)\n",
    "    loss_list = []\n",
    "    for i in range(max_epochs):\n",
    "        history = mlp.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=1)\n",
    "        loss_list.append(history.history[\"loss\"][0])\n",
    "\n",
    "        # Stop condition\n",
    "        if len(loss_list) > 5 and loss_list[-6] - loss_list[-1] < criterion:\n",
    "            print(len(loss_list))\n",
    "            break\n",
    "    \n",
    "    return loss_list[-1], len(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6466b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.0764 - accuracy: 0.4214 - val_loss: 1.7342 - val_accuracy: 0.5058\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4601 - accuracy: 0.5927 - val_loss: 1.2573 - val_accuracy: 0.6330\n",
      "1688/1688 [==============================] - 2s 995us/step - loss: 1.1359 - accuracy: 0.6638 - val_loss: 1.0339 - val_accuracy: 0.6807\n",
      "1688/1688 [==============================] - 2s 994us/step - loss: 0.9574 - accuracy: 0.6956 - val_loss: 0.8923 - val_accuracy: 0.7097\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8384 - accuracy: 0.7185 - val_loss: 0.7962 - val_accuracy: 0.7290\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7582 - accuracy: 0.7356 - val_loss: 0.7316 - val_accuracy: 0.7415\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7038 - accuracy: 0.7480 - val_loss: 0.6869 - val_accuracy: 0.7547\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6652 - accuracy: 0.7582 - val_loss: 0.6541 - val_accuracy: 0.7613\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6357 - accuracy: 0.7669 - val_loss: 0.6280 - val_accuracy: 0.7683\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6115 - accuracy: 0.7766 - val_loss: 0.6060 - val_accuracy: 0.7760\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5906 - accuracy: 0.7858 - val_loss: 0.5866 - val_accuracy: 0.7842\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5720 - accuracy: 0.7937 - val_loss: 0.5691 - val_accuracy: 0.7918\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5552 - accuracy: 0.8006 - val_loss: 0.5531 - val_accuracy: 0.7987\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5400 - accuracy: 0.8083 - val_loss: 0.5388 - val_accuracy: 0.8080\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5265 - accuracy: 0.8136 - val_loss: 0.5260 - val_accuracy: 0.8110\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5145 - accuracy: 0.8180 - val_loss: 0.5147 - val_accuracy: 0.8147\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5039 - accuracy: 0.8226 - val_loss: 0.5046 - val_accuracy: 0.8168\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4945 - accuracy: 0.8259 - val_loss: 0.4956 - val_accuracy: 0.8202\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4862 - accuracy: 0.8286 - val_loss: 0.4876 - val_accuracy: 0.8240\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4786 - accuracy: 0.8315 - val_loss: 0.4803 - val_accuracy: 0.8270\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4718 - accuracy: 0.8339 - val_loss: 0.4737 - val_accuracy: 0.8288\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4656 - accuracy: 0.8361 - val_loss: 0.4676 - val_accuracy: 0.8335\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4599 - accuracy: 0.8381 - val_loss: 0.4620 - val_accuracy: 0.8353\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4546 - accuracy: 0.8399 - val_loss: 0.4568 - val_accuracy: 0.8367\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4497 - accuracy: 0.8415 - val_loss: 0.4520 - val_accuracy: 0.8390\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4450 - accuracy: 0.8432 - val_loss: 0.4475 - val_accuracy: 0.8402\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4407 - accuracy: 0.8445 - val_loss: 0.4433 - val_accuracy: 0.8422\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4367 - accuracy: 0.8461 - val_loss: 0.4394 - val_accuracy: 0.8427\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4328 - accuracy: 0.8472 - val_loss: 0.4357 - val_accuracy: 0.8448\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4292 - accuracy: 0.8487 - val_loss: 0.4322 - val_accuracy: 0.8452\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4257 - accuracy: 0.8496 - val_loss: 0.4290 - val_accuracy: 0.8463\n",
      "31\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.9981 - accuracy: 0.4755 - val_loss: 1.6055 - val_accuracy: 0.5913\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.3420 - accuracy: 0.6401 - val_loss: 1.1452 - val_accuracy: 0.6605\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.0257 - accuracy: 0.6861 - val_loss: 0.9321 - val_accuracy: 0.6963\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8658 - accuracy: 0.7105 - val_loss: 0.8156 - val_accuracy: 0.7187\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7734 - accuracy: 0.7275 - val_loss: 0.7435 - val_accuracy: 0.7348\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7139 - accuracy: 0.7414 - val_loss: 0.6949 - val_accuracy: 0.7455\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6727 - accuracy: 0.7525 - val_loss: 0.6600 - val_accuracy: 0.7552\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6419 - accuracy: 0.7624 - val_loss: 0.6329 - val_accuracy: 0.7663\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6170 - accuracy: 0.7723 - val_loss: 0.6102 - val_accuracy: 0.7773\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5957 - accuracy: 0.7833 - val_loss: 0.5903 - val_accuracy: 0.7863\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5767 - accuracy: 0.7928 - val_loss: 0.5724 - val_accuracy: 0.7933\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5596 - accuracy: 0.8006 - val_loss: 0.5563 - val_accuracy: 0.7975\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5443 - accuracy: 0.8072 - val_loss: 0.5419 - val_accuracy: 0.8037\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5306 - accuracy: 0.8125 - val_loss: 0.5289 - val_accuracy: 0.8103\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5184 - accuracy: 0.8173 - val_loss: 0.5173 - val_accuracy: 0.8148\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5076 - accuracy: 0.8215 - val_loss: 0.5070 - val_accuracy: 0.8198\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4980 - accuracy: 0.8250 - val_loss: 0.4978 - val_accuracy: 0.8212\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4895 - accuracy: 0.8277 - val_loss: 0.4896 - val_accuracy: 0.8237\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4818 - accuracy: 0.8307 - val_loss: 0.4822 - val_accuracy: 0.8253\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4749 - accuracy: 0.8332 - val_loss: 0.4754 - val_accuracy: 0.8283\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4686 - accuracy: 0.8353 - val_loss: 0.4693 - val_accuracy: 0.8313\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4629 - accuracy: 0.8379 - val_loss: 0.4636 - val_accuracy: 0.8330\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4576 - accuracy: 0.8404 - val_loss: 0.4584 - val_accuracy: 0.8333\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4527 - accuracy: 0.8416 - val_loss: 0.4536 - val_accuracy: 0.8358\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4482 - accuracy: 0.8433 - val_loss: 0.4491 - val_accuracy: 0.8385\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4439 - accuracy: 0.8447 - val_loss: 0.4450 - val_accuracy: 0.8402\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4400 - accuracy: 0.8461 - val_loss: 0.4411 - val_accuracy: 0.8407\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4362 - accuracy: 0.8470 - val_loss: 0.4375 - val_accuracy: 0.8413\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4327 - accuracy: 0.8480 - val_loss: 0.4341 - val_accuracy: 0.8425\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4294 - accuracy: 0.8491 - val_loss: 0.4309 - val_accuracy: 0.8438\n",
      "30\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 1.9690 - accuracy: 0.4764 - val_loss: 1.5522 - val_accuracy: 0.5837\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2944 - accuracy: 0.6431 - val_loss: 1.1066 - val_accuracy: 0.6697\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.9952 - accuracy: 0.6926 - val_loss: 0.9086 - val_accuracy: 0.7040\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8437 - accuracy: 0.7182 - val_loss: 0.7964 - val_accuracy: 0.7295\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7535 - accuracy: 0.7358 - val_loss: 0.7265 - val_accuracy: 0.7422\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6964 - accuracy: 0.7488 - val_loss: 0.6811 - val_accuracy: 0.7530\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6581 - accuracy: 0.7598 - val_loss: 0.6488 - val_accuracy: 0.7628\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6296 - accuracy: 0.7699 - val_loss: 0.6234 - val_accuracy: 0.7740\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.6064 - accuracy: 0.7790 - val_loss: 0.6020 - val_accuracy: 0.7810\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5864 - accuracy: 0.7870 - val_loss: 0.5831 - val_accuracy: 0.7903\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5688 - accuracy: 0.7954 - val_loss: 0.5662 - val_accuracy: 0.7962\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5529 - accuracy: 0.8028 - val_loss: 0.5511 - val_accuracy: 0.8022\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5388 - accuracy: 0.8083 - val_loss: 0.5375 - val_accuracy: 0.8065\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5262 - accuracy: 0.8138 - val_loss: 0.5253 - val_accuracy: 0.8108\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.5150 - accuracy: 0.8180 - val_loss: 0.5146 - val_accuracy: 0.8150\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5051 - accuracy: 0.8214 - val_loss: 0.5050 - val_accuracy: 0.8183\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4963 - accuracy: 0.8248 - val_loss: 0.4964 - val_accuracy: 0.8205\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.4885 - accuracy: 0.8270 - val_loss: 0.4888 - val_accuracy: 0.8248\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4814 - accuracy: 0.8300 - val_loss: 0.4818 - val_accuracy: 0.8260\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.4750 - accuracy: 0.8321 - val_loss: 0.4755 - val_accuracy: 0.8285\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4692 - accuracy: 0.8346 - val_loss: 0.4697 - val_accuracy: 0.8308\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4638 - accuracy: 0.8365 - val_loss: 0.4644 - val_accuracy: 0.8325\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4589 - accuracy: 0.8381 - val_loss: 0.4595 - val_accuracy: 0.8343\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4543 - accuracy: 0.8397 - val_loss: 0.4549 - val_accuracy: 0.8360\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4500 - accuracy: 0.8411 - val_loss: 0.4507 - val_accuracy: 0.8367\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4460 - accuracy: 0.8426 - val_loss: 0.4467 - val_accuracy: 0.8370\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4422 - accuracy: 0.8440 - val_loss: 0.4430 - val_accuracy: 0.8383\n",
      "1688/1688 [==============================] - 3s 1ms/step - loss: 0.4386 - accuracy: 0.8450 - val_loss: 0.4395 - val_accuracy: 0.8393\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4352 - accuracy: 0.8459 - val_loss: 0.4363 - val_accuracy: 0.8408\n",
      "29\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 1.9480 - accuracy: 0.4653 - val_loss: 1.5226 - val_accuracy: 0.5795\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 1.2741 - accuracy: 0.6367 - val_loss: 1.0943 - val_accuracy: 0.6667\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.9860 - accuracy: 0.6929 - val_loss: 0.9016 - val_accuracy: 0.7058\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.8379 - accuracy: 0.7202 - val_loss: 0.7915 - val_accuracy: 0.7277\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.7494 - accuracy: 0.7370 - val_loss: 0.7228 - val_accuracy: 0.7433\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6930 - accuracy: 0.7492 - val_loss: 0.6776 - val_accuracy: 0.7548\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6547 - accuracy: 0.7604 - val_loss: 0.6454 - val_accuracy: 0.7652\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6261 - accuracy: 0.7711 - val_loss: 0.6199 - val_accuracy: 0.7740\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6028 - accuracy: 0.7805 - val_loss: 0.5983 - val_accuracy: 0.7817\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5826 - accuracy: 0.7891 - val_loss: 0.5792 - val_accuracy: 0.7898\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5648 - accuracy: 0.7968 - val_loss: 0.5621 - val_accuracy: 0.7978\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5488 - accuracy: 0.8034 - val_loss: 0.5469 - val_accuracy: 0.8045\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5347 - accuracy: 0.8095 - val_loss: 0.5333 - val_accuracy: 0.8085\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5222 - accuracy: 0.8148 - val_loss: 0.5213 - val_accuracy: 0.8138\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5112 - accuracy: 0.8191 - val_loss: 0.5108 - val_accuracy: 0.8170\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5016 - accuracy: 0.8222 - val_loss: 0.5014 - val_accuracy: 0.8190\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4931 - accuracy: 0.8250 - val_loss: 0.4932 - val_accuracy: 0.8220\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4855 - accuracy: 0.8283 - val_loss: 0.4858 - val_accuracy: 0.8240\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4787 - accuracy: 0.8309 - val_loss: 0.4791 - val_accuracy: 0.8267\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4725 - accuracy: 0.8333 - val_loss: 0.4730 - val_accuracy: 0.8293\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4669 - accuracy: 0.8352 - val_loss: 0.4675 - val_accuracy: 0.8307\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4618 - accuracy: 0.8371 - val_loss: 0.4624 - val_accuracy: 0.8327\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4571 - accuracy: 0.8388 - val_loss: 0.4577 - val_accuracy: 0.8345\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4527 - accuracy: 0.8403 - val_loss: 0.4534 - val_accuracy: 0.8355\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4486 - accuracy: 0.8415 - val_loss: 0.4493 - val_accuracy: 0.8372\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4447 - accuracy: 0.8426 - val_loss: 0.4456 - val_accuracy: 0.8385\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4411 - accuracy: 0.8437 - val_loss: 0.4420 - val_accuracy: 0.8398\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4377 - accuracy: 0.8448 - val_loss: 0.4387 - val_accuracy: 0.8400\n",
      "28\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 1.9466 - accuracy: 0.4900 - val_loss: 1.5072 - val_accuracy: 0.6040\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 1.2489 - accuracy: 0.6576 - val_loss: 1.0637 - val_accuracy: 0.6797\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.9579 - accuracy: 0.7007 - val_loss: 0.8769 - val_accuracy: 0.7113\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.8172 - accuracy: 0.7249 - val_loss: 0.7744 - val_accuracy: 0.7318\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.7355 - accuracy: 0.7410 - val_loss: 0.7116 - val_accuracy: 0.7452\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6839 - accuracy: 0.7527 - val_loss: 0.6701 - val_accuracy: 0.7553\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6483 - accuracy: 0.7632 - val_loss: 0.6397 - val_accuracy: 0.7665\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.6211 - accuracy: 0.7730 - val_loss: 0.6152 - val_accuracy: 0.7770\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5985 - accuracy: 0.7824 - val_loss: 0.5942 - val_accuracy: 0.7857\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5788 - accuracy: 0.7910 - val_loss: 0.5756 - val_accuracy: 0.7930\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5614 - accuracy: 0.7985 - val_loss: 0.5589 - val_accuracy: 0.7998\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5458 - accuracy: 0.8055 - val_loss: 0.5439 - val_accuracy: 0.8027\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5320 - accuracy: 0.8109 - val_loss: 0.5307 - val_accuracy: 0.8093\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5199 - accuracy: 0.8159 - val_loss: 0.5191 - val_accuracy: 0.8140\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5093 - accuracy: 0.8202 - val_loss: 0.5088 - val_accuracy: 0.8172\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5000 - accuracy: 0.8231 - val_loss: 0.4997 - val_accuracy: 0.8218\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4918 - accuracy: 0.8260 - val_loss: 0.4917 - val_accuracy: 0.8232\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4844 - accuracy: 0.8289 - val_loss: 0.4845 - val_accuracy: 0.8257\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4779 - accuracy: 0.8314 - val_loss: 0.4779 - val_accuracy: 0.8280\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4719 - accuracy: 0.8336 - val_loss: 0.4720 - val_accuracy: 0.8305\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4664 - accuracy: 0.8354 - val_loss: 0.4665 - val_accuracy: 0.8317\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4614 - accuracy: 0.8370 - val_loss: 0.4615 - val_accuracy: 0.8337\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4567 - accuracy: 0.8389 - val_loss: 0.4568 - val_accuracy: 0.8347\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4524 - accuracy: 0.8405 - val_loss: 0.4525 - val_accuracy: 0.8365\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4483 - accuracy: 0.8421 - val_loss: 0.4485 - val_accuracy: 0.8368\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4445 - accuracy: 0.8434 - val_loss: 0.4447 - val_accuracy: 0.8382\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4410 - accuracy: 0.8446 - val_loss: 0.4412 - val_accuracy: 0.8383\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4376 - accuracy: 0.8454 - val_loss: 0.4379 - val_accuracy: 0.8400\n",
      "28\n",
      "1688/1688 [==============================] - 5s 2ms/step - loss: 1.9301 - accuracy: 0.4911 - val_loss: 1.4847 - val_accuracy: 0.6127\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 1.2330 - accuracy: 0.6584 - val_loss: 1.0528 - val_accuracy: 0.6855\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.9478 - accuracy: 0.7055 - val_loss: 0.8670 - val_accuracy: 0.7185\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.8065 - accuracy: 0.7298 - val_loss: 0.7646 - val_accuracy: 0.7335\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.7259 - accuracy: 0.7451 - val_loss: 0.7040 - val_accuracy: 0.7473\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.6765 - accuracy: 0.7565 - val_loss: 0.6646 - val_accuracy: 0.7593\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.6425 - accuracy: 0.7664 - val_loss: 0.6356 - val_accuracy: 0.7692\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.6164 - accuracy: 0.7765 - val_loss: 0.6120 - val_accuracy: 0.7792\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5947 - accuracy: 0.7854 - val_loss: 0.5917 - val_accuracy: 0.7880\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5758 - accuracy: 0.7936 - val_loss: 0.5738 - val_accuracy: 0.7937\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5591 - accuracy: 0.8005 - val_loss: 0.5578 - val_accuracy: 0.8002\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5442 - accuracy: 0.8068 - val_loss: 0.5436 - val_accuracy: 0.8053\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5310 - accuracy: 0.8124 - val_loss: 0.5309 - val_accuracy: 0.8113\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5193 - accuracy: 0.8174 - val_loss: 0.5196 - val_accuracy: 0.8147\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5089 - accuracy: 0.8210 - val_loss: 0.5095 - val_accuracy: 0.8172\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4997 - accuracy: 0.8240 - val_loss: 0.5006 - val_accuracy: 0.8202\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4915 - accuracy: 0.8263 - val_loss: 0.4926 - val_accuracy: 0.8222\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4841 - accuracy: 0.8294 - val_loss: 0.4853 - val_accuracy: 0.8253\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4775 - accuracy: 0.8320 - val_loss: 0.4787 - val_accuracy: 0.8273\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4714 - accuracy: 0.8340 - val_loss: 0.4727 - val_accuracy: 0.8293\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4659 - accuracy: 0.8363 - val_loss: 0.4672 - val_accuracy: 0.8317\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4608 - accuracy: 0.8381 - val_loss: 0.4622 - val_accuracy: 0.8343\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4561 - accuracy: 0.8398 - val_loss: 0.4575 - val_accuracy: 0.8348\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4517 - accuracy: 0.8415 - val_loss: 0.4532 - val_accuracy: 0.8350\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4477 - accuracy: 0.8429 - val_loss: 0.4491 - val_accuracy: 0.8357\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4439 - accuracy: 0.8442 - val_loss: 0.4454 - val_accuracy: 0.8370\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4403 - accuracy: 0.8455 - val_loss: 0.4419 - val_accuracy: 0.8387\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4369 - accuracy: 0.8463 - val_loss: 0.4386 - val_accuracy: 0.8400\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "# Running in arround 960s\n",
    "\n",
    "# some possible numbers to choose\n",
    "hidden_layer_1 = [100, 200, 300, 400, 500, 600]\n",
    "\n",
    "# results\n",
    "loss_history_1 = []\n",
    "epoch_history_1 = []\n",
    "\n",
    "# for the first hidden layer\n",
    "for i in hidden_layer_1:\n",
    "    loss, epoch = test_train_mlp(i, 50)\n",
    "    loss_history_1.append(loss)\n",
    "    epoch_history_1.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "938aefdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The min loss is 0.4257 when neurons is equal to 100\n",
      "Loss history: [0.4257, 0.4294, 0.4352, 0.4377, 0.4376, 0.4369]\n",
      "Epoch history: [31, 30, 29, 28, 28, 28]\n"
     ]
    }
   ],
   "source": [
    "loss_history_1 = [round(i, 4) for i in loss_history_1]\n",
    "print(f\"The min loss is {min(loss_history_1)} when neurons is equal to {hidden_layer_1[loss_history_1.index(min(loss_history_1))]}\")\n",
    "\n",
    "print(f\"Loss history: {loss_history_1}\")\n",
    "print(f\"Epoch history: {epoch_history_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ede2ba3",
   "metadata": {},
   "source": [
    "It shows that when the number of neurons of the first layer is 100, the loss is minimal. Although the epochs is slightly larger, which may enhance the performance, the training time is much faster because of the lesser neurons. To keep the model small, we choose the 100 as the first number of neurons of the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5e83f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.1365 - accuracy: 0.3363 - val_loss: 1.8808 - val_accuracy: 0.4397\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.6100 - accuracy: 0.5462 - val_loss: 1.3973 - val_accuracy: 0.6027\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2698 - accuracy: 0.6288 - val_loss: 1.1570 - val_accuracy: 0.6437\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.0734 - accuracy: 0.6632 - val_loss: 0.9944 - val_accuracy: 0.6720\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.9393 - accuracy: 0.6847 - val_loss: 0.8852 - val_accuracy: 0.6940\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8490 - accuracy: 0.7011 - val_loss: 0.8106 - val_accuracy: 0.7128\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7857 - accuracy: 0.7149 - val_loss: 0.7569 - val_accuracy: 0.7232\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7393 - accuracy: 0.7264 - val_loss: 0.7167 - val_accuracy: 0.7350\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7038 - accuracy: 0.7383 - val_loss: 0.6854 - val_accuracy: 0.7442\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6753 - accuracy: 0.7479 - val_loss: 0.6598 - val_accuracy: 0.7515\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6513 - accuracy: 0.7579 - val_loss: 0.6377 - val_accuracy: 0.7603\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6300 - accuracy: 0.7667 - val_loss: 0.6180 - val_accuracy: 0.7698\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6105 - accuracy: 0.7761 - val_loss: 0.6001 - val_accuracy: 0.7800\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5926 - accuracy: 0.7845 - val_loss: 0.5838 - val_accuracy: 0.7873\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5763 - accuracy: 0.7928 - val_loss: 0.5691 - val_accuracy: 0.7943\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5615 - accuracy: 0.7998 - val_loss: 0.5558 - val_accuracy: 0.8000\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5480 - accuracy: 0.8062 - val_loss: 0.5437 - val_accuracy: 0.8055\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5358 - accuracy: 0.8114 - val_loss: 0.5326 - val_accuracy: 0.8105\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5246 - accuracy: 0.8160 - val_loss: 0.5224 - val_accuracy: 0.8142\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5144 - accuracy: 0.8201 - val_loss: 0.5130 - val_accuracy: 0.8182\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5051 - accuracy: 0.8235 - val_loss: 0.5045 - val_accuracy: 0.8212\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4966 - accuracy: 0.8267 - val_loss: 0.4966 - val_accuracy: 0.8233\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4888 - accuracy: 0.8295 - val_loss: 0.4894 - val_accuracy: 0.8258\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4818 - accuracy: 0.8322 - val_loss: 0.4829 - val_accuracy: 0.8288\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4754 - accuracy: 0.8341 - val_loss: 0.4769 - val_accuracy: 0.8307\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4695 - accuracy: 0.8367 - val_loss: 0.4714 - val_accuracy: 0.8312\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4641 - accuracy: 0.8385 - val_loss: 0.4664 - val_accuracy: 0.8318\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4591 - accuracy: 0.8406 - val_loss: 0.4617 - val_accuracy: 0.8335\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4545 - accuracy: 0.8422 - val_loss: 0.4573 - val_accuracy: 0.8357\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4502 - accuracy: 0.8438 - val_loss: 0.4532 - val_accuracy: 0.8367\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4461 - accuracy: 0.8452 - val_loss: 0.4493 - val_accuracy: 0.8382\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4423 - accuracy: 0.8461 - val_loss: 0.4456 - val_accuracy: 0.8402\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4386 - accuracy: 0.8471 - val_loss: 0.4422 - val_accuracy: 0.8412\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4352 - accuracy: 0.8483 - val_loss: 0.4389 - val_accuracy: 0.8427\n",
      "34\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.0688 - accuracy: 0.4429 - val_loss: 1.7436 - val_accuracy: 0.5248\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4827 - accuracy: 0.5942 - val_loss: 1.2786 - val_accuracy: 0.6217\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1467 - accuracy: 0.6560 - val_loss: 1.0372 - val_accuracy: 0.6682\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.9572 - accuracy: 0.6917 - val_loss: 0.8919 - val_accuracy: 0.7018\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8381 - accuracy: 0.7173 - val_loss: 0.7973 - val_accuracy: 0.7225\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7588 - accuracy: 0.7369 - val_loss: 0.7331 - val_accuracy: 0.7407\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7039 - accuracy: 0.7502 - val_loss: 0.6880 - val_accuracy: 0.7530\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6644 - accuracy: 0.7601 - val_loss: 0.6547 - val_accuracy: 0.7613\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6344 - accuracy: 0.7695 - val_loss: 0.6286 - val_accuracy: 0.7713\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6100 - accuracy: 0.7794 - val_loss: 0.6067 - val_accuracy: 0.7792\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5893 - accuracy: 0.7881 - val_loss: 0.5875 - val_accuracy: 0.7873\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5709 - accuracy: 0.7961 - val_loss: 0.5704 - val_accuracy: 0.7938\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5546 - accuracy: 0.8038 - val_loss: 0.5550 - val_accuracy: 0.8002\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5401 - accuracy: 0.8096 - val_loss: 0.5412 - val_accuracy: 0.8078\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5271 - accuracy: 0.8148 - val_loss: 0.5290 - val_accuracy: 0.8122\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5157 - accuracy: 0.8193 - val_loss: 0.5181 - val_accuracy: 0.8152\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5056 - accuracy: 0.8221 - val_loss: 0.5084 - val_accuracy: 0.8190\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4965 - accuracy: 0.8249 - val_loss: 0.4997 - val_accuracy: 0.8212\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4884 - accuracy: 0.8276 - val_loss: 0.4919 - val_accuracy: 0.8237\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4810 - accuracy: 0.8300 - val_loss: 0.4847 - val_accuracy: 0.8255\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4743 - accuracy: 0.8324 - val_loss: 0.4781 - val_accuracy: 0.8280\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4682 - accuracy: 0.8351 - val_loss: 0.4720 - val_accuracy: 0.8302\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4625 - accuracy: 0.8367 - val_loss: 0.4663 - val_accuracy: 0.8322\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4572 - accuracy: 0.8389 - val_loss: 0.4610 - val_accuracy: 0.8337\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4522 - accuracy: 0.8406 - val_loss: 0.4561 - val_accuracy: 0.8352\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4475 - accuracy: 0.8423 - val_loss: 0.4515 - val_accuracy: 0.8365\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4431 - accuracy: 0.8439 - val_loss: 0.4471 - val_accuracy: 0.8392\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4390 - accuracy: 0.8458 - val_loss: 0.4430 - val_accuracy: 0.8408\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4350 - accuracy: 0.8473 - val_loss: 0.4391 - val_accuracy: 0.8417\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4313 - accuracy: 0.8484 - val_loss: 0.4355 - val_accuracy: 0.8428\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4277 - accuracy: 0.8493 - val_loss: 0.4320 - val_accuracy: 0.8438\n",
      "31\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.0653 - accuracy: 0.4333 - val_loss: 1.7175 - val_accuracy: 0.5725\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4502 - accuracy: 0.6172 - val_loss: 1.2486 - val_accuracy: 0.6430\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1186 - accuracy: 0.6708 - val_loss: 1.0098 - val_accuracy: 0.6792\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.9312 - accuracy: 0.6975 - val_loss: 0.8688 - val_accuracy: 0.7050\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8186 - accuracy: 0.7177 - val_loss: 0.7807 - val_accuracy: 0.7285\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7449 - accuracy: 0.7365 - val_loss: 0.7199 - val_accuracy: 0.7443\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6929 - accuracy: 0.7490 - val_loss: 0.6760 - val_accuracy: 0.7550\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6547 - accuracy: 0.7605 - val_loss: 0.6432 - val_accuracy: 0.7638\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6253 - accuracy: 0.7723 - val_loss: 0.6170 - val_accuracy: 0.7745\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6011 - accuracy: 0.7823 - val_loss: 0.5949 - val_accuracy: 0.7825\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5802 - accuracy: 0.7918 - val_loss: 0.5755 - val_accuracy: 0.7927\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5617 - accuracy: 0.7999 - val_loss: 0.5581 - val_accuracy: 0.8000\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5453 - accuracy: 0.8067 - val_loss: 0.5425 - val_accuracy: 0.8060\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5308 - accuracy: 0.8123 - val_loss: 0.5287 - val_accuracy: 0.8102\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5180 - accuracy: 0.8170 - val_loss: 0.5165 - val_accuracy: 0.8147\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5068 - accuracy: 0.8216 - val_loss: 0.5058 - val_accuracy: 0.8178\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4969 - accuracy: 0.8253 - val_loss: 0.4963 - val_accuracy: 0.8207\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4882 - accuracy: 0.8278 - val_loss: 0.4879 - val_accuracy: 0.8245\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4804 - accuracy: 0.8301 - val_loss: 0.4802 - val_accuracy: 0.8263\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4733 - accuracy: 0.8326 - val_loss: 0.4733 - val_accuracy: 0.8288\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4668 - accuracy: 0.8351 - val_loss: 0.4669 - val_accuracy: 0.8313\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4609 - accuracy: 0.8374 - val_loss: 0.4611 - val_accuracy: 0.8343\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4554 - accuracy: 0.8396 - val_loss: 0.4557 - val_accuracy: 0.8372\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4503 - accuracy: 0.8411 - val_loss: 0.4507 - val_accuracy: 0.8382\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4455 - accuracy: 0.8426 - val_loss: 0.4460 - val_accuracy: 0.8402\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4410 - accuracy: 0.8441 - val_loss: 0.4416 - val_accuracy: 0.8412\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4368 - accuracy: 0.8457 - val_loss: 0.4375 - val_accuracy: 0.8427\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4328 - accuracy: 0.8472 - val_loss: 0.4337 - val_accuracy: 0.8432\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4290 - accuracy: 0.8485 - val_loss: 0.4301 - val_accuracy: 0.8437\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4254 - accuracy: 0.8498 - val_loss: 0.4267 - val_accuracy: 0.8443\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4220 - accuracy: 0.8510 - val_loss: 0.4235 - val_accuracy: 0.8455\n",
      "31\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.0711 - accuracy: 0.4232 - val_loss: 1.7194 - val_accuracy: 0.5698\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4452 - accuracy: 0.6097 - val_loss: 1.2367 - val_accuracy: 0.6330\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1097 - accuracy: 0.6600 - val_loss: 1.0070 - val_accuracy: 0.6763\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.9330 - accuracy: 0.6980 - val_loss: 0.8732 - val_accuracy: 0.7097\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8199 - accuracy: 0.7241 - val_loss: 0.7812 - val_accuracy: 0.7327\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7412 - accuracy: 0.7427 - val_loss: 0.7169 - val_accuracy: 0.7463\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6868 - accuracy: 0.7548 - val_loss: 0.6723 - val_accuracy: 0.7592\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6485 - accuracy: 0.7645 - val_loss: 0.6399 - val_accuracy: 0.7665\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6200 - accuracy: 0.7739 - val_loss: 0.6147 - val_accuracy: 0.7755\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5972 - accuracy: 0.7819 - val_loss: 0.5938 - val_accuracy: 0.7827\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5779 - accuracy: 0.7901 - val_loss: 0.5758 - val_accuracy: 0.7893\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5611 - accuracy: 0.7970 - val_loss: 0.5599 - val_accuracy: 0.7968\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5463 - accuracy: 0.8042 - val_loss: 0.5458 - val_accuracy: 0.8028\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5330 - accuracy: 0.8087 - val_loss: 0.5331 - val_accuracy: 0.8083\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5211 - accuracy: 0.8138 - val_loss: 0.5218 - val_accuracy: 0.8122\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5105 - accuracy: 0.8182 - val_loss: 0.5116 - val_accuracy: 0.8153\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5009 - accuracy: 0.8219 - val_loss: 0.5025 - val_accuracy: 0.8188\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4923 - accuracy: 0.8252 - val_loss: 0.4942 - val_accuracy: 0.8210\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4845 - accuracy: 0.8284 - val_loss: 0.4867 - val_accuracy: 0.8238\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4774 - accuracy: 0.8313 - val_loss: 0.4799 - val_accuracy: 0.8267\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4710 - accuracy: 0.8336 - val_loss: 0.4736 - val_accuracy: 0.8280\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4650 - accuracy: 0.8363 - val_loss: 0.4677 - val_accuracy: 0.8307\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4595 - accuracy: 0.8381 - val_loss: 0.4623 - val_accuracy: 0.8330\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4544 - accuracy: 0.8404 - val_loss: 0.4573 - val_accuracy: 0.8340\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4496 - accuracy: 0.8418 - val_loss: 0.4525 - val_accuracy: 0.8355\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4451 - accuracy: 0.8432 - val_loss: 0.4481 - val_accuracy: 0.8362\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4409 - accuracy: 0.8446 - val_loss: 0.4439 - val_accuracy: 0.8378\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4369 - accuracy: 0.8457 - val_loss: 0.4400 - val_accuracy: 0.8390\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.4331 - accuracy: 0.8465 - val_loss: 0.4363 - val_accuracy: 0.8400\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4295 - accuracy: 0.8476 - val_loss: 0.4328 - val_accuracy: 0.8408\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4261 - accuracy: 0.8487 - val_loss: 0.4294 - val_accuracy: 0.8420\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# Running in arround 340s\n",
    "\n",
    "# some possible numbers to choose\n",
    "hidden_layer_2 = [20, 40, 60, 80]\n",
    "\n",
    "# results\n",
    "loss_history_2 = []\n",
    "epoch_history_2 = []\n",
    "\n",
    "# for the first hidden layer\n",
    "for i in hidden_layer_2:\n",
    "    loss, epoch = test_train_mlp(100, i)\n",
    "    loss_history_2.append(loss)\n",
    "    epoch_history_2.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5ec56b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The min loss is 0.422 when neurons is equal to 60\n",
      "Loss history: [0.4352, 0.4277, 0.422, 0.4261]\n",
      "Epoch history: [34, 31, 31, 31]\n"
     ]
    }
   ],
   "source": [
    "loss_history_2 = [round(i, 4) for i in loss_history_2]\n",
    "print(f\"The min loss is {min(loss_history_2)} when neurons is equal to {hidden_layer_2[loss_history_2.index(min(loss_history_2))]}\")\n",
    "\n",
    "print(f\"Loss history: {loss_history_2}\")\n",
    "print(f\"Epoch history: {epoch_history_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f06bf4",
   "metadata": {},
   "source": [
    "We select 60 as the neurons of the second hidden layer, as it achieves the best result with a less epochs, and the training time is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbde004",
   "metadata": {},
   "source": [
    "The architecture of CNN is settled in this section.   \n",
    "\n",
    "The input shape shoule be (28, 28, 1). This last dimension indicates the channel. The dataset consists of grey images, thus, the channel is 1. Although usually the CNN performs well, the theory of determining the architecture, such as the number of convolutional layers, the number of filters, is unclear. Sometimes it is determined by experiment and experience. Given that the condition is limited, we just choose the similar architecture from the tutorial. In other words, we choose two conv and pool blocks with 32, 64 (empirically it should be exponentiation of 2) filters respectively, follower by a simple FC layer.   \n",
    "\n",
    "We also choose Max Pooling due to the distribution of pixels each image (features are presented as large value of pixels).   \n",
    "\n",
    "We have discussed the ReLU and Softmax previously.\n",
    "\n",
    "The drouout layer is added to avoid overfitting.\n",
    "\n",
    "The rest paras such as filter size, strides, and learning rate, will be tuned in section 3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3fe177b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1d1b7770d60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.Sequential([\n",
    "    # Specify the input shape\n",
    "    keras.Input(shape=(*IMAGE_SIZE, 1)),\n",
    "    \n",
    "    # Conv and pool block 1\n",
    "    keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", strides=(1, 1), kernel_initializer=initializer),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'), # padding evenly\n",
    "    \n",
    "    # Conv and pool block 2\n",
    "    keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\", strides=(1, 1), kernel_initializer=initializer),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'), # padding evenly\n",
    "    \n",
    "    # Flatten and classify using dense output layer\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=initializer),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fd29f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method in this section.\n",
    "def get_result(estimator, paras, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, epochs=1):\n",
    "    \"\"\"get grid search result.\n",
    "    estimator: model to be tuned.\n",
    "    paras: an instance of ParameterGrid.\"\"\"\n",
    "    # Return a dict\n",
    "    result = {\n",
    "        \"best_paras\": None,\n",
    "        \"best_score\": 0,\n",
    "        \"best_estimator\": None,\n",
    "        \"results\": [],\n",
    "    }\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    # Grid search for each combination.\n",
    "    for para in paras:\n",
    "        # Set para\n",
    "        current_estimator = clone(estimator)\n",
    "        current_estimator.set_params(**para)\n",
    "\n",
    "        # Training and timer\n",
    "        t1 = time.time()\n",
    "        if epochs == 1:\n",
    "            current_estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            current_estimator.fit(X_train, y_train, epochs=epochs)\n",
    "        t2 = time.time()\n",
    "\n",
    "        # Score on validation set\n",
    "        score = current_estimator.score(X_valid, y_valid)\n",
    "        t3 = time.time()\n",
    "\n",
    "        # result for each combination\n",
    "        temp = {}\n",
    "        temp[\"paras\"] = para\n",
    "        temp[\"training_time\"] = t2 - t1\n",
    "        temp[\"validation_time\"] = t3 - t2\n",
    "        temp[\"score\"] =score\n",
    "        \n",
    "        # Update the best result\n",
    "        result[\"results\"].append(temp)\n",
    "        if score > result[\"best_score\"]:\n",
    "            result[\"best_paras\"] = para\n",
    "            result[\"best_score\"] = score\n",
    "            result[\"best_estimator\"] = current_estimator\n",
    "        \n",
    "        i += 1\n",
    "        print(f\"{i} out of {len(list(paras))} finished: {para}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def show_results(name, result, X_test=X_test, y_test=y_test):\n",
    "    \"\"\"Show the results.\"\"\"\n",
    "    print(f\"Results for {name}:\")\n",
    "    print(\"Best parameters: {}\".format(result[\"best_paras\"]))\n",
    "    print(\"Best validation score: {:.4f}\".format(result[\"best_score\"]))\n",
    "    print(\"Test set score: {:.4f}\".format(result[\"best_estimator\"].score(X_test, y_test)))\n",
    "\n",
    "    # table of results\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"Score\", \"training_time\", \"validation_time\"], \n",
    "        index=[str(result[\"results\"][i][\"paras\"]) for i in range(len(result[\"results\"]))]\n",
    "        )\n",
    "\n",
    "    for i in range(len(result[\"results\"])):\n",
    "        df.loc[str(result[\"results\"][i][\"paras\"])] = [\n",
    "            round(result[\"results\"][i][\"score\"], 4),\n",
    "            round(result[\"results\"][i][\"training_time\"], 2),\n",
    "            round(result[\"results\"][i][\"validation_time\"], 2)\n",
    "            ]\n",
    "\n",
    "    df.to_csv(f'{name}_results.csv')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5065b2",
   "metadata": {},
   "source": [
    "First, to determine a rough trend of the accuracy with different k, we calculate an accuracy every 10 with different k, until k = 245 (sqrt(#examples)), e.g., k = [1, 11, 21, ..., 241]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7796726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\t finished.\n",
      "k = 11\t finished.\n",
      "k = 21\t finished.\n",
      "k = 31\t finished.\n",
      "k = 41\t finished.\n",
      "k = 51\t finished.\n",
      "k = 61\t finished.\n",
      "k = 71\t finished.\n",
      "k = 81\t finished.\n",
      "k = 91\t finished.\n",
      "k = 101\t finished.\n",
      "k = 111\t finished.\n",
      "k = 121\t finished.\n",
      "k = 131\t finished.\n",
      "k = 141\t finished.\n",
      "k = 151\t finished.\n",
      "k = 161\t finished.\n",
      "k = 171\t finished.\n",
      "k = 181\t finished.\n",
      "k = 191\t finished.\n",
      "k = 201\t finished.\n",
      "k = 211\t finished.\n",
      "k = 221\t finished.\n",
      "k = 231\t finished.\n",
      "k = 241\t finished.\n"
     ]
    }
   ],
   "source": [
    "# Running in arroung 190s\n",
    "\n",
    "k_value = [1 + i for i in range(245)]\n",
    "k_acc = []\n",
    "\n",
    "for i in k_value:\n",
    "    if i % 10 != 1:\n",
    "        continue\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "    k_acc.append(knn.score(X_test.reshape(X_test.shape[0], -1), y_test))\n",
    "    print(f\"k = {i}\\t finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8feae03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 144x144 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2, 2))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot([i*10 + 1 for i in range(25)], k_acc, marker=\"o\")\n",
    "ax.set(xlabel=\"k\", ylabel=\"ACC\", xticks=range(1, 250, 40))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c385ef",
   "metadata": {},
   "source": [
    "From the figure, the trend is roughly decending. Therefore, we can choose a range of k in [1, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa801bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\t finished.\n",
      "k = 2\t finished.\n",
      "k = 3\t finished.\n",
      "k = 4\t finished.\n",
      "k = 5\t finished.\n",
      "k = 6\t finished.\n",
      "k = 7\t finished.\n",
      "k = 8\t finished.\n",
      "k = 9\t finished.\n",
      "k = 10\t finished.\n",
      "k = 11\t finished.\n",
      "k = 12\t finished.\n",
      "k = 13\t finished.\n",
      "k = 14\t finished.\n",
      "k = 15\t finished.\n",
      "k = 16\t finished.\n",
      "k = 17\t finished.\n",
      "k = 18\t finished.\n",
      "k = 19\t finished.\n",
      "k = 20\t finished.\n"
     ]
    }
   ],
   "source": [
    "# Running in arroung 140s\n",
    "\n",
    "k_value = [i for i in range(1,21)]\n",
    "k_acc = []\n",
    "\n",
    "for i in k_value:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "    k_acc.append(knn.score(X_test.reshape(X_test.shape[0], -1), y_test))\n",
    "    print(f\"k = {i}\\t finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4272973c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 144x144 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2, 2))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(k_value, k_acc, marker=\"o\")\n",
    "ax.set(xlabel=\"k\", ylabel=\"ACC\", xticks=k_value)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089faa21",
   "metadata": {},
   "source": [
    "From the figure above, we chose k = [3, 5, 9]. For p, we chose p [1, 2], it represents manhattan_distance and euclidean_distance respectively. We also consider the weight (uniform, distance) each point contribute to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca9e1e",
   "metadata": {},
   "source": [
    "If we use CV with 10 folds, totally we need 3 * 2 * 2 * cv = 120 on 90% training set. Considering the running time, we use standard grid search instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfadede2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 combinations.\n",
      "Parameter grid:\n",
      "{'n_neighbors': [3, 5, 9], 'p': [1, 2], 'weights': ['uniform', 'distance']}\n"
     ]
    }
   ],
   "source": [
    "# Setting parameters\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 9],\n",
    "    'p': [1, 2],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    }\n",
    "\n",
    "knn_paras = ParameterGrid(param_grid)\n",
    "\n",
    "print(f\"There are {len(list(knn_paras))} combinations.\")\n",
    "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CV for tuning\n",
    "# Setting the 10 fold stratified cross-validation\n",
    "\n",
    "# cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "# grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=cvKFold, return_train_score=True, verbose=3)\n",
    "# grid_search.fit(X_train_full.reshape(60000, -1), y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d58ecbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 12 finished: {'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}\n",
      "2 out of 12 finished: {'n_neighbors': 3, 'p': 1, 'weights': 'distance'}\n",
      "3 out of 12 finished: {'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}\n",
      "4 out of 12 finished: {'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n",
      "5 out of 12 finished: {'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}\n",
      "6 out of 12 finished: {'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n",
      "7 out of 12 finished: {'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "8 out of 12 finished: {'n_neighbors': 5, 'p': 2, 'weights': 'distance'}\n",
      "9 out of 12 finished: {'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}\n",
      "10 out of 12 finished: {'n_neighbors': 9, 'p': 1, 'weights': 'distance'}\n",
      "11 out of 12 finished: {'n_neighbors': 9, 'p': 2, 'weights': 'uniform'}\n",
      "12 out of 12 finished: {'n_neighbors': 9, 'p': 2, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# Running in around 410s\n",
    "\n",
    "# Tuning KNN paras, 12 combination in total.\n",
    "knn_result = get_result(\n",
    "    KNeighborsClassifier(), \n",
    "    knn_paras, \n",
    "    X_train=X_train.reshape(X_train.shape[0], -1), \n",
    "    X_valid=X_valid.reshape(X_valid.shape[0], -1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e925ac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for KNN:\n",
      "Best parameters: {'n_neighbors': 3, 'p': 1, 'weights': 'distance'}\n",
      "Best validation score: 0.8653\n",
      "Test set score: 0.8567\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>training_time</th>\n",
       "      <th>validation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}</th>\n",
       "      <td>0.8622</td>\n",
       "      <td>0.05</td>\n",
       "      <td>40.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 3, 'p': 1, 'weights': 'distance'}</th>\n",
       "      <td>0.8653</td>\n",
       "      <td>0.05</td>\n",
       "      <td>40.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}</th>\n",
       "      <td>0.8583</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 3, 'p': 2, 'weights': 'distance'}</th>\n",
       "      <td>0.8602</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}</th>\n",
       "      <td>0.8603</td>\n",
       "      <td>0.05</td>\n",
       "      <td>40.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 5, 'p': 1, 'weights': 'distance'}</th>\n",
       "      <td>0.8622</td>\n",
       "      <td>0.04</td>\n",
       "      <td>40.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}</th>\n",
       "      <td>0.855</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 5, 'p': 2, 'weights': 'distance'}</th>\n",
       "      <td>0.8577</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}</th>\n",
       "      <td>0.8628</td>\n",
       "      <td>0.05</td>\n",
       "      <td>40.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 9, 'p': 1, 'weights': 'distance'}</th>\n",
       "      <td>0.8645</td>\n",
       "      <td>0.04</td>\n",
       "      <td>40.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 9, 'p': 2, 'weights': 'uniform'}</th>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'n_neighbors': 9, 'p': 2, 'weights': 'distance'}</th>\n",
       "      <td>0.8533</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Score training_time  \\\n",
       "{'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}   0.8622          0.05   \n",
       "{'n_neighbors': 3, 'p': 1, 'weights': 'distance'}  0.8653          0.05   \n",
       "{'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}   0.8583          0.05   \n",
       "{'n_neighbors': 3, 'p': 2, 'weights': 'distance'}  0.8602          0.04   \n",
       "{'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}   0.8603          0.05   \n",
       "{'n_neighbors': 5, 'p': 1, 'weights': 'distance'}  0.8622          0.04   \n",
       "{'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}    0.855          0.05   \n",
       "{'n_neighbors': 5, 'p': 2, 'weights': 'distance'}  0.8577          0.05   \n",
       "{'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}   0.8628          0.05   \n",
       "{'n_neighbors': 9, 'p': 1, 'weights': 'distance'}  0.8645          0.04   \n",
       "{'n_neighbors': 9, 'p': 2, 'weights': 'uniform'}   0.8512          0.05   \n",
       "{'n_neighbors': 9, 'p': 2, 'weights': 'distance'}  0.8533          0.05   \n",
       "\n",
       "                                                  validation_time  \n",
       "{'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}            40.38  \n",
       "{'n_neighbors': 3, 'p': 1, 'weights': 'distance'}           40.33  \n",
       "{'n_neighbors': 3, 'p': 2, 'weights': 'uniform'}             2.81  \n",
       "{'n_neighbors': 3, 'p': 2, 'weights': 'distance'}            2.68  \n",
       "{'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}            40.39  \n",
       "{'n_neighbors': 5, 'p': 1, 'weights': 'distance'}           40.23  \n",
       "{'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}             2.92  \n",
       "{'n_neighbors': 5, 'p': 2, 'weights': 'distance'}            2.68  \n",
       "{'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}            40.42  \n",
       "{'n_neighbors': 9, 'p': 1, 'weights': 'distance'}           40.57  \n",
       "{'n_neighbors': 9, 'p': 2, 'weights': 'uniform'}             3.06  \n",
       "{'n_neighbors': 9, 'p': 2, 'weights': 'distance'}            2.89  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running in around 110s\n",
    "\n",
    "show_results(\"KNN\", knn_result, X_test=X_test.reshape(X_test.shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fully connected neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd231b38",
   "metadata": {},
   "source": [
    "From section 2.2, we settled the numbers of neurons (100, 20) in hidden layers. Although the number of hidden layers as well as number of neurons are also hyperparameter, to avoid a great running time due to a number of combination of paras, we design the structure first and tune the other paras in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce23462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24 combinations.\n",
      "Parameter grid:\n",
      "{'optimizer__lr': [0.1, 0.01, 0.001], 'activation_function': ['relu', 'sigmoid', 'tanh', None], 'optimizer': ['sgd', 'Adam']}\n"
     ]
    }
   ],
   "source": [
    "def build_mlp(activation_function=\"relu\"):\n",
    "    \"\"\"Build a Keras MLP for 10 class classification with desired parameters.\"\"\"\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Add the input layer\n",
    "    model.add(keras.layers.Flatten(input_shape=IMAGE_SIZE))\n",
    "    \n",
    "    # Add the hidden layers with activation function\n",
    "    model.add(keras.layers.Dense(100, activation=activation_function, kernel_initializer=initializer))\n",
    "    model.add(keras.layers.Dense(60, activation=activation_function, kernel_initializer=initializer))\n",
    "        \n",
    "    # Add the output layer for 10 class classification\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=initializer))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier object which works with sklearn grid searches\n",
    "# We need to pass default values of arguments in build_mlp if we wish to tune them\n",
    "keras_classifier = KerasClassifier(build_mlp,\n",
    "                                   activation_function=\"relu\",\n",
    "                                   loss=\"sparse_categorical_crossentropy\",\n",
    "                                   optimizer=\"sgd\",\n",
    "                                   optimizer__lr=0.01,\n",
    "                                   metrics=[\"accuracy\"]\n",
    "                                  )\n",
    "\n",
    "param_grid = {\n",
    "    \"optimizer__lr\": [0.1, 0.01, 0.001],\n",
    "    \"activation_function\": [\"relu\", \"sigmoid\", \"tanh\", None],\n",
    "    \"optimizer\": [\"sgd\", \"Adam\"],\n",
    "}\n",
    "\n",
    "mlp_paras = ParameterGrid(param_grid)\n",
    "\n",
    "print(f\"There are {len(list(mlp_paras))} combinations.\")\n",
    "print(\"Parameter grid:\\n{}\".format(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cd83cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 974us/step - loss: 0.5523 - accuracy: 0.8009\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 965us/step - loss: 0.4040 - accuracy: 0.8533\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 968us/step - loss: 0.3632 - accuracy: 0.8651\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 963us/step - loss: 0.3385 - accuracy: 0.8740\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 967us/step - loss: 0.3194 - accuracy: 0.8820\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 955us/step - loss: 0.3053 - accuracy: 0.8870\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 953us/step - loss: 0.2929 - accuracy: 0.8906\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 975us/step - loss: 0.2823 - accuracy: 0.8937\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 954us/step - loss: 0.2732 - accuracy: 0.8967\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 949us/step - loss: 0.2654 - accuracy: 0.8996\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 947us/step - loss: 0.2582 - accuracy: 0.9033\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 957us/step - loss: 0.2512 - accuracy: 0.9045\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 953us/step - loss: 0.2447 - accuracy: 0.9069\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 963us/step - loss: 0.2378 - accuracy: 0.9090\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2328 - accuracy: 0.9118\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 997us/step - loss: 0.2268 - accuracy: 0.9141\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 967us/step - loss: 0.2222 - accuracy: 0.9161\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 943us/step - loss: 0.2172 - accuracy: 0.9175\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 942us/step - loss: 0.2127 - accuracy: 0.9184\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 944us/step - loss: 0.2062 - accuracy: 0.9213\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 944us/step - loss: 0.2040 - accuracy: 0.9217\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 944us/step - loss: 0.1996 - accuracy: 0.9235\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 970us/step - loss: 0.1948 - accuracy: 0.9253\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 958us/step - loss: 0.1900 - accuracy: 0.9268\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 974us/step - loss: 0.1877 - accuracy: 0.9287\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 946us/step - loss: 0.1853 - accuracy: 0.9280\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 946us/step - loss: 0.1824 - accuracy: 0.9291\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 943us/step - loss: 0.1764 - accuracy: 0.9323\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 940us/step - loss: 0.1765 - accuracy: 0.9331\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 942us/step - loss: 0.1718 - accuracy: 0.9343\n",
      "188/188 [==============================] - 0s 698us/step\n",
      "1 out of 24 finished: {'activation_function': 'relu', 'optimizer': 'sgd', 'optimizer__lr': 0.1}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 960us/step - loss: 0.7777 - accuracy: 0.7462\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 983us/step - loss: 0.5046 - accuracy: 0.8268\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 961us/step - loss: 0.4568 - accuracy: 0.8404\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 956us/step - loss: 0.4297 - accuracy: 0.8486\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 959us/step - loss: 0.4100 - accuracy: 0.8567\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 962us/step - loss: 0.3938 - accuracy: 0.8626\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 957us/step - loss: 0.3807 - accuracy: 0.8661\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 958us/step - loss: 0.3701 - accuracy: 0.8691\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 972us/step - loss: 0.3599 - accuracy: 0.8726\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 957us/step - loss: 0.3518 - accuracy: 0.8746\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 958us/step - loss: 0.3432 - accuracy: 0.8784\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 968us/step - loss: 0.3363 - accuracy: 0.8798\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 961us/step - loss: 0.3298 - accuracy: 0.8813\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 966us/step - loss: 0.3232 - accuracy: 0.8843\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 978us/step - loss: 0.3170 - accuracy: 0.8862\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 953us/step - loss: 0.3113 - accuracy: 0.8880\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 960us/step - loss: 0.3054 - accuracy: 0.8906\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 965us/step - loss: 0.3015 - accuracy: 0.8906\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 960us/step - loss: 0.2961 - accuracy: 0.8935\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 960us/step - loss: 0.2902 - accuracy: 0.8953\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 966us/step - loss: 0.2866 - accuracy: 0.8962\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 960us/step - loss: 0.2831 - accuracy: 0.8973\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 959us/step - loss: 0.2786 - accuracy: 0.8990\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 957us/step - loss: 0.2753 - accuracy: 0.9011\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 959us/step - loss: 0.2708 - accuracy: 0.9031\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 961us/step - loss: 0.2676 - accuracy: 0.9030\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 954us/step - loss: 0.2641 - accuracy: 0.9035\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 951us/step - loss: 0.2602 - accuracy: 0.9058\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 957us/step - loss: 0.2569 - accuracy: 0.9074\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 962us/step - loss: 0.2533 - accuracy: 0.9084\n",
      "188/188 [==============================] - 0s 696us/step\n",
      "2 out of 24 finished: {'activation_function': 'relu', 'optimizer': 'sgd', 'optimizer__lr': 0.01}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 953us/step - loss: 1.6533 - accuracy: 0.5011\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 957us/step - loss: 0.9808 - accuracy: 0.6989\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 963us/step - loss: 0.7788 - accuracy: 0.7530\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 970us/step - loss: 0.6913 - accuracy: 0.7784\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 961us/step - loss: 0.6389 - accuracy: 0.7931\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 951us/step - loss: 0.6022 - accuracy: 0.8038\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 950us/step - loss: 0.5752 - accuracy: 0.8100\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 985us/step - loss: 0.5540 - accuracy: 0.8154\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 961us/step - loss: 0.5372 - accuracy: 0.8201\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 952us/step - loss: 0.5237 - accuracy: 0.8233\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 951us/step - loss: 0.5119 - accuracy: 0.8260\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 951us/step - loss: 0.5018 - accuracy: 0.8291\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 955us/step - loss: 0.4934 - accuracy: 0.8316\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 952us/step - loss: 0.4858 - accuracy: 0.8334\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 951us/step - loss: 0.4789 - accuracy: 0.8355\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 951us/step - loss: 0.4726 - accuracy: 0.8372\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 954us/step - loss: 0.4668 - accuracy: 0.8389\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 951us/step - loss: 0.4617 - accuracy: 0.8407\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 962us/step - loss: 0.4567 - accuracy: 0.8422\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 958us/step - loss: 0.4521 - accuracy: 0.8430\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 970us/step - loss: 0.4480 - accuracy: 0.8443\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 959us/step - loss: 0.4440 - accuracy: 0.8464\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 954us/step - loss: 0.4401 - accuracy: 0.8477\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 955us/step - loss: 0.4368 - accuracy: 0.8489\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 958us/step - loss: 0.4330 - accuracy: 0.8501\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 955us/step - loss: 0.4299 - accuracy: 0.8511\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 964us/step - loss: 0.4265 - accuracy: 0.8523\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 966us/step - loss: 0.4236 - accuracy: 0.8534\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 962us/step - loss: 0.4208 - accuracy: 0.8543\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 967us/step - loss: 0.4179 - accuracy: 0.8547\n",
      "188/188 [==============================] - 0s 697us/step\n",
      "3 out of 24 finished: {'activation_function': 'relu', 'optimizer': 'sgd', 'optimizer__lr': 0.001}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.9412 - accuracy: 0.2261\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.9550 - accuracy: 0.1905\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.9386 - accuracy: 0.1932\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.0199 - accuracy: 0.1769\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.8893 - accuracy: 0.1932\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.8357 - accuracy: 0.1922\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.8877 - accuracy: 0.1916\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.8569 - accuracy: 0.1944\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7971 - accuracy: 0.1955\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7549 - accuracy: 0.1944\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7524 - accuracy: 0.1949\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7534 - accuracy: 0.1946\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7534 - accuracy: 0.1991\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7526 - accuracy: 0.1978\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7529 - accuracy: 0.1973\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7523 - accuracy: 0.1963\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7540 - accuracy: 0.2010\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7521 - accuracy: 0.1990\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7552 - accuracy: 0.1960\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7526 - accuracy: 0.2008\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7532 - accuracy: 0.1969\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7537 - accuracy: 0.1965\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7529 - accuracy: 0.1979\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7510 - accuracy: 0.2008\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7542 - accuracy: 0.1971\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7529 - accuracy: 0.1967\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7525 - accuracy: 0.1975\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7518 - accuracy: 0.2006\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7531 - accuracy: 0.1986\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7521 - accuracy: 0.1974\n",
      "188/188 [==============================] - 0s 711us/step\n",
      "4 out of 24 finished: {'activation_function': 'relu', 'optimizer': 'Adam', 'optimizer__lr': 0.1}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5480 - accuracy: 0.8045\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4461 - accuracy: 0.8405\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4232 - accuracy: 0.8490\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4166 - accuracy: 0.8520\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4067 - accuracy: 0.8541\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3957 - accuracy: 0.8585\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3893 - accuracy: 0.8604\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3806 - accuracy: 0.8635\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3764 - accuracy: 0.8653\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3885 - accuracy: 0.8606\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3822 - accuracy: 0.8637\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3769 - accuracy: 0.8666\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3724 - accuracy: 0.8665\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3640 - accuracy: 0.8696\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3579 - accuracy: 0.8716\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3598 - accuracy: 0.8717\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3612 - accuracy: 0.8706\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3477 - accuracy: 0.8741\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3455 - accuracy: 0.8755\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3510 - accuracy: 0.8743\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3454 - accuracy: 0.8756\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3569 - accuracy: 0.8719\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3540 - accuracy: 0.8714\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3470 - accuracy: 0.8765\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3413 - accuracy: 0.8769\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3398 - accuracy: 0.8784\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3349 - accuracy: 0.8797\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3364 - accuracy: 0.8791\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3312 - accuracy: 0.8805\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3364 - accuracy: 0.8785\n",
      "188/188 [==============================] - 0s 763us/step\n",
      "5 out of 24 finished: {'activation_function': 'relu', 'optimizer': 'Adam', 'optimizer__lr': 0.01}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5115 - accuracy: 0.8196\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3767 - accuracy: 0.8640\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3391 - accuracy: 0.8759\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3171 - accuracy: 0.8829\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2985 - accuracy: 0.8889\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2863 - accuracy: 0.8931\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2728 - accuracy: 0.8998\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2595 - accuracy: 0.9026\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2523 - accuracy: 0.9046\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2444 - accuracy: 0.9075\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2326 - accuracy: 0.9126\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2266 - accuracy: 0.9146\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2185 - accuracy: 0.9170\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2130 - accuracy: 0.9197\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2058 - accuracy: 0.9209\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1997 - accuracy: 0.9235\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1960 - accuracy: 0.9259\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1888 - accuracy: 0.9283\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1849 - accuracy: 0.9289\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1831 - accuracy: 0.9306\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1796 - accuracy: 0.9313\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1696 - accuracy: 0.9346\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1685 - accuracy: 0.9353\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1644 - accuracy: 0.9372\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1600 - accuracy: 0.9397\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1554 - accuracy: 0.9404\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1527 - accuracy: 0.9417\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1472 - accuracy: 0.9431\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1449 - accuracy: 0.9436\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1413 - accuracy: 0.9453\n",
      "188/188 [==============================] - 0s 715us/step\n",
      "6 out of 24 finished: {'activation_function': 'relu', 'optimizer': 'Adam', 'optimizer__lr': 0.001}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.9854 - accuracy: 0.6590\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5370 - accuracy: 0.8100\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4667 - accuracy: 0.8337\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4331 - accuracy: 0.8441\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4111 - accuracy: 0.8536\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3940 - accuracy: 0.8597\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3807 - accuracy: 0.8630\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3702 - accuracy: 0.8664\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3603 - accuracy: 0.8699\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3519 - accuracy: 0.8719\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3434 - accuracy: 0.8769\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3369 - accuracy: 0.8778\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3300 - accuracy: 0.8795\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3236 - accuracy: 0.8824\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3174 - accuracy: 0.8844\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1000us/step - loss: 0.3119 - accuracy: 0.8860\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3055 - accuracy: 0.8898\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3014 - accuracy: 0.8887\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2965 - accuracy: 0.8922\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2909 - accuracy: 0.8929\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2869 - accuracy: 0.8934\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2832 - accuracy: 0.8965\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2790 - accuracy: 0.8980\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2752 - accuracy: 0.8989\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2707 - accuracy: 0.9003\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2678 - accuracy: 0.9012\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2637 - accuracy: 0.9027\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2594 - accuracy: 0.9035\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2578 - accuracy: 0.9043\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2529 - accuracy: 0.9071\n",
      "188/188 [==============================] - 0s 734us/step\n",
      "7 out of 24 finished: {'activation_function': 'sigmoid', 'optimizer': 'sgd', 'optimizer__lr': 0.1}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.0653 - accuracy: 0.4333\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4501 - accuracy: 0.6122\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1183 - accuracy: 0.6688\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.9311 - accuracy: 0.6953\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8186 - accuracy: 0.7176\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7447 - accuracy: 0.7360\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6929 - accuracy: 0.7507\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6548 - accuracy: 0.7615\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6253 - accuracy: 0.7721\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6012 - accuracy: 0.7828\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5803 - accuracy: 0.7923\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5617 - accuracy: 0.8006\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5454 - accuracy: 0.8066\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5309 - accuracy: 0.8126\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5182 - accuracy: 0.8170\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5069 - accuracy: 0.8212\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4970 - accuracy: 0.8254\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4882 - accuracy: 0.8286\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4804 - accuracy: 0.8312\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4734 - accuracy: 0.8324\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4669 - accuracy: 0.8351\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4610 - accuracy: 0.8372\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4554 - accuracy: 0.8387\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4504 - accuracy: 0.8408\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4454 - accuracy: 0.8418\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4410 - accuracy: 0.8448\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4367 - accuracy: 0.8464\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4327 - accuracy: 0.8467\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4290 - accuracy: 0.8483\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4255 - accuracy: 0.8492\n",
      "188/188 [==============================] - 0s 739us/step\n",
      "8 out of 24 finished: {'activation_function': 'sigmoid', 'optimizer': 'sgd', 'optimizer__lr': 0.01}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 994us/step - loss: 2.3311 - accuracy: 0.1289\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.2469 - accuracy: 0.3896\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.2102 - accuracy: 0.4634\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 994us/step - loss: 2.1686 - accuracy: 0.5458\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 990us/step - loss: 2.1194 - accuracy: 0.5681\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 993us/step - loss: 2.0606 - accuracy: 0.6006\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 997us/step - loss: 1.9917 - accuracy: 0.6125\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 994us/step - loss: 1.9150 - accuracy: 0.6008\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 992us/step - loss: 1.8350 - accuracy: 0.6037\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 996us/step - loss: 1.7565 - accuracy: 0.5934\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 994us/step - loss: 1.6831 - accuracy: 0.6012\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.6165 - accuracy: 0.5999\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5566 - accuracy: 0.6091\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5027 - accuracy: 0.6119\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4539 - accuracy: 0.6214\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4096 - accuracy: 0.6267\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 989us/step - loss: 1.3689 - accuracy: 0.6340\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 997us/step - loss: 1.3313 - accuracy: 0.6452\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 998us/step - loss: 1.2962 - accuracy: 0.6489\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2632 - accuracy: 0.6524\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2323 - accuracy: 0.6591\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.2030 - accuracy: 0.6629\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1752 - accuracy: 0.6652\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1488 - accuracy: 0.6693\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1238 - accuracy: 0.6722\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 999us/step - loss: 1.1000 - accuracy: 0.6747\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 998us/step - loss: 1.0774 - accuracy: 0.6773\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.0560 - accuracy: 0.6810\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 997us/step - loss: 1.0356 - accuracy: 0.6818\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.0163 - accuracy: 0.6852\n",
      "188/188 [==============================] - 0s 744us/step\n",
      "9 out of 24 finished: {'activation_function': 'sigmoid', 'optimizer': 'sgd', 'optimizer__lr': 0.001}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.6611 - accuracy: 0.2850\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.6164 - accuracy: 0.2919\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5782 - accuracy: 0.2944\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5460 - accuracy: 0.3080\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5494 - accuracy: 0.3295\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.6137 - accuracy: 0.3158\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5360 - accuracy: 0.3406\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.7608 - accuracy: 0.2815\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.1013 - accuracy: 0.1883\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.0346 - accuracy: 0.1886\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.0469 - accuracy: 0.1911\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.8669 - accuracy: 0.2439\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.6129 - accuracy: 0.3397\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.6031 - accuracy: 0.3514\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5742 - accuracy: 0.3729\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5823 - accuracy: 0.3680\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5034 - accuracy: 0.4039\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4689 - accuracy: 0.4148\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5391 - accuracy: 0.3947\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4537 - accuracy: 0.4192\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5505 - accuracy: 0.3939\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4749 - accuracy: 0.4164\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4844 - accuracy: 0.4178\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4685 - accuracy: 0.4247\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4752 - accuracy: 0.4231\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5007 - accuracy: 0.4201\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4804 - accuracy: 0.4323\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4532 - accuracy: 0.4409\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4527 - accuracy: 0.4308\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.4818 - accuracy: 0.4280\n",
      "188/188 [==============================] - 0s 758us/step\n",
      "10 out of 24 finished: {'activation_function': 'sigmoid', 'optimizer': 'Adam', 'optimizer__lr': 0.1}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5692 - accuracy: 0.7917\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4766 - accuracy: 0.8277\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4439 - accuracy: 0.8389\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4365 - accuracy: 0.8410\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4318 - accuracy: 0.8442\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4265 - accuracy: 0.8456\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4108 - accuracy: 0.8529\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4088 - accuracy: 0.8534\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4006 - accuracy: 0.8553\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3939 - accuracy: 0.8552\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3994 - accuracy: 0.8549\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3880 - accuracy: 0.8599\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3838 - accuracy: 0.8606\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3784 - accuracy: 0.8630\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3842 - accuracy: 0.8603\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3757 - accuracy: 0.8646\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3822 - accuracy: 0.8637\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3779 - accuracy: 0.8635\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3759 - accuracy: 0.8659\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3760 - accuracy: 0.8647\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3698 - accuracy: 0.8671\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3671 - accuracy: 0.8664\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3708 - accuracy: 0.8669\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3694 - accuracy: 0.8668\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3734 - accuracy: 0.8648\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3627 - accuracy: 0.8679\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3743 - accuracy: 0.8650\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3674 - accuracy: 0.8667\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3644 - accuracy: 0.8694\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3713 - accuracy: 0.8659\n",
      "188/188 [==============================] - 0s 741us/step\n",
      "11 out of 24 finished: {'activation_function': 'sigmoid', 'optimizer': 'Adam', 'optimizer__lr': 0.01}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6677 - accuracy: 0.7802\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4039 - accuracy: 0.8548\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3615 - accuracy: 0.8694\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3375 - accuracy: 0.8782\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3197 - accuracy: 0.8845\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3047 - accuracy: 0.8879\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2923 - accuracy: 0.8935\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2793 - accuracy: 0.8969\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2704 - accuracy: 0.9001\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2610 - accuracy: 0.9039\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2520 - accuracy: 0.9064\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2450 - accuracy: 0.9090\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2369 - accuracy: 0.9118\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2307 - accuracy: 0.9137\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2231 - accuracy: 0.9170\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2175 - accuracy: 0.9193\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2115 - accuracy: 0.9206\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2037 - accuracy: 0.9248\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1993 - accuracy: 0.9261\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1959 - accuracy: 0.9275\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1897 - accuracy: 0.9289\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1858 - accuracy: 0.9311\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1797 - accuracy: 0.9334\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1761 - accuracy: 0.9343\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1724 - accuracy: 0.9366\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1672 - accuracy: 0.9383\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1628 - accuracy: 0.9395\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1588 - accuracy: 0.9416\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1547 - accuracy: 0.9429\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1531 - accuracy: 0.9430\n",
      "188/188 [==============================] - 0s 750us/step\n",
      "12 out of 24 finished: {'activation_function': 'sigmoid', 'optimizer': 'Adam', 'optimizer__lr': 0.001}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 976us/step - loss: 0.5242 - accuracy: 0.8104\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 983us/step - loss: 0.4022 - accuracy: 0.8542\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 976us/step - loss: 0.3634 - accuracy: 0.8670\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 984us/step - loss: 0.3389 - accuracy: 0.8759\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 981us/step - loss: 0.3202 - accuracy: 0.8826\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 985us/step - loss: 0.3045 - accuracy: 0.8871\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 989us/step - loss: 0.2912 - accuracy: 0.8921\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 972us/step - loss: 0.2820 - accuracy: 0.8958\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 964us/step - loss: 0.2721 - accuracy: 0.8979\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 979us/step - loss: 0.2639 - accuracy: 0.9009\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 975us/step - loss: 0.2566 - accuracy: 0.9040\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 980us/step - loss: 0.2492 - accuracy: 0.9077\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 972us/step - loss: 0.2417 - accuracy: 0.9093\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 976us/step - loss: 0.2352 - accuracy: 0.9120\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 983us/step - loss: 0.2286 - accuracy: 0.9143\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 972us/step - loss: 0.2236 - accuracy: 0.9167\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 987us/step - loss: 0.2169 - accuracy: 0.9195\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 978us/step - loss: 0.2124 - accuracy: 0.9198\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2067 - accuracy: 0.9219\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 981us/step - loss: 0.2011 - accuracy: 0.9235\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 975us/step - loss: 0.1967 - accuracy: 0.9265\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 974us/step - loss: 0.1943 - accuracy: 0.9265\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 974us/step - loss: 0.1868 - accuracy: 0.9306\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 973us/step - loss: 0.1837 - accuracy: 0.9321\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 986us/step - loss: 0.1792 - accuracy: 0.9334\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 976us/step - loss: 0.1773 - accuracy: 0.9332\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 977us/step - loss: 0.1731 - accuracy: 0.9351\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 973us/step - loss: 0.1688 - accuracy: 0.9366\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 987us/step - loss: 0.1660 - accuracy: 0.9361\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 980us/step - loss: 0.1613 - accuracy: 0.9403\n",
      "188/188 [==============================] - 0s 720us/step\n",
      "13 out of 24 finished: {'activation_function': 'tanh', 'optimizer': 'sgd', 'optimizer__lr': 0.1}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 983us/step - loss: 0.7262 - accuracy: 0.7684\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 985us/step - loss: 0.4912 - accuracy: 0.8301\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 982us/step - loss: 0.4465 - accuracy: 0.8421\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 986us/step - loss: 0.4212 - accuracy: 0.8510\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 986us/step - loss: 0.4032 - accuracy: 0.8583\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 985us/step - loss: 0.3888 - accuracy: 0.8631\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 985us/step - loss: 0.3774 - accuracy: 0.8667\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 981us/step - loss: 0.3676 - accuracy: 0.8690\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 985us/step - loss: 0.3593 - accuracy: 0.8717\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 984us/step - loss: 0.3524 - accuracy: 0.8732\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 983us/step - loss: 0.3446 - accuracy: 0.8777\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 981us/step - loss: 0.3389 - accuracy: 0.8787\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 984us/step - loss: 0.3332 - accuracy: 0.8797\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3275 - accuracy: 0.8830\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3224 - accuracy: 0.8849\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3176 - accuracy: 0.8848\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 983us/step - loss: 0.3125 - accuracy: 0.8881\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 984us/step - loss: 0.3087 - accuracy: 0.8883\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 984us/step - loss: 0.3045 - accuracy: 0.8906\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 982us/step - loss: 0.3000 - accuracy: 0.8918\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 981us/step - loss: 0.2965 - accuracy: 0.8929\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 981us/step - loss: 0.2929 - accuracy: 0.8944\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 984us/step - loss: 0.2895 - accuracy: 0.8952\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 982us/step - loss: 0.2862 - accuracy: 0.8958\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2825 - accuracy: 0.8982\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 981us/step - loss: 0.2797 - accuracy: 0.8985\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 987us/step - loss: 0.2761 - accuracy: 0.8996\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 988us/step - loss: 0.2733 - accuracy: 0.9002\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 983us/step - loss: 0.2705 - accuracy: 0.9011\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 981us/step - loss: 0.2675 - accuracy: 0.9025\n",
      "188/188 [==============================] - 0s 723us/step\n",
      "14 out of 24 finished: {'activation_function': 'tanh', 'optimizer': 'sgd', 'optimizer__lr': 0.01}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 973us/step - loss: 1.3950 - accuracy: 0.6037\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 974us/step - loss: 0.8948 - accuracy: 0.7302\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 970us/step - loss: 0.7577 - accuracy: 0.7646\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 976us/step - loss: 0.6848 - accuracy: 0.7839\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 990us/step - loss: 0.6372 - accuracy: 0.7955\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 975us/step - loss: 0.6028 - accuracy: 0.8040\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 981us/step - loss: 0.5769 - accuracy: 0.8100\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 977us/step - loss: 0.5562 - accuracy: 0.8153\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 976us/step - loss: 0.5394 - accuracy: 0.8190\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 985us/step - loss: 0.5257 - accuracy: 0.8216\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 980us/step - loss: 0.5140 - accuracy: 0.8249\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 974us/step - loss: 0.5038 - accuracy: 0.8273\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 977us/step - loss: 0.4952 - accuracy: 0.8290\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 976us/step - loss: 0.4876 - accuracy: 0.8322\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 978us/step - loss: 0.4807 - accuracy: 0.8336\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 978us/step - loss: 0.4744 - accuracy: 0.8355\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 983us/step - loss: 0.4687 - accuracy: 0.8368\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 984us/step - loss: 0.4635 - accuracy: 0.8387\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 982us/step - loss: 0.4587 - accuracy: 0.8406\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 989us/step - loss: 0.4543 - accuracy: 0.8410\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 984us/step - loss: 0.4500 - accuracy: 0.8426\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 989us/step - loss: 0.4461 - accuracy: 0.8431\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 998us/step - loss: 0.4424 - accuracy: 0.8457\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 993us/step - loss: 0.4390 - accuracy: 0.8461\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4354 - accuracy: 0.8473\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 994us/step - loss: 0.4323 - accuracy: 0.8484\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 993us/step - loss: 0.4292 - accuracy: 0.8498\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 987us/step - loss: 0.4263 - accuracy: 0.8500\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 989us/step - loss: 0.4235 - accuracy: 0.8516\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 977us/step - loss: 0.4210 - accuracy: 0.8514\n",
      "188/188 [==============================] - 0s 709us/step\n",
      "15 out of 24 finished: {'activation_function': 'tanh', 'optimizer': 'sgd', 'optimizer__lr': 0.001}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8471 - accuracy: 0.1004\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8309 - accuracy: 0.1002\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8583 - accuracy: 0.1000\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8601 - accuracy: 0.0985\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8354 - accuracy: 0.1016\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8326 - accuracy: 0.1012\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8048 - accuracy: 0.1018\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8371 - accuracy: 0.0993\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8450 - accuracy: 0.1003\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8541 - accuracy: 0.0982\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8407 - accuracy: 0.1018\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8154 - accuracy: 0.1008\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8465 - accuracy: 0.1002\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8406 - accuracy: 0.1006\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8792 - accuracy: 0.0994\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8425 - accuracy: 0.1004\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8374 - accuracy: 0.0991\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8663 - accuracy: 0.0981\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8442 - accuracy: 0.0996\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8399 - accuracy: 0.0998\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8872 - accuracy: 0.0993\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8069 - accuracy: 0.1004\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8597 - accuracy: 0.0996\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8370 - accuracy: 0.0980\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8713 - accuracy: 0.1023\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8610 - accuracy: 0.1001\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8411 - accuracy: 0.0986\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8385 - accuracy: 0.0982\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8479 - accuracy: 0.0997\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 2.8448 - accuracy: 0.1005\n",
      "188/188 [==============================] - 0s 782us/step\n",
      "16 out of 24 finished: {'activation_function': 'tanh', 'optimizer': 'Adam', 'optimizer__lr': 0.1}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6807 - accuracy: 0.7537\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6639 - accuracy: 0.7633\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6622 - accuracy: 0.7596\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6841 - accuracy: 0.7497\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6846 - accuracy: 0.7524\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6729 - accuracy: 0.7712\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7334 - accuracy: 0.7183\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6781 - accuracy: 0.7455\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6698 - accuracy: 0.7530\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6718 - accuracy: 0.7595\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6580 - accuracy: 0.7683\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6510 - accuracy: 0.7666\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6532 - accuracy: 0.7651\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6654 - accuracy: 0.7623\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6647 - accuracy: 0.7625\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6374 - accuracy: 0.7735\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6382 - accuracy: 0.7731\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6738 - accuracy: 0.7502\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6679 - accuracy: 0.7573\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6393 - accuracy: 0.7760\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6284 - accuracy: 0.7768\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6436 - accuracy: 0.7764\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6380 - accuracy: 0.7764\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6272 - accuracy: 0.7832\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6444 - accuracy: 0.7638\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6753 - accuracy: 0.7539\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6846 - accuracy: 0.7473\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6539 - accuracy: 0.7651\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6943 - accuracy: 0.7530\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6696 - accuracy: 0.7642\n",
      "188/188 [==============================] - 0s 768us/step\n",
      "17 out of 24 finished: {'activation_function': 'tanh', 'optimizer': 'Adam', 'optimizer__lr': 0.01}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4843 - accuracy: 0.8262\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3712 - accuracy: 0.8659\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3352 - accuracy: 0.8773\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3142 - accuracy: 0.8840\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2957 - accuracy: 0.8907\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2834 - accuracy: 0.8943\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2723 - accuracy: 0.8994\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2596 - accuracy: 0.9034\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2502 - accuracy: 0.9055\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2447 - accuracy: 0.9082\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2366 - accuracy: 0.9115\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2310 - accuracy: 0.9133\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2226 - accuracy: 0.9159\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2189 - accuracy: 0.9175\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2099 - accuracy: 0.9211\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2066 - accuracy: 0.9221\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2027 - accuracy: 0.9238\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1975 - accuracy: 0.9244\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1907 - accuracy: 0.9288\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1877 - accuracy: 0.9291\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1851 - accuracy: 0.9309\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1799 - accuracy: 0.9330\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1776 - accuracy: 0.9334\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1743 - accuracy: 0.9334\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1701 - accuracy: 0.9351\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1649 - accuracy: 0.9381\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1664 - accuracy: 0.9367\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1604 - accuracy: 0.9391\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1589 - accuracy: 0.9396\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1561 - accuracy: 0.9415\n",
      "188/188 [==============================] - 0s 759us/step\n",
      "18 out of 24 finished: {'activation_function': 'tanh', 'optimizer': 'Adam', 'optimizer__lr': 0.001}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6285 - accuracy: 0.7890\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4883 - accuracy: 0.8287\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4665 - accuracy: 0.8383\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4565 - accuracy: 0.8393\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4475 - accuracy: 0.8433\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4412 - accuracy: 0.8469\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4356 - accuracy: 0.8478\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4341 - accuracy: 0.8479\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4287 - accuracy: 0.8503\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4274 - accuracy: 0.8506\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4256 - accuracy: 0.8507\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4238 - accuracy: 0.8525\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4217 - accuracy: 0.8522\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4209 - accuracy: 0.8518\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4187 - accuracy: 0.8534\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4173 - accuracy: 0.8540\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4144 - accuracy: 0.8540\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4137 - accuracy: 0.8529\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4134 - accuracy: 0.8550\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4103 - accuracy: 0.8560\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4115 - accuracy: 0.8558\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4092 - accuracy: 0.8565\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4088 - accuracy: 0.8556\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4081 - accuracy: 0.8572\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4066 - accuracy: 0.8577\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4067 - accuracy: 0.8567\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4074 - accuracy: 0.8554\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4051 - accuracy: 0.8567\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4051 - accuracy: 0.8571\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4054 - accuracy: 0.8575\n",
      "188/188 [==============================] - 0s 749us/step\n",
      "19 out of 24 finished: {'activation_function': None, 'optimizer': 'sgd', 'optimizer__lr': 0.1}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6610 - accuracy: 0.7794\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4938 - accuracy: 0.8293\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4627 - accuracy: 0.8385\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4478 - accuracy: 0.8437\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4373 - accuracy: 0.8485\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4296 - accuracy: 0.8525\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4238 - accuracy: 0.8530\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4195 - accuracy: 0.8547\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4151 - accuracy: 0.8555\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4121 - accuracy: 0.8566\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4094 - accuracy: 0.8581\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4065 - accuracy: 0.8595\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4046 - accuracy: 0.8589\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4019 - accuracy: 0.8602\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4000 - accuracy: 0.8606\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3985 - accuracy: 0.8611\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3958 - accuracy: 0.8621\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3953 - accuracy: 0.8607\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3929 - accuracy: 0.8628\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3908 - accuracy: 0.8627\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3906 - accuracy: 0.8626\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3896 - accuracy: 0.8632\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3883 - accuracy: 0.8642\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3872 - accuracy: 0.8651\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3858 - accuracy: 0.8656\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3855 - accuracy: 0.8650\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3843 - accuracy: 0.8648\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3828 - accuracy: 0.8651\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3825 - accuracy: 0.8657\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3815 - accuracy: 0.8658\n",
      "188/188 [==============================] - 0s 763us/step\n",
      "20 out of 24 finished: {'activation_function': None, 'optimizer': 'sgd', 'optimizer__lr': 0.01}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.1842 - accuracy: 0.6423\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7551 - accuracy: 0.7579\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6598 - accuracy: 0.7864\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6095 - accuracy: 0.7996\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5773 - accuracy: 0.8082\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5540 - accuracy: 0.8153\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5369 - accuracy: 0.8185\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5231 - accuracy: 0.8222\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5120 - accuracy: 0.8264\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5030 - accuracy: 0.8291\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4950 - accuracy: 0.8313\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4879 - accuracy: 0.8331\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4821 - accuracy: 0.8339\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4768 - accuracy: 0.8369\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4721 - accuracy: 0.8376\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4676 - accuracy: 0.8384\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4636 - accuracy: 0.8404\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4600 - accuracy: 0.8411\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4566 - accuracy: 0.8429\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4535 - accuracy: 0.8436\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4506 - accuracy: 0.8446\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4480 - accuracy: 0.8453\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4455 - accuracy: 0.8465\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4434 - accuracy: 0.8470\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4408 - accuracy: 0.8482\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4389 - accuracy: 0.8489\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4368 - accuracy: 0.8492\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4350 - accuracy: 0.8497\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4333 - accuracy: 0.8507\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4316 - accuracy: 0.8507\n",
      "188/188 [==============================] - 0s 759us/step\n",
      "21 out of 24 finished: {'activation_function': None, 'optimizer': 'sgd', 'optimizer__lr': 0.001}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 11.5627 - accuracy: 0.7412\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1019.8622 - accuracy: 0.7256\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 32.1146 - accuracy: 0.7461\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 15.4382 - accuracy: 0.7470\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 914.6314 - accuracy: 0.7195\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 52.7696 - accuracy: 0.7643\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 28.1203 - accuracy: 0.7514\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 833.8857 - accuracy: 0.7334\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 52.6891 - accuracy: 0.7636\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 38.0421 - accuracy: 0.7512\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 934.8671 - accuracy: 0.7513\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 41.6756 - accuracy: 0.7646\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 716.1636 - accuracy: 0.7335\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 213.9136 - accuracy: 0.7755\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 56.5122 - accuracy: 0.7596\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 735.7065 - accuracy: 0.7509\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 60.1095 - accuracy: 0.7732\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 610.9733 - accuracy: 0.7379\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 318.1247 - accuracy: 0.7743\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 59.1774 - accuracy: 0.7636\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1068.1047 - accuracy: 0.7574\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 63.0517 - accuracy: 0.7805\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 109.9158 - accuracy: 0.7563\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 353.7023 - accuracy: 0.7634\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 576.0347 - accuracy: 0.7564\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 65.1387 - accuracy: 0.7748\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 641.3946 - accuracy: 0.7534\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 77.1088 - accuracy: 0.7787\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 856.2628 - accuracy: 0.7558\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 136.4784 - accuracy: 0.7841\n",
      "188/188 [==============================] - 0s 756us/step\n",
      "22 out of 24 finished: {'activation_function': None, 'optimizer': 'Adam', 'optimizer__lr': 0.1}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7356 - accuracy: 0.7692\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8600 - accuracy: 0.7878\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5032 - accuracy: 0.8278\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5822 - accuracy: 0.8106\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6317 - accuracy: 0.8097\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.5931 - accuracy: 0.8140\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4842 - accuracy: 0.8362\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5471 - accuracy: 0.8221\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5862 - accuracy: 0.8167\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.0495 - accuracy: 0.8127\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4656 - accuracy: 0.8416\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5994 - accuracy: 0.8154\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5602 - accuracy: 0.8216\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6120 - accuracy: 0.8150\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8399 - accuracy: 0.8131\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4671 - accuracy: 0.8408\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.9075 - accuracy: 0.8171\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4823 - accuracy: 0.8362\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6057 - accuracy: 0.8153\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5552 - accuracy: 0.8232\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6068 - accuracy: 0.8156\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6159 - accuracy: 0.8217\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.8498 - accuracy: 0.8244\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6292 - accuracy: 0.8258\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4953 - accuracy: 0.8360\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 1.0496 - accuracy: 0.8182\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4680 - accuracy: 0.8424\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.7327 - accuracy: 0.8166\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5536 - accuracy: 0.8310\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.6967 - accuracy: 0.8202\n",
      "188/188 [==============================] - 0s 757us/step\n",
      "23 out of 24 finished: {'activation_function': None, 'optimizer': 'Adam', 'optimizer__lr': 0.01}\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5494 - accuracy: 0.8081\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4708 - accuracy: 0.8358\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4517 - accuracy: 0.8435\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4407 - accuracy: 0.8465\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4332 - accuracy: 0.8486\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4258 - accuracy: 0.8503\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4213 - accuracy: 0.8515\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4145 - accuracy: 0.8552\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4121 - accuracy: 0.8542\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4123 - accuracy: 0.8549\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4079 - accuracy: 0.8572\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4043 - accuracy: 0.8584\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.4021 - accuracy: 0.8588\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3993 - accuracy: 0.8600\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3970 - accuracy: 0.8600\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3963 - accuracy: 0.8599\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3944 - accuracy: 0.8606\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3921 - accuracy: 0.8602\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3914 - accuracy: 0.8627\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3906 - accuracy: 0.8622\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3899 - accuracy: 0.8620\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3882 - accuracy: 0.8628\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3872 - accuracy: 0.8637\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3878 - accuracy: 0.8625\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3855 - accuracy: 0.8639\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3839 - accuracy: 0.8640\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3828 - accuracy: 0.8639\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3820 - accuracy: 0.8646\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3806 - accuracy: 0.8659\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3806 - accuracy: 0.8650\n",
      "188/188 [==============================] - 0s 751us/step\n",
      "24 out of 24 finished: {'activation_function': None, 'optimizer': 'Adam', 'optimizer__lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Running in arround 1800s\n",
    "\n",
    "mlp_result = get_result(keras_classifier, mlp_paras, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c82c50e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for MLP:\n",
      "Best parameters: {'activation_function': 'sigmoid', 'optimizer': 'Adam', 'optimizer__lr': 0.001}\n",
      "Best validation score: 0.8940\n",
      "313/313 [==============================] - 0s 791us/step\n",
      "Test set score: 0.8886\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>training_time</th>\n",
       "      <th>validation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'relu', 'optimizer': 'sgd', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.872</td>\n",
       "      <td>49.03</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'relu', 'optimizer': 'sgd', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.8833</td>\n",
       "      <td>49.13</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'relu', 'optimizer': 'sgd', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.8488</td>\n",
       "      <td>48.98</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'relu', 'optimizer': 'Adam', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.198</td>\n",
       "      <td>52.16</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'relu', 'optimizer': 'Adam', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.8577</td>\n",
       "      <td>52.98</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'relu', 'optimizer': 'Adam', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.8927</td>\n",
       "      <td>52.72</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'sigmoid', 'optimizer': 'sgd', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.8867</td>\n",
       "      <td>51.64</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'sigmoid', 'optimizer': 'sgd', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.8455</td>\n",
       "      <td>51.55</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'sigmoid', 'optimizer': 'sgd', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.6782</td>\n",
       "      <td>51.52</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'sigmoid', 'optimizer': 'Adam', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.4063</td>\n",
       "      <td>54.35</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'sigmoid', 'optimizer': 'Adam', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.8545</td>\n",
       "      <td>53.68</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'sigmoid', 'optimizer': 'Adam', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.894</td>\n",
       "      <td>53.34</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'tanh', 'optimizer': 'sgd', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.8873</td>\n",
       "      <td>50.07</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'tanh', 'optimizer': 'sgd', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.8873</td>\n",
       "      <td>50.41</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'tanh', 'optimizer': 'sgd', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.8467</td>\n",
       "      <td>50.21</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'tanh', 'optimizer': 'Adam', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.1</td>\n",
       "      <td>57.98</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'tanh', 'optimizer': 'Adam', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.7078</td>\n",
       "      <td>57.59</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': 'tanh', 'optimizer': 'Adam', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.8857</td>\n",
       "      <td>56.22</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': None, 'optimizer': 'sgd', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.8398</td>\n",
       "      <td>52.35</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': None, 'optimizer': 'sgd', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.8512</td>\n",
       "      <td>57.59</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': None, 'optimizer': 'sgd', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.8485</td>\n",
       "      <td>52.61</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': None, 'optimizer': 'Adam', 'optimizer__lr': 0.1}</th>\n",
       "      <td>0.7832</td>\n",
       "      <td>55.43</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': None, 'optimizer': 'Adam', 'optimizer__lr': 0.01}</th>\n",
       "      <td>0.7968</td>\n",
       "      <td>55.06</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'activation_function': None, 'optimizer': 'Adam', 'optimizer__lr': 0.001}</th>\n",
       "      <td>0.8475</td>\n",
       "      <td>54.47</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Score training_time  \\\n",
       "{'activation_function': 'relu', 'optimizer': 's...   0.872         49.03   \n",
       "{'activation_function': 'relu', 'optimizer': 's...  0.8833         49.13   \n",
       "{'activation_function': 'relu', 'optimizer': 's...  0.8488         48.98   \n",
       "{'activation_function': 'relu', 'optimizer': 'A...   0.198         52.16   \n",
       "{'activation_function': 'relu', 'optimizer': 'A...  0.8577         52.98   \n",
       "{'activation_function': 'relu', 'optimizer': 'A...  0.8927         52.72   \n",
       "{'activation_function': 'sigmoid', 'optimizer':...  0.8867         51.64   \n",
       "{'activation_function': 'sigmoid', 'optimizer':...  0.8455         51.55   \n",
       "{'activation_function': 'sigmoid', 'optimizer':...  0.6782         51.52   \n",
       "{'activation_function': 'sigmoid', 'optimizer':...  0.4063         54.35   \n",
       "{'activation_function': 'sigmoid', 'optimizer':...  0.8545         53.68   \n",
       "{'activation_function': 'sigmoid', 'optimizer':...   0.894         53.34   \n",
       "{'activation_function': 'tanh', 'optimizer': 's...  0.8873         50.07   \n",
       "{'activation_function': 'tanh', 'optimizer': 's...  0.8873         50.41   \n",
       "{'activation_function': 'tanh', 'optimizer': 's...  0.8467         50.21   \n",
       "{'activation_function': 'tanh', 'optimizer': 'A...     0.1         57.98   \n",
       "{'activation_function': 'tanh', 'optimizer': 'A...  0.7078         57.59   \n",
       "{'activation_function': 'tanh', 'optimizer': 'A...  0.8857         56.22   \n",
       "{'activation_function': None, 'optimizer': 'sgd...  0.8398         52.35   \n",
       "{'activation_function': None, 'optimizer': 'sgd...  0.8512         57.59   \n",
       "{'activation_function': None, 'optimizer': 'sgd...  0.8485         52.61   \n",
       "{'activation_function': None, 'optimizer': 'Ada...  0.7832         55.43   \n",
       "{'activation_function': None, 'optimizer': 'Ada...  0.7968         55.06   \n",
       "{'activation_function': None, 'optimizer': 'Ada...  0.8475         54.47   \n",
       "\n",
       "                                                   validation_time  \n",
       "{'activation_function': 'relu', 'optimizer': 's...            0.31  \n",
       "{'activation_function': 'relu', 'optimizer': 's...            0.25  \n",
       "{'activation_function': 'relu', 'optimizer': 's...            0.25  \n",
       "{'activation_function': 'relu', 'optimizer': 'A...            0.25  \n",
       "{'activation_function': 'relu', 'optimizer': 'A...            0.26  \n",
       "{'activation_function': 'relu', 'optimizer': 'A...            0.25  \n",
       "{'activation_function': 'sigmoid', 'optimizer':...            0.25  \n",
       "{'activation_function': 'sigmoid', 'optimizer':...            0.26  \n",
       "{'activation_function': 'sigmoid', 'optimizer':...            0.26  \n",
       "{'activation_function': 'sigmoid', 'optimizer':...            0.26  \n",
       "{'activation_function': 'sigmoid', 'optimizer':...            0.25  \n",
       "{'activation_function': 'sigmoid', 'optimizer':...            0.26  \n",
       "{'activation_function': 'tanh', 'optimizer': 's...            0.25  \n",
       "{'activation_function': 'tanh', 'optimizer': 's...            0.25  \n",
       "{'activation_function': 'tanh', 'optimizer': 's...            0.25  \n",
       "{'activation_function': 'tanh', 'optimizer': 'A...            0.27  \n",
       "{'activation_function': 'tanh', 'optimizer': 'A...            0.26  \n",
       "{'activation_function': 'tanh', 'optimizer': 'A...            0.27  \n",
       "{'activation_function': None, 'optimizer': 'sgd...            0.26  \n",
       "{'activation_function': None, 'optimizer': 'sgd...            0.26  \n",
       "{'activation_function': None, 'optimizer': 'sgd...            0.26  \n",
       "{'activation_function': None, 'optimizer': 'Ada...            0.26  \n",
       "{'activation_function': None, 'optimizer': 'Ada...            0.26  \n",
       "{'activation_function': None, 'optimizer': 'Ada...            0.27  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_results(\"MLP\", mlp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098737b",
   "metadata": {},
   "source": [
    "From the table above, both {'activation_function': 'relu', 'optimizer': 'Adam', 'optimizer__lr': 0.001} and {'activation_function': 'sigmoid', 'optimizer': 'Adam', 'optimizer__lr': 0.001} are good. Considering the limitations of Sigmoid activation function, we choose **ReLU** in this study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb5d694",
   "metadata": {},
   "source": [
    "Then we observe the trend of epochs with these paras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "188af150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_35 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 60)                6060      \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 10)                610       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,170\n",
      "Trainable params: 85,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build a model with paras we just chose\n",
    "mlp_model = build_mlp(activation_function=\"relu\")\n",
    "\n",
    "mlp_model.summary()\n",
    "\n",
    "mlp_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc3a90a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.5115 - accuracy: 0.8196 - val_loss: 0.3997 - val_accuracy: 0.8535\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3767 - accuracy: 0.8640 - val_loss: 0.3651 - val_accuracy: 0.8670\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3391 - accuracy: 0.8759 - val_loss: 0.3397 - val_accuracy: 0.8723\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3171 - accuracy: 0.8829 - val_loss: 0.3278 - val_accuracy: 0.8783\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2985 - accuracy: 0.8889 - val_loss: 0.3359 - val_accuracy: 0.8768\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2863 - accuracy: 0.8931 - val_loss: 0.3239 - val_accuracy: 0.8792\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2728 - accuracy: 0.8998 - val_loss: 0.3243 - val_accuracy: 0.8797\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2595 - accuracy: 0.9026 - val_loss: 0.3194 - val_accuracy: 0.8820\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2523 - accuracy: 0.9046 - val_loss: 0.3274 - val_accuracy: 0.8808\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2444 - accuracy: 0.9075 - val_loss: 0.3019 - val_accuracy: 0.8902\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2326 - accuracy: 0.9126 - val_loss: 0.3307 - val_accuracy: 0.8810\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2266 - accuracy: 0.9146 - val_loss: 0.3046 - val_accuracy: 0.8952\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2185 - accuracy: 0.9170 - val_loss: 0.3076 - val_accuracy: 0.8952\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2130 - accuracy: 0.9197 - val_loss: 0.3079 - val_accuracy: 0.8908\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2058 - accuracy: 0.9209 - val_loss: 0.3383 - val_accuracy: 0.8852\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1997 - accuracy: 0.9235 - val_loss: 0.3601 - val_accuracy: 0.8822\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1960 - accuracy: 0.9259 - val_loss: 0.3288 - val_accuracy: 0.8913\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1888 - accuracy: 0.9283 - val_loss: 0.3342 - val_accuracy: 0.8920\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1849 - accuracy: 0.9289 - val_loss: 0.3312 - val_accuracy: 0.8890\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1831 - accuracy: 0.9306 - val_loss: 0.3298 - val_accuracy: 0.8853\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1796 - accuracy: 0.9313 - val_loss: 0.3571 - val_accuracy: 0.8872\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1696 - accuracy: 0.9346 - val_loss: 0.3485 - val_accuracy: 0.8890\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1685 - accuracy: 0.9353 - val_loss: 0.3449 - val_accuracy: 0.8975\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1644 - accuracy: 0.9372 - val_loss: 0.3374 - val_accuracy: 0.8937\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1600 - accuracy: 0.9397 - val_loss: 0.3626 - val_accuracy: 0.8902\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1554 - accuracy: 0.9404 - val_loss: 0.3714 - val_accuracy: 0.8968\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1527 - accuracy: 0.9417 - val_loss: 0.3586 - val_accuracy: 0.8905\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1472 - accuracy: 0.9431 - val_loss: 0.4041 - val_accuracy: 0.8907\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1449 - accuracy: 0.9436 - val_loss: 0.3459 - val_accuracy: 0.9018\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.1413 - accuracy: 0.9453 - val_loss: 0.3922 - val_accuracy: 0.8927\n"
     ]
    }
   ],
   "source": [
    "mlp_history = mlp_model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b64a0961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the history dictionary to a Pandas dataframe and extract the accuracies\n",
    "accuracies = pd.DataFrame(mlp_history.history)[['accuracy', 'val_accuracy']]\n",
    "\n",
    "# Plot the accuracies\n",
    "accuracies.plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0.8, 1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3dd083",
   "metadata": {},
   "source": [
    "The accuracy of validation set fluctuated since epoch = 10, thus, we set the epochs = 10 to keep it small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df470e47",
   "metadata": {},
   "source": [
    "In section 2.2, we fixed a set ot paras as well as the architecture of CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b6a59f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 combinations.\n",
      "Parameter grid:\n",
      "{'optimizer__lr': [0.01, 0.005, 0.001], 'kernel_size': [(3, 3), (5, 5)], 'strides': [(1, 1), (2, 2)]}\n"
     ]
    }
   ],
   "source": [
    "def build_cnn(kernel_size=(3, 3), strides=(1, 1), activation_function=\"relu\"):\n",
    "    \"\"\"Build a Keras CNN for 10 class classification with desired parameters.\"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        # Specify the input shape\n",
    "        keras.Input(shape=(*IMAGE_SIZE, 1)),\n",
    "        \n",
    "        # Conv and pool block 1\n",
    "        keras.layers.Conv2D(\n",
    "            32, \n",
    "            kernel_size=kernel_size, \n",
    "            activation=activation_function, \n",
    "            strides=strides,\n",
    "            kernel_initializer=initializer\n",
    "            ),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'), # padding evenly\n",
    "        \n",
    "        # Conv and pool block 2\n",
    "        keras.layers.Conv2D(\n",
    "            64, \n",
    "            kernel_size=kernel_size, \n",
    "            activation=activation_function, \n",
    "            strides=strides,\n",
    "            kernel_initializer=initializer\n",
    "            ),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'), # padding evenly\n",
    "        \n",
    "        # Flatten and classify using dense output layer\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(len(class_names), activation=\"softmax\", kernel_initializer=initializer),\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier object which works with sklearn grid searches\n",
    "# We need to pass default values of arguments in build_cnn if we wish to tune them\n",
    "keras_classifier = KerasClassifier(build_cnn,\n",
    "                                   kernel_size=(3, 3),\n",
    "                                   strides=(1, 1),\n",
    "                                   activation_function=\"relu\",\n",
    "                                   loss=\"sparse_categorical_crossentropy\",\n",
    "                                   optimizer=\"adam\",\n",
    "                                   optimizer__lr=0.01,\n",
    "                                   metrics=[\"accuracy\"]\n",
    "                                  )\n",
    "\n",
    "# For an odd-sized filter, all the previous layer pixels would be symmetrical around the output pixel.\n",
    "param_grid = {\n",
    "    \"optimizer__lr\": [0.01, 0.005, 0.001],\n",
    "    \"kernel_size\": [(3, 3), (5, 5)],\n",
    "    \"strides\": [(1, 1), (2, 2)]\n",
    "}\n",
    "\n",
    "cnn_paras = ParameterGrid(param_grid)\n",
    "\n",
    "print(f\"There are {len(list(cnn_paras))} combinations.\")\n",
    "print(\"Parameter grid:\\n{}\".format(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2f6f1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.5044 - accuracy: 0.8175\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4250 - accuracy: 0.8472\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4157 - accuracy: 0.8500\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4070 - accuracy: 0.8526\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4085 - accuracy: 0.8510\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4048 - accuracy: 0.8523\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4004 - accuracy: 0.8531\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3999 - accuracy: 0.8557\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3996 - accuracy: 0.8537\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4034 - accuracy: 0.8536\n",
      "188/188 [==============================] - 0s 2ms/step\n",
      "1 out of 12 finished: {'kernel_size': (3, 3), 'optimizer__lr': 0.01, 'strides': (1, 1)}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.6453 - accuracy: 0.7617\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5320 - accuracy: 0.8065\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5166 - accuracy: 0.8136\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5010 - accuracy: 0.8199\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4991 - accuracy: 0.8179\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4936 - accuracy: 0.8190\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4884 - accuracy: 0.8219\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4878 - accuracy: 0.8227\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4846 - accuracy: 0.8223\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4837 - accuracy: 0.8220\n",
      "188/188 [==============================] - 0s 998us/step\n",
      "2 out of 12 finished: {'kernel_size': (3, 3), 'optimizer__lr': 0.01, 'strides': (2, 2)}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4701 - accuracy: 0.8295\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3734 - accuracy: 0.8655\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3519 - accuracy: 0.8721\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3374 - accuracy: 0.8782\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3284 - accuracy: 0.8799\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3258 - accuracy: 0.8808\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3199 - accuracy: 0.8826\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3155 - accuracy: 0.8835\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3061 - accuracy: 0.8873\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3098 - accuracy: 0.8860\n",
      "188/188 [==============================] - 0s 2ms/step\n",
      "3 out of 12 finished: {'kernel_size': (3, 3), 'optimizer__lr': 0.005, 'strides': (1, 1)}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.6295 - accuracy: 0.7703\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4755 - accuracy: 0.8267\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4458 - accuracy: 0.8371\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4258 - accuracy: 0.8459\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4152 - accuracy: 0.8508\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.4043 - accuracy: 0.8530\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3981 - accuracy: 0.8532\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3947 - accuracy: 0.8547\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.3920 - accuracy: 0.8544\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3892 - accuracy: 0.8576\n",
      "188/188 [==============================] - 0s 1ms/step\n",
      "4 out of 12 finished: {'kernel_size': (3, 3), 'optimizer__lr': 0.005, 'strides': (2, 2)}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.5201 - accuracy: 0.8115\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3668 - accuracy: 0.8700\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.3272 - accuracy: 0.8831\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3026 - accuracy: 0.8908\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2882 - accuracy: 0.8964\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.2735 - accuracy: 0.9010\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2611 - accuracy: 0.9049\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2527 - accuracy: 0.9085\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2463 - accuracy: 0.9091\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2414 - accuracy: 0.9126\n",
      "188/188 [==============================] - 1s 2ms/step\n",
      "5 out of 12 finished: {'kernel_size': (3, 3), 'optimizer__lr': 0.001, 'strides': (1, 1)}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.7486 - accuracy: 0.7271\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.5156 - accuracy: 0.8138\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4618 - accuracy: 0.8321\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4325 - accuracy: 0.8451\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4119 - accuracy: 0.8521\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.3971 - accuracy: 0.8567\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3840 - accuracy: 0.8605\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3750 - accuracy: 0.8638\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3660 - accuracy: 0.8673\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3599 - accuracy: 0.8704\n",
      "188/188 [==============================] - 0s 992us/step\n",
      "6 out of 12 finished: {'kernel_size': (3, 3), 'optimizer__lr': 0.001, 'strides': (2, 2)}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.5870 - accuracy: 0.7874\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4884 - accuracy: 0.8250\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4843 - accuracy: 0.8254\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4742 - accuracy: 0.8286\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4735 - accuracy: 0.8298\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4656 - accuracy: 0.8309\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4777 - accuracy: 0.8279\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.4648 - accuracy: 0.8329\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.4661 - accuracy: 0.8282\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.4647 - accuracy: 0.8328\n",
      "188/188 [==============================] - 1s 3ms/step\n",
      "7 out of 12 finished: {'kernel_size': (5, 5), 'optimizer__lr': 0.01, 'strides': (1, 1)}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.6395 - accuracy: 0.7672\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.5430 - accuracy: 0.8005\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5268 - accuracy: 0.8028\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5178 - accuracy: 0.8073\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5118 - accuracy: 0.8103\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5085 - accuracy: 0.8110\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5047 - accuracy: 0.8121\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.5016 - accuracy: 0.8131\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.4968 - accuracy: 0.8129\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4983 - accuracy: 0.8140\n",
      "188/188 [==============================] - 0s 1ms/step\n",
      "8 out of 12 finished: {'kernel_size': (5, 5), 'optimizer__lr': 0.01, 'strides': (2, 2)}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.5323 - accuracy: 0.8076\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.4223 - accuracy: 0.8460\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.3962 - accuracy: 0.8539\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3853 - accuracy: 0.8608\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3756 - accuracy: 0.8621\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3715 - accuracy: 0.8636\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3687 - accuracy: 0.8651\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3588 - accuracy: 0.8683\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3582 - accuracy: 0.8688\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3554 - accuracy: 0.8701\n",
      "188/188 [==============================] - 1s 3ms/step\n",
      "9 out of 12 finished: {'kernel_size': (5, 5), 'optimizer__lr': 0.005, 'strides': (1, 1)}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.6417 - accuracy: 0.7657\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4959 - accuracy: 0.8187\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4638 - accuracy: 0.8282\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4484 - accuracy: 0.8337\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4397 - accuracy: 0.8359\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4279 - accuracy: 0.8392\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4168 - accuracy: 0.8424\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4161 - accuracy: 0.8434\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4096 - accuracy: 0.8454\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4080 - accuracy: 0.8471\n",
      "188/188 [==============================] - 0s 1ms/step\n",
      "10 out of 12 finished: {'kernel_size': (5, 5), 'optimizer__lr': 0.005, 'strides': (2, 2)}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 15s 9ms/step - loss: 0.5577 - accuracy: 0.7958\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3883 - accuracy: 0.8614\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3465 - accuracy: 0.8748\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3230 - accuracy: 0.8843\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3048 - accuracy: 0.8896\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2918 - accuracy: 0.8934\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2797 - accuracy: 0.8995\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2705 - accuracy: 0.9016\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2617 - accuracy: 0.9042\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2557 - accuracy: 0.9061\n",
      "188/188 [==============================] - 1s 3ms/step\n",
      "11 out of 12 finished: {'kernel_size': (5, 5), 'optimizer__lr': 0.001, 'strides': (1, 1)}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Anaconda3-2021.11x64\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.7298 - accuracy: 0.7438\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.5274 - accuracy: 0.8151\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4729 - accuracy: 0.8302\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4409 - accuracy: 0.8399\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4174 - accuracy: 0.8481\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.4015 - accuracy: 0.8536\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3896 - accuracy: 0.8577\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3779 - accuracy: 0.8606\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3678 - accuracy: 0.8639\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.3632 - accuracy: 0.8647\n",
      "188/188 [==============================] - 0s 997us/step\n",
      "12 out of 12 finished: {'kernel_size': (5, 5), 'optimizer__lr': 0.001, 'strides': (2, 2)}\n"
     ]
    }
   ],
   "source": [
    "# Running in arround 1100s\n",
    "\n",
    "cnn_result = get_result(\n",
    "    keras_classifier, \n",
    "    cnn_paras,\n",
    "    X_train=np.expand_dims(X_train, -1),\n",
    "    X_valid=np.expand_dims(X_valid, -1), \n",
    "    epochs=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caa7df72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for CNN:\n",
      "Best parameters: {'kernel_size': (3, 3), 'optimizer__lr': 0.001, 'strides': (1, 1)}\n",
      "Best validation score: 0.9175\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "Test set score: 0.9125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>training_time</th>\n",
       "      <th>validation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{'kernel_size': (3, 3), 'optimizer__lr': 0.01, 'strides': (1, 1)}</th>\n",
       "      <td>0.8717</td>\n",
       "      <td>130.88</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'kernel_size': (3, 3), 'optimizer__lr': 0.01, 'strides': (2, 2)}</th>\n",
       "      <td>0.8602</td>\n",
       "      <td>43.34</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'kernel_size': (3, 3), 'optimizer__lr': 0.005, 'strides': (1, 1)}</th>\n",
       "      <td>0.8988</td>\n",
       "      <td>130.16</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'kernel_size': (3, 3), 'optimizer__lr': 0.005, 'strides': (2, 2)}</th>\n",
       "      <td>0.889</td>\n",
       "      <td>42.69</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'kernel_size': (3, 3), 'optimizer__lr': 0.001, 'strides': (1, 1)}</th>\n",
       "      <td>0.9175</td>\n",
       "      <td>138.12</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'kernel_size': (3, 3), 'optimizer__lr': 0.001, 'strides': (2, 2)}</th>\n",
       "      <td>0.889</td>\n",
       "      <td>45.52</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'kernel_size': (5, 5), 'optimizer__lr': 0.01, 'strides': (1, 1)}</th>\n",
       "      <td>0.8502</td>\n",
       "      <td>136.82</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'kernel_size': (5, 5), 'optimizer__lr': 0.01, 'strides': (2, 2)}</th>\n",
       "      <td>0.8483</td>\n",
       "      <td>44.92</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'kernel_size': (5, 5), 'optimizer__lr': 0.005, 'strides': (1, 1)}</th>\n",
       "      <td>0.8833</td>\n",
       "      <td>141.77</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'kernel_size': (5, 5), 'optimizer__lr': 0.005, 'strides': (2, 2)}</th>\n",
       "      <td>0.8688</td>\n",
       "      <td>43.64</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'kernel_size': (5, 5), 'optimizer__lr': 0.001, 'strides': (1, 1)}</th>\n",
       "      <td>0.9137</td>\n",
       "      <td>140.6</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'kernel_size': (5, 5), 'optimizer__lr': 0.001, 'strides': (2, 2)}</th>\n",
       "      <td>0.8857</td>\n",
       "      <td>43.86</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Score training_time  \\\n",
       "{'kernel_size': (3, 3), 'optimizer__lr': 0.01, ...  0.8717        130.88   \n",
       "{'kernel_size': (3, 3), 'optimizer__lr': 0.01, ...  0.8602         43.34   \n",
       "{'kernel_size': (3, 3), 'optimizer__lr': 0.005,...  0.8988        130.16   \n",
       "{'kernel_size': (3, 3), 'optimizer__lr': 0.005,...   0.889         42.69   \n",
       "{'kernel_size': (3, 3), 'optimizer__lr': 0.001,...  0.9175        138.12   \n",
       "{'kernel_size': (3, 3), 'optimizer__lr': 0.001,...   0.889         45.52   \n",
       "{'kernel_size': (5, 5), 'optimizer__lr': 0.01, ...  0.8502        136.82   \n",
       "{'kernel_size': (5, 5), 'optimizer__lr': 0.01, ...  0.8483         44.92   \n",
       "{'kernel_size': (5, 5), 'optimizer__lr': 0.005,...  0.8833        141.77   \n",
       "{'kernel_size': (5, 5), 'optimizer__lr': 0.005,...  0.8688         43.64   \n",
       "{'kernel_size': (5, 5), 'optimizer__lr': 0.001,...  0.9137         140.6   \n",
       "{'kernel_size': (5, 5), 'optimizer__lr': 0.001,...  0.8857         43.86   \n",
       "\n",
       "                                                   validation_time  \n",
       "{'kernel_size': (3, 3), 'optimizer__lr': 0.01, ...            0.54  \n",
       "{'kernel_size': (3, 3), 'optimizer__lr': 0.01, ...            0.31  \n",
       "{'kernel_size': (3, 3), 'optimizer__lr': 0.005,...            0.54  \n",
       "{'kernel_size': (3, 3), 'optimizer__lr': 0.005,...            0.31  \n",
       "{'kernel_size': (3, 3), 'optimizer__lr': 0.001,...             0.6  \n",
       "{'kernel_size': (3, 3), 'optimizer__lr': 0.001,...            0.31  \n",
       "{'kernel_size': (5, 5), 'optimizer__lr': 0.01, ...            0.61  \n",
       "{'kernel_size': (5, 5), 'optimizer__lr': 0.01, ...            0.32  \n",
       "{'kernel_size': (5, 5), 'optimizer__lr': 0.005,...            0.61  \n",
       "{'kernel_size': (5, 5), 'optimizer__lr': 0.005,...            0.32  \n",
       "{'kernel_size': (5, 5), 'optimizer__lr': 0.001,...            0.61  \n",
       "{'kernel_size': (5, 5), 'optimizer__lr': 0.001,...            0.31  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_results(\"CNN\", cnn_result, X_test=np.expand_dims(X_test, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c783e6",
   "metadata": {},
   "source": [
    "From the table above, the best paras in our setting is: 'kernel_size': (3, 3), 'optimizer__lr': 0.001, 'strides': (1, 1)   \n",
    "\n",
    "Then we observe the trend of epochs with these paras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6328a3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_26 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPoolin  (None, 13, 13, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPoolin  (None, 6, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_48 (Flatten)        (None, 2304)              0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 10)                23050     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,866\n",
      "Trainable params: 41,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build a model with paras we just chose\n",
    "cnn_model = build_cnn(\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    activation_function=\"relu\"\n",
    "    )\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "cnn_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15d63815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.5201 - accuracy: 0.8115 - val_loss: 0.3583 - val_accuracy: 0.8712\n",
      "Epoch 2/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3668 - accuracy: 0.8700 - val_loss: 0.3091 - val_accuracy: 0.8873\n",
      "Epoch 3/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3272 - accuracy: 0.8831 - val_loss: 0.2851 - val_accuracy: 0.8962\n",
      "Epoch 4/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.3026 - accuracy: 0.8908 - val_loss: 0.2628 - val_accuracy: 0.9043\n",
      "Epoch 5/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2882 - accuracy: 0.8964 - val_loss: 0.2507 - val_accuracy: 0.9078\n",
      "Epoch 6/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2735 - accuracy: 0.9010 - val_loss: 0.2696 - val_accuracy: 0.8988\n",
      "Epoch 7/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2611 - accuracy: 0.9049 - val_loss: 0.2415 - val_accuracy: 0.9127\n",
      "Epoch 8/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2527 - accuracy: 0.9085 - val_loss: 0.2334 - val_accuracy: 0.9147\n",
      "Epoch 9/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2463 - accuracy: 0.9091 - val_loss: 0.2297 - val_accuracy: 0.9177\n",
      "Epoch 10/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2414 - accuracy: 0.9126 - val_loss: 0.2257 - val_accuracy: 0.9175\n",
      "Epoch 11/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2351 - accuracy: 0.9136 - val_loss: 0.2237 - val_accuracy: 0.9165\n",
      "Epoch 12/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2296 - accuracy: 0.9164 - val_loss: 0.2211 - val_accuracy: 0.9190\n",
      "Epoch 13/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2274 - accuracy: 0.9159 - val_loss: 0.2206 - val_accuracy: 0.9188\n",
      "Epoch 14/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2206 - accuracy: 0.9191 - val_loss: 0.2210 - val_accuracy: 0.9165\n",
      "Epoch 15/30\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2181 - accuracy: 0.9215 - val_loss: 0.2265 - val_accuracy: 0.9162\n",
      "Epoch 16/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2152 - accuracy: 0.9215 - val_loss: 0.2166 - val_accuracy: 0.9198\n",
      "Epoch 17/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2128 - accuracy: 0.9211 - val_loss: 0.2193 - val_accuracy: 0.9198\n",
      "Epoch 18/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2079 - accuracy: 0.9229 - val_loss: 0.2151 - val_accuracy: 0.9217\n",
      "Epoch 19/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2018 - accuracy: 0.9259 - val_loss: 0.2156 - val_accuracy: 0.9220\n",
      "Epoch 20/30\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.2064 - accuracy: 0.9232 - val_loss: 0.2105 - val_accuracy: 0.9235\n",
      "Epoch 21/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.1998 - accuracy: 0.9259 - val_loss: 0.2279 - val_accuracy: 0.9172\n",
      "Epoch 22/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.2003 - accuracy: 0.9269 - val_loss: 0.2213 - val_accuracy: 0.9203\n",
      "Epoch 23/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.1965 - accuracy: 0.9279 - val_loss: 0.2173 - val_accuracy: 0.9200\n",
      "Epoch 24/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.1913 - accuracy: 0.9299 - val_loss: 0.2124 - val_accuracy: 0.9252\n",
      "Epoch 25/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.1930 - accuracy: 0.9283 - val_loss: 0.2219 - val_accuracy: 0.9203\n",
      "Epoch 26/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.1909 - accuracy: 0.9301 - val_loss: 0.2231 - val_accuracy: 0.9203\n",
      "Epoch 27/30\n",
      "1688/1688 [==============================] - 13s 8ms/step - loss: 0.1885 - accuracy: 0.9303 - val_loss: 0.2157 - val_accuracy: 0.9202\n",
      "Epoch 28/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.1845 - accuracy: 0.9315 - val_loss: 0.2181 - val_accuracy: 0.9218\n",
      "Epoch 29/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.1838 - accuracy: 0.9319 - val_loss: 0.2164 - val_accuracy: 0.9238\n",
      "Epoch 30/30\n",
      "1688/1688 [==============================] - 14s 8ms/step - loss: 0.1820 - accuracy: 0.9326 - val_loss: 0.2155 - val_accuracy: 0.9225\n"
     ]
    }
   ],
   "source": [
    "cnn_history = cnn_model.fit(\n",
    "    np.expand_dims(X_train, -1),\n",
    "    y_train, \n",
    "    epochs=30, \n",
    "    validation_data=(np.expand_dims(X_valid, -1), y_valid)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "910663a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAFBCAYAAACIOv02AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABDHklEQVR4nO3deXzU1b3/8dcnk5VsbCGERUBAdhBFVLA1iLtV1IJirQt1qbcurd57q/W2t/baxXqrrba2Xlpp9VcsWi11KXUBDdYdVATZwyYhO1sSINvM+f3xnYQhTpJJCCTDvJ+Pxzxm5rvPYch7zvd7vueYcw4RERGJLnGdfQAiIiLSdgpwERGRKKQAFxERiUIKcBERkSikABcREYlCCnAREZEo1GqAm9k8Mys1s8+amW9m9qiZ5ZvZSjM7KWTe+Wa2PjjvnpDpPc3sdTPbGHzu0TEfR0REJDZEUgP/E3B+C/MvAIYHHzcDvwMwMx/wWHD+aOAqMxsdXOceYIlzbjiwJPheREREItRqgDvn3gJ2tbDIDOAp53kf6G5mOcBkIN85t9k5VwssCC7bsM6TwddPApe28/hFRERiUkdcA+8PbA95XxCc1tx0gGznXBFA8LlPBxyHiIhIzIjvgG1YmGmuhelt27jZzXin5klJSTl54MCBbd1EswKBAHFxasfXlMolPJVLeCqX8FQu4alcwmuuXDZs2FDunMsKt05HBHgBEJqqA4BCILGZ6QAlZpbjnCsKnm4vbW7jzrm5wFyASZMmueXLl3fAIXvy8vLIzc3tsO0dK1Qu4alcwlO5hKdyCU/lEl5z5WJm25pbpyN+Br0IXBtsjX4asDd4WnwZMNzMhphZIjA7uGzDOtcFX18HvNABxyEiIhIzWq2Bm9lfgFygt5kVAD8EEgCcc48Di4ALgXxgPzAnOK/ezG4DXgV8wDzn3OrgZh8AnjWzG4DPgVkd+JlERESOea0GuHPuqlbmO+DWZuYtwgv4ptN3AtMjPEYRERFpoiOugYuISJSpq6ujoKCA6urqo7rfzMxM1q5de1T3GQ3S0tKoq6sjISEh4nUU4CIiMaigoID09HQGDx6MWbibho6MyspK0tPTj9r+ooFzjoKCAgoKChgyZEjE66ktv4hIDKqurqZXr15HNbwlPDMjMzOzzWdDFOAiIjFK4d11tOffQgEuIiIShRTgIiJyTKuvr+/sQzgiFOAiItJpLr30Uk4++WTGjBnD3LlzAXjllVc46aSTmDBhAtOne3ccV1VVMWfOHMaNG8f48eN5/vnnAa/1doPnnnuO66+/HoDrr7+eu+66i2nTpnH33Xfz4YcfMmXKFCZOnMiUKVNYv349AH6/n//4j/9o3O6vf/1rlixZwmWXXda43ddff53LL7/8aBRHm6gVuoiIdJp58+bRs2dPDhw4wCmnnMKMGTO46aabeOuttxgyZAi7dnmDYd5///1kZmayatUqAHbv3t3qtjds2MDixYvx+XxUVFTw1ltvER8fz+LFi7n33nt5/vnnmTt3Llu2bOGTTz4hPj6eXbt20aNHD2699VbKysrIysrij3/8I3PmzDmi5dAeCnARkRj3o5dWs6awokO3ObpfBj+8eEyryz366KMsXLgQgO3btzN37ly+/OUvN95O1bNnTwAWL17MggULGtfr0aNHq9ueNWsWPp8PgL1793LdddexceNGzIy6urrG7d5yyy3Ex8cfsr9rrrmGP//5z8yZM4f33nuPp556KtKPftQowEVEpFPk5eWxePFi3nvvPbp160Zubi4TJkxoPL0dyjkXtqV26LSmt2GlpqY2vv7BD37AtGnTWLhwIVu3bm0cOKS57c6ZM4eLL76Y5ORkZs2a1RjwXUnXOyIRETmqIqkpHwl79+6lR48edOvWjXXr1vH+++9TU1PD0qVL2bJlS+Mp9J49e3Luuefym9/8hl/96leAdwq9R48eZGdns3btWkaMGMHChQub7SRm79699O/fH4A//elPjdPPPfdcHn/8cXJzcxtPoffs2ZN+/frRr18/fvzjH/P6668f6aJoFzViExGRTnH++edTX1/P+PHj+cEPfsBpp51GVlYWc+fO5fLLL2fChAlceeWVAHz/+99n9+7djB07lgkTJvDmm28C8MADD/CVr3yFs846i5ycnGb39d3vfpfvfe97TJ06Fb/f3zj9xhtv5LjjjmP8+PFMmDCBp59+unHe1VdfzcCBAxk9evQRKoHDoxq4iIh0iqSkJP75z3+GnXfBBRcc8j4tLY0nn3zyC8vNnDmTmTNnfmF6aC0b4PTTT2fDhg2N7++//34A4uPjefjhh3n44Ye/sI23336bm266qdXP0VkU4CIiIk2cfPLJpKam8tBDD3X2oTRLAS4iItLERx991NmH0CpdAxcREYlCCnAREZEopAAXERGJQgpwERGRKKQAFxERiUIKcBERiQqhI4+JAlxERKRNusr44gpwERHpFHfffTe//e1vG9/fd999/OhHP2L69OmcdNJJjBs3jhdeeCGibVVVVTW73lNPPdXYVeo111wDQElJCZdddhkTJkxgwoQJvPvuu2zdupWxY8c2rveLX/yC++67D4Dc3FzuvfdezjzzTB555BFeeuklTj31VCZOnMjZZ59NSUlJ43E0Hbf8iSee4M4772zc7u9//3vuuuuudpdbA3XkIiIS6/55DxSv6tht9h0HFzzQ4iKzZ8/mO9/5Dt/61rcAePbZZ3nllVe48847ycjIoLy8nNNOO41LLrkk7IhhoZKTk1m4cOEX1luzZg0/+clPeOedd+jdu3fj+OJ33HEHZ555JgsXLsTv91NVVdXqGON79uxh6dKlgDeYyvvvv4+Z8Yc//IEHH3yQhx56KOy45YmJiYwfP54HH3yQhIQE/vjHP/J///d/ERVjSxTgIiLSKSZOnEhpaSmFhYWUlZXRo0cPcnJyuPPOO3nrrbeIi4tjx44dlJSU0Ldv3xa35Zzj3nvv/cJ6b7zxBjNnzqR3797AwfG+33jjjcYxvn0+H5mZma0GeMPAKgAFBQVceeWVFBUVUVtb2zh+eXPjlp911lm8/PLLjBo1irq6OsaNG9fG0vqiiALczM4HHgF8wB+ccw80md8DmAcMBaqBbzjnPjOzEcAzIYseD/y3c+5XZnYfcBNQFpx3r3Nu0eF8GBERaYdWaspH0syZM3nuuecoLi5m9uzZzJ8/n7KyMj766CMSEhIYPHjwF8b5Dqe59Zob7zuc+Ph4AoFA4/uWxhe//fbbueuuu7jkkkvIy8trPNXe3P5uvPFGfvrTnzJy5EjmzJkT0fG0ptVr4GbmAx4DLgBGA1eZWdOx1e4FVjjnxgPX4oU9zrn1zrkTnXMnAicD+4GFIev9smG+wltEJPbMnj2bBQsW8NxzzzFz5kz27t1Lnz59SEhI4M0332Tbtm0Rbae59aZPn86zzz7Lzp07ARpPoU+fPp3f/e53APj9fioqKsjOzqa0tJSdO3dSU1PDyy+/3OL+GsYXDx0lrWHc8gYNtfpTTz2V7du38/TTT3PVVVdFWjwtiqQR22Qg3zm32TlXCywAZjRZZjSwBMA5tw4YbGbZTZaZDmxyzkX2ryEiIse8MWPGUFlZSf/+/cnJyeHqq69m+fLlTJo0ifnz5zNy5MiIttPcemPGjOG//uu/OPPMM5kwYUJj47FHHnmEN998k3HjxnHyySezevVqEhIS+O///m9OPfVUvvKVr7S47/vuu49Zs2bxpS99qfH0PDQ/bjnAFVdcwdSpUxtPqx+uSE6h9we2h7wvAE5tssynwOXA22Y2GRgEDABKQpaZDfylyXq3mdm1wHLg351zLV+AEBGRY05Dgy+A3r17895774VdrqqqqtlttLTeddddx3XXXXfItOzs7LAt3O+44w7uuOOOL0zPy8s75P2MGTOYMaNpXbb5ccvBG188tDX64TLnXMsLmM0CznPO3Rh8fw0w2Tl3e8gyGXinzScCq4CRwI3OuU+D8xOBQmCMc64kOC0bKAcccD+Q45z7Rpj93wzcDJCdnX1yaOOAw1VVVaWOAcJQuYSncglP5RJeVy+XzMxMhg0bdtT36/f78fl8R32/nWnPnj1MmzaNcePGNTaca8rv97Nlyxb27t17yPRp06Z95JybFG6dSGrgBcDAkPcD8MK4kXOuApgDYN7V+y3BR4MLgI8bwju4TuNrM/s9EPZig3NuLjAXYNKkSS43NzeCQ45MXl4eHbm9Y4XKJTyVS3gql/C6ermsXbuW9PT0o77fysrKw9rvqlWrGu/lbpCUlMQHH3xwuId2xKSnp5Ofn9/iMpWVlSQnJzNx4sSItxtJgC8DhpvZEGAH3qnwr4UuYGbdgf3Ba+Q3Am8FQ73BVTQ5fW5mOc65ouDby4DPIj5qERGJSePGjWPFihWdfRhdQqsB7pyrN7PbgFfxbiOb55xbbWa3BOc/DowCnjIzP7AGuKFhfTPrBpwDfLPJph80sxPxTqFvDTNfRESOoLbcYiVHVmuXs8OJ6D7w4C1ei5pMezzk9XvA8GbW3Q/0CjP9mjCLi4jIUZCcnMzOnTvp1auXQryTOefYu3cvycnJbVpPPbGJiMSgAQMGUFBQQFlZWesLd6Dq6uo2B1Us2LdvHxMmTGjTOgpwEZEYlJCQ0Nj959GUl5fXpoZasSIvL4+EhIQ2raPRyERERKKQAlxERCQKKcBFRESikAJcREQkCinARUREopACXEREJAopwEVERKKQAlxERCQKKcBFRESikAJcREQkCinARUREopACXEREJAopwEVERKKQAlxERCQKKcBFRESikAJcREQkCinARUREopACXEREJAopwEVERKKQAlxERCQKKcBFRESikAJcREQkCinARUREolBEAW5m55vZejPLN7N7wszvYWYLzWylmX1oZmND5m01s1VmtsLMlodM72lmr5vZxuBzj475SCIiIse+VgPczHzAY8AFwGjgKjMb3WSxe4EVzrnxwLXAI03mT3POneicmxQy7R5giXNuOLAk+F5EREQiEEkNfDKQ75zb7JyrBRYAM5osMxovhHHOrQMGm1l2K9udATwZfP0kcGmkBy0iIhLrIgnw/sD2kPcFwWmhPgUuBzCzycAgYEBwngNeM7OPzOzmkHWynXNFAMHnPm0/fBERkdgUH8EyFmaaa/L+AeARM1sBrAI+AeqD86Y65wrNrA/wupmtc869FekBBkP/ZoDs7Gzy8vIiXbVVVVVVHbq9Y4XKJTyVS3gql/BULuGpXMJrT7lEEuAFwMCQ9wOAwtAFnHMVwBwAMzNgS/CBc64w+FxqZgvxTsm/BZSYWY5zrsjMcoDScDt3zs0F5gJMmjTJ5ebmRvzhWpOXl0dHbu9YoXIJT+USnsolPJVLeCqX8NpTLpGcQl8GDDezIWaWCMwGXgxdwMy6B+cB3Ai85ZyrMLNUM0sPLpMKnAt8FlzuReC64OvrgBfadOQiIiIxrNUauHOu3sxuA14FfMA859xqM7slOP9xYBTwlJn5gTXADcHVs4GFXqWceOBp59wrwXkPAM+a2Q3A58CsjvtYIiIix7ZITqHjnFsELGoy7fGQ1+8Bw8OstxmY0Mw2dwLT23KwIiIi4lFPbCIiIlFIAS4iIhKFFOAiIiJRSAEuIiIShRTgIiIiUUgBLiIiEoUU4CIiIlFIAS4iIhKFFOAiIiJRSAEuIiIShRTgIiIiUUgBLiIiEoUU4CIiIlFIAS4iIhKFFOAiIiJRSAEuIiIShRTgIiIiUSi+sw9AREQkmvkDjsrqOvYeqMMfcByflXZU9qsAFxGRmOUPOPbX1nOgzs+BWj/7a/0cqPNTVV3P3gN1jY+KkNdNH1U19TjnbW9Mvwz+cceXjsqxK8BFRCTq1fsDlFTWsGP3AXbs2U/hnmoKdh9g174a9tf6qa4LhnMwoBuCurY+ENH2E+PjyExJaHxkZyRzQnY6mSkJZIRMz8lMPsKf9CAFuIiIdHnVdX527DkQDOiQ5+Dr4opq/AF3yDq9UhPpnZZESqKPlAQffTMSSE700S3BR7dEX/B1PCmJcaQkxtMtwectm+gjLSn+kMBOTvB10idvngJcRCTGVNf52VBSyWc7KjhQ52doVirD+qTRLzOFuDjrsP0EAo7Pd+1nXXEFa4sqWVdcQX7BAR7+7G3q/Y6Ac9QHHP6Aoz4QIBCA+kAAf+O0g89Na8pxBjmZKfTvnsIpg3vQv0cK/bt3Cz57j5TErhe6HUkBLiJyDKusrmNNYQWrGx97yS+tor5JbRUgJcHH0D6pDMtKY2hWGsP6eI9BvVJJjG/5pqW9++tYV1zBuuLKxsDeUFLJ/lo/4AXu4N6pdIs3eqUm4ouLIz7O8AUf8XFGXPDZF2Z6WmL8wXDukULfjGTifbF9I5UCXETkGFFeVcPqwgo+27E3GNp72bpzf+P8rPQkxvTL4OxR2Yzpl8GYfpmkJvnYVLaP/NIq71FWxbKtu/n7isLG9eLjjON6dWNYSKjH++JYVxQM7KIKCvdWNy7fvVsCo/pmcOUpAxnVN4OROekM75NOSqKPvLw8cnMnH9VyOVYpwEVEurhAwLF7fy1lVTWUVtRQVllDaaX37E2rZuvOfZRU1DSuc1zPbozpl8HMkwcwpn8mY/pl0Cc9fAOrXmlJTB7S85Bp+2rq2Vy2j/yyyoPhXlrFG+tKG2vv8XHG0Kw0Jg/pycicDEb2TWdUTgZ90pMw67hT8RJeRAFuZucDjwA+4A/OuQeazO8BzAOGAtXAN5xzn5nZQOApoC8QAOY65x4JrnMfcBNQFtzMvc65RYf9iUREokS9P0B5VS0lFdWNj4ZgbgzoyhrKq2rCnvJOTfTRJyOZrLQkpg7tzehgrXp0vwwyUxIO69hSk+IZNyCTcQMyD5le5w+wbec+ausdQ/ukkhR/bF9n7spaDXAz8wGPAecABcAyM3vRObcmZLF7gRXOucvMbGRw+elAPfDvzrmPzSwd+MjMXg9Z95fOuV905AcSEelszjkqax1riypCwrnmC6/Lq2pomstx5tWI+6QnkZWexKicdLLSk8hKS/LCOvg6Kz2J1KSjfxI1wRfHsD7pR32/8kWR/OtPBvKdc5sBzGwBMAMIDfDRwM8AnHPrzGywmWU754qAouD0SjNbC/Rvsq6ISIfbX1vPe5t2snRDGXnry6ioruPUIT05Y1hvpgzrzfG9Uzv0NG/R3gO8k7+Td/PLeTu/nNLKGnjjX4cs0zM1keyMZLIzkhidk0F2RhLZmclkpyc3Tu+VloSvA1uCy7ErkgDvD2wPeV8AnNpkmU+By4G3zWwyMAgYAJQ0LGBmg4GJwAch691mZtcCy/Fq6rvb+gFERMCr9W4q20fe+lKWbijjgy27qK0PkJLgY8rQXvRITeS9TTt5dbX3Z6lvRjJThvXijGG9mTqsN9kZbeuAY++BOt7fvJN38st5J7+cTWX7AO/e4ynDepNWU86XTh7rhXSw5txlTjcH/LB6Ibz1v3BgN5z6TZh0A6R07+wjkzYw5754XeWQBcxmAec5524Mvr8GmOycuz1kmQy8a+QTgVXASOBG59ynwflpwFLgJ865vwWnZQPlgAPuB3Kcc98Is/+bgZsBsrOzT16wYMFhfeBQVVVVpKUdnT5ro4nKJTyVS3idWS7V9Y61u/ysLPOzqtxP+QHv71m/VGNclo9xveM5oUcciT6vRuuco3S/Y81OP2t2+Vm7009VnbetnFRjdC8fo3v5GNnTR2rCobXgWr9j054Aq3f6WbPTz5a9ARyQ6IORPXzBdeMYkB5HnFnX/L64AFll7zB46zOk7t/Ovm4DqUnqRc/dK6j3daOw3/kUDLiY2qSerW+rnbpkuYRyAVIOFOHz11Cd3If6hKNzrM2Vy7Rp0z5yzk0Kt04kAX46cJ9z7rzg++8BOOd+1szyBmwBxjvnKswsAXgZeNU593Az6wwGXnbOjW3pWCZNmuSWL1/e4vG2hXc7Q26Hbe9YoXIJT+US3tEsF+ccG0uryFtfSt76MpZt3UWd39Et0ceUob3JHZHFmSdkMbBnt4i2Fwg41hRV8O6mct7dWMr2rRvJ8RdyvBUzKX0XY5LLqE/qwe/iruKf2+OpqQ/gizNOHNidqcN6c8aw3pw4sHvYe6S71PclEIA1f4elP4eydZA1Es68G0ZfCnFxULQS3vmVVyuPS4ATvwZTbodeQzv8ULpUufjroXw9FH0afKyE4pVQW3VwmeRM6D4IegwKPg/2Ht0HQffjIKFjuk5trlzMrNkAj+QU+jJguJkNAXYAs4GvNdlBd2C/c64WuBF4KxjeBjwBrG0a3maWE7xGDnAZ8FkExyIiMaK6zs/GkirWFlWwtrjCey6qZO8Br8p8QnYac6YOIfeELCYN7tlqRyMAOAeVRbBzE+zaRNzOfMbu3MzYXZu4edcW8NV499oA1dVJbN7flyG2nAdsMRcNuhHflG9xytBs0pMPr4X3URMIwNoXIO/nULYWeo+AmfOCwR1yOj9nvDf9rO/Du7+GT+bDx096y53xHciZ0EkfoAPVVUPpai+kGwK7dA3UB+9fT+gGfcfBhKu8z5ucAbu3wZ5t3nPZetjwGvhrDt1uWl8v3BtCvc9IGPvVo/KRWg1w51y9md0GvIr31Z7nnFttZrcE5z8OjAKeMjM/XgO1G4KrTwWuAVaZ2YrgtIbbxR40sxPxTqFvBb7ZUR9KRNomEHBUVNc1vjes4YX3dOjbxsZfhndq2TnX7gZhzjnKKmtYEwxoL6gr2Fy+r7Fv65QEH+OyE7lx6F5OzNzHmD5J9EzcA/X5UFYDRdXeH+L6aqivgboD3nN9yPR9O2HXZqjbd3DnviToOQR6DoXh53o1zp5DoddQktNzOK7WT3X5Fnos/QHnbHgMlr4J6b+E405r12c9agIBWPuiV+MuXQO9T4CvPgFjLjs0uJvqeTx85Zde7fz938GyJ2D132DodDjjThh8xsEvQ1cVCEBFAZRt8GrXJauDYb0WnNcrHEmZ3o+WU270wjpnAvQa1nLZNGy7quRgqDc8794K296FVX/1ttVVAhwgGLiLmkx7POT1e8DwMOu9zcH/803nXdOmIxWRDlNbH2DVjr0s27qLZVt2sXzb7saabXvELV5ESshAEN0S4hsHjWiYlhIcQKJhuX019Y2BvXNfbeO2+mckckbWAb6VU8xoXwH9azaRumc9tnMTlLcyclR8MsQnQXxK8DnZO8UZnwwZOV4A9Rp6MKgzB7T4RzstKR76D4evLYB1/4BF34V558HEa+Cc/4FuR+5acbsEArDuJa/GXbo68uBuKr0vnPMjL7SXz4P3fwtPfgX6T/KmjbjQO/Xemfz1XnCWr/cuC5Rt8J7LNx76Iy01ywvVE87znvuO92rL7fkhEhfnfY8ycsL/iKuvheq97f1Ebaae2ERiQFVNPR9v282yrbv4cMsuVmzfQ01wcIjjs1K5YGxfhmenE2c0jmvc0DqmaTuZg/O9Fxs2biJn4KDG4Rm9MZXrOVAX4EBtPaWVdV8YxrG2PkBifBwnZRk3HFfKxMQdHB/YRq99+cSXr4UdIdcgewyG7LFeCGWP8U5XJnT7YlDHJx3Z2uHIi2DImV6t9v3feoF+7v0w4WsdE2bVFbBpiXeWoPGHSPKhj4YfI+E+++Y879hKPoNew+HyP8DYy9sW3E2ldIcv3QWn/RuseBreeQSeudo7FT/mMohre4Qct20LvP2Jt25cgnd8cfGHPnxN3sf5vNbyDbXqsvWwMx/8B3/4kd4Psk6Ak67xfrhkjYSsEZDau/2fv63iEyEt6+jt7qjtSUSOmrLKGpZv3cWHW3exbOsu1hRWEHDgizPG9Mvg66cN4pTBPZk0uAe905LavoOAH7YshZV/pXL3+6TXtdBSNyH4SD04yQHs34XtLoCGm0eTu3sBfeLXvOc+Y7zriUldqNOQpLRgaM+Gl++CF26FT/4MFz0M2aPbvr2KQli/CNYtgq3/OjSQ2qPXMLj8994p3MMJ7qYSUuCUG+Ck67zGcG//CpY+0NpaYR0PXjPndjHvB13WCBh29sGQ7j3ca2wWYxTgIkdC9V74/APY9g7s3vLF2kRj7SM+TA3EB74ESOkJA07xahOt1PDq/AE+2LyL19YU86+N5Wwp904hJifEMXFgD247azinDO7BScf1aH/vXc5B0QpY+Vf47DnvWmBSBrWpwyGjT5s2ZeD98c0e7dWu+4yGjH5d//pqg+wxMOefsGI+vP4D+L8vwem3eteOE1ObX88575rs+kVeDb5ohTe95/Ew+Wavlp9zIgTqvEZXDdf060Ov6TdzjT9zAIy6pGODuylfPIyb6T389e3axNKlb3Lml84Afx0E6r0fg4H6kIff+/yHvK/3yrXXMO/HhAAKcJGOsW8nfP6u15Bl2ztQvApcwAvpHoO8P9yR/LEKJ7k7DJwcfJwK/U6CpDQqq+tYuqGM19eU8Ma6Uiqr60lOiGPq0N7MPmUgpwzpydh+mZG1zm7Jri1e45yVz8LOjd5nOuE8GDcLTjiPVe980HVuCzqa4uK807UjLoTF/+2dXv7sb3DBgzDywoPL+eu878X6Rd5jz+eAwYBJMP2HXmj3PuGLP166eo3S1774cHEJXggriA+bAlyiy94C2Pg6bHzNa7DyhVptc4+Q+YndIC3be6T3PfR1fISnkysKD4b1tne9YwHveuSAU+DL34XBU71GP4mR3ZOMc17oN4R5RSFs/xC2f+A9b3wNgID5+DzheP514HiW+YezKXkMF4wZzTljcjhjWG9SEjugBrav3LsneOUzULDMmzZoKky5DUbPgJQeh7+PY0VqL5jxGJz4dXj5TlhwFYy4kGwbAc//2ft3q97rfTeOz4Uv/QeccD6kZ3f2kUuUU4BL1+av9wJk46tecJcEuwvIPA4GnNxyzba+Jvi67tD5tftgX5kXlk0ldz8Y6k2e+xatgL//9eBpcYDEdK816vgrvYDrN9FryBIUCDj2VdfRLTG+9f6tzcB8wVOgSd51vd7Dye8/g9cyinn3s3x8hR9zUtwGptomrkx4i2t8r3rj/H2eA24y7D0V+owKaeQVrsFTcvhT8rX7vGuxq56F/CXeLTd9xsDZ98HYmdB9YNv//WLJoNPhln95DdzyHmBU3SLo1gtGfgVGXABDz2r59LpIGynApevZVw75i72aS/4SqN7j1ZyPO927dWf4eV7DlcO5XhrweyFeVQKVJVBVHPJc7E3//D1vWrDjhpHg1TwHTYXJN8GgKZA97pBTic45Pt+5j3fyd/LOpnLe27STXcFbpJIT4khNjCc1KZ5uiT5Sk7zXqQ2vE310S4onLTi/uKKa11eXsDl4PXv8gEzOPXsW54zuywnZaVjA793j21BD3/4BrHkhss/vS/xiuFcUerffZPT3atrjroC+LXaOKE35EmDqt2HcFXyU9yInf+XGI3tNWmKaAlw6XyDgdV+48TXvUbAccJDax7s+OPxcGDqtY68Jxvm8mnV6X8hpYTnnvB8QlSV8uGwZky+4+gu117LKGq8rzvydvJ1fzo49BwBvsIzcEVmMyE6nui7Avtp69tXUs7/WT1VNPftr69l7oI6iPQcap+2rqW8c9zk+zjh9aC/mTB3M2aOzyclscs3QF+91RpEz3vtBAd6Pj12bm2ns1KQx1CGNpKphyJe9246Om9L59/hGu4wcKjNGKLzliFKAy9G3r9xrfdvQpeHn73s1Xwz6nwS534Ph53itcTs7SMy8WndKD/anFkNcHJXVdXy4ZZdXy84vZ31JJQAZyfGcPrQX3zzzeKYM7c3QrPYNV1lT72d/jZ/E+Li2txhv+FEiIsc8BbgcOc55p2Ub+h0uDgZ2xY6Dy/QY7DX2GnaOd1/nUewEoTX7a+spraihtLKG0spqXttYy6Nr3uHTgr34A46k+DhOGdyTGRP7ccaw3ozpl9kh4zgnxfu6zrCTItJlKcClYzjnNexqHNUnOLLP/vLgAubdKjNoarDv4fHewAGd0Jq5qqaekorqYDiHPFfWeNMrayirqKGy5tDbugyYMBD+7cyhTBnWi5OO60FygoJWRDqHAlw6xj+/Cx/O9V7HxXstoUec750G7zveawzViS1wN5dV8fLKIv6xsqjxlHeo5IQ4+qQn0yc9iVF9M/jy8CT6ZCTRJz2Z7ODz5s+Wc8HZUzvh6EVEvkgBLodv1XNeeJ90LUy6wQvvSO+nPoK279rPSysLefnTItYUVQAweXBP/vO8EfTrnkx2ejJ9MpLISk8mIzm+1evVReuipJcwEYkJCnA5PDs3wUvf8XoIu+hh7zaaTlS45wD/WFnEyysL+bTAGxXoxIHd+f5Fo7hofM4XW3KLiEQpBbi0X30N/PV671aZrz7RaeFdWlHNP1YV8fLKIj7a5o2MMbZ/BvdcMJKLxuUwsGeEPaGJiEQRBbi032vf91qWz/7LEe+lyznHvlo/VdX1VFbXUVFdz5qiCl7+tJAPt+7CORjZN53/OPcEvjK+H4N7q8crETm2KcClfda86F33Pu3WQwduaAN/wPHB5p2sKNhDxQEvmKtq6qkMhrT3fHB6wH1xG0OzUrnjrOFcPCGHYX260LCTIiJHmAL8WFRfA5ve8MYWHnVJxw/RuHsrvHCbNyrW2fe1aVXnHJ8W7OXFFYW8vLKQ0kqvm9JEXxzpyfGkJ8eTlhxPelICx/XsRlpyPBnJCd70pHjSG14nxzOgewrD+qS1q7MUEZFopwA/VvjrYPNSWP03WPsy1HgNuDjxarj4kY67Pl1fC899w3s964+HDNzRko0llbz4aSEvflrItp37SfTFkTsiixkn9id3RFb7x6gWEYlR+qsZzQJ+2PovbwzitS/BgV3e6FgjL/L6tN7xESz9udcb2hVPQXLG4e9zyY+87V7xlNeLWgsKdu/npU+LeGHFDtYVVxJnMGVob26dNozzxvQlM6VzW6yLiEQzBXi0CQRg+/vw2fPeyFP7yiAh1RuucOzlMHQ6JCR7y55wHnQ/Dl76NvzxAvjas5DZv/37Xv9PeO83cMpN3pjQYZRX1bBoVREvrChsbBE+8bju3HfxaC4cn0Of9OT2719ERBopwKOBc94IXav/Bqv/DpWF3lCQJ5wHYy73RutKbOZWqYlfh/QcePY6+MPZcPWzXhembbW3AP7+b16vauf++JBZB2r9vLq6mL99soN38svxBxwjstP5z/NGcPH4fhzXS7dxiYh0NAV4V7fiL/DmT2Hv594YzsPOgbH3wwnnQ1JaZNsYNh2+8U+YfwXMuwCueNKbFil/nXfd218Hs/4ECcmNjdGeXb6dl1YUUllTz4AeKXzzy8dzyYn9GNm3A07Xi4hIsxTgXVV9LbxyDyx/AgZMhmn3erdrtXdM7L7j4MbFMH8WPH2F17Bt4tcjW/fNn8D2D+CrT1CWOIC/v7WZZ5dvZ2NpFckJcVw4NodZkwZy6pCexHXAaFwiItI6BXhXVFEEf73OC80pd8D0H4KvA/6pMvt7NfFnr4UXboU9n3tjb7d0G9bGxfD2Lyk4/gp+9Mlg3vzLEuoDjonHdednl4/jovE5ZCSrMZqIyNEWUSqY2fnAI4AP+INz7oEm83sA84ChQDXwDefcZy2ta2Y9gWeAwcBW4Arn3O7D/0hR7vP3vYCtqYSZf/QapnWk5Ey4+jmvYdvSn8Oe7V5tPMztYJs3b6TvMzdQyHFctOZC0tP2cMMZQ5g1aYA6TRER6WStBriZ+YDHgHOAAmCZmb3onFsTsti9wArn3GVmNjK4/PRW1r0HWOKce8DM7gm+v7sjP1xUcQ6W/QFe+R5kDoBr/g7Zo4/MvnwJMOMx6D4I8n4KFTvgyv8HyZlU1dSTt72OR3/zFt8t/S597QBPDXyYx6ZM4cwRWST44o7MMYmISJtEUgOfDOQ75zYDmNkCYAYQGuCjgZ8BOOfWmdlgM8sGjm9h3RlAbnD9J4E8YjXA66rhH3fBivlei/LLfw8p3Y/sPs0g926vD/MXb6fu9+fy+ICfM3dFDZU19dyf+Rynxa2l8oJf8z+nfvXIHouIiLRZJAHeH9ge8r4AOLXJMp8ClwNvm9lkYBAwoJV1s51zRQDOuSIz69P2wz8G7NkOz3wdilbAmXfDmfdA3NGr5a7qfRFL++3juu0/YFb59VQNepAJCTu4YNMzMOFrpJ967VE7FhERiVwkAR6uhVPTYSUeAB4xsxXAKuAToD7CdVveudnNwM0A2dnZ5OXltWX1FlVVVXXo9tqq++6VjF7zv8QF6lk79l522qnw1ltHfL8B5/i0zM8rW+pYvztAsq8fO/vez39W/oz/3HE7fktkf7f+fJQxg0Anlk9X09nfl65K5RKeyiU8lUt47SmXSAK8AAgdK3IAUBi6gHOuApgDYN7IEluCj24trFtiZjnB2ncOUBpu5865ucBcgEmTJrnc3NwIDjkyeXl5dOT2IuYcvPcYrPwh9BoOs+czrvfwI77bA7V+nvu4gHlvb2FLeQ39u6fw/YsGc+UpA0lPToCKi2D+FVjZepKue5UvH6lr8FGq074vXZzKJTyVS3gql/DaUy6RBPgyYLiZDQF2ALOBr4UuYGbdgf3OuVrgRuAt51yFmbW07ovAdXi19+uAF9p05NGqdh+8eLvXFeqoi+HS30HSkW3RXVpZzf97bxt/fn8bu/fXMWFAJr++aiIXjO1LfGijtIx+cONiPnjjH0xReIuIdGmtBrhzrt7MbgNexbsVbJ5zbrWZ3RKc/zgwCnjKzPx4DdRuaGnd4KYfAJ41sxuAz4FZHfvRuqBdm2HB16F0jXdv9xl3dvxQnyHWFVfwxL+28MKKQuoCAc4Zlc1NXz6eSYN6ND8EZ0IytUm9jtgxiYhIx4joPnDn3CJgUZNpj4e8fg8Iew443LrB6TuBNvTnGeXyl8BzcwCDrz/ftq5M27qr0koefGU9r60pISXBx+zJA/nG1CEM7p16xPYpIiJHl3piOxqWz4N//AdkjYTZ86HnkCOym5KKan61eAPPLNtOt8R47jrnBK49fRDdu0U2ZreIiEQPBfiRFAjA4v+Gd3/t3d89c94Rud5dWV3H/y3dzB/e3ow/4Lj29MHcftYweqUldfi+RESka1CAHym1++FvN8G6l73xs89/oGP6Mw/dRX2A+R9s49dv5LNrXy0XT+jHf547QsN3iojEAAX4kVBZAn+ZDYWfeMF96i0d2lgtEHC8vKqIX7y6ns937WfK0F7cc8FIxg/o3mH7EBGRrk0B3tFK1njDde7fCbOf9oYA7UDv5pfzs3+uY9WOvYzsm86f5pzCmSdkNd+qXEREjkkK8I6UvwT+ej0kdIM5i6DfxA7b9NqiCh745zqWbiijX2YyD82awKUT++PT+NsiIjFJAd5Rlv8R/vHvXkvzq5/1RhTrACUV1fz8lXUs/GQH6Unx3HvhSK49fTDJCb4O2b6IiEQnBfjhCgRg8Q/h3Udh2NneGN7JGR2y6aUbyrjzmRVU1dRz05eO51u5Q3VLmIiIAArww1O7HxbeDGtfgkk3wAUPdkhL83p/gF8t3shjefmc0CedZ795GsP6HNnuVkVEJLoowNsrtKX5eT+F077VIS3NSyqqueMvn/DBll1cMWkAP7pkLCmJOl0uIiKHUoC3R+lamH8F7C/3elYbeVGHbPZfG8v4zoIV7K/189CsCXz15I65ji4iIsceBXhbff4BzJ8JCSkd1tLcH3A8sngDv34zn2FZaSy4+SSGZ+uUuYiINE8B3ha1+7ze1br1hOtehu4DW1+nFaUV1dyx4BPe37yLmScP4H9mjKFbov5ZRESkZUqKtsj7GezZBtf/o0PC+538cr69YAVVNXX878zxzJp0+NsUEZHYoACPVOEKeO8xOOlaGHzGYW3KH3A8umQjj76xkaFZaTx906mcoFPmIiLSBgrwSPjr4cXboVtvOOd/DmtTpZXVfGfBCt7dtJPLT+rPjy8dq1PmIiLSZkqOSLz/WyheCbP+BCk92r2Zdzd5p8wrq3XKXEREDo8CvDW7tsCbP4URF8LoS9u9mWeWfc73/raKIb1T+fMNpzKir06Zi4hI+ynAW+IcvHwnxMXDhb9od0ctH23bzff//hlTh/Xm8a+fTGqSil1ERA6PkqQlK5+BzW964Z3Zv12bKK2s5lvzPyInM4XfXHWSwltERDqE0qQ5+8rhle/BgMleP+ftUOcPcNv8T9h7oI6F35pMZreEDj5IERGJVQrw5rx6L9RUwiWPQlxcuzbxs0Xr+HDrLh6ZfSKjcjpmhDIRERGA9iXTsS5/sXf6/Iw7oc+odm3ihRU7mPfOFq6fMpgZJ7bv9LuIiEhzFOBN1e7zGq71Gg5f+vd2bWJdcQX3PL+KUwb34L8uat8PABERkZboFHpTb/4U9nwOc/4JCcltXn3vgTq++f8+Ij05nse+dhIJPv1GEhGRjhdRupjZ+Wa23szyzeyeMPMzzewlM/vUzFab2Zzg9BFmtiLkUWFm3wnOu8/MdoTMu7BDP1l7FH7iddpy8vUwaEqbVw8EHHc9s4Iduw/wu6+fRJ+Mtv8AEBERiUSrNXAz8wGPAecABcAyM3vRObcmZLFbgTXOuYvNLAtYb2bznXPrgRNDtrMDWBiy3i+dc7/omI9ymPz18OIdkNoHzv5Ruzbx6zfyWbKulB9dMoaTB/Xs4AMUERE5KJIa+GQg3zm32TlXCywAZjRZxgHpZmZAGrALqG+yzHRgk3Nu22Ee85Hx/mNed6kXPggp3du8+pvrS/nVkg1cPrE/154+qOOPT0REJEQkAd4f2B7yviA4LdRvgFFAIbAK+LZzLtBkmdnAX5pMu83MVprZPDNrfyfjh2vXFnjzZzDiIhh1SZtX37ZzH9/+yyeM7JvBTy4bh7WzxzYREZFImXOu5QXMZgHnOeduDL6/BpjsnLs9ZJmZwFTgLmAo8DowwTlXEZyfiBfuY5xzJcFp2UA5Xu39fiDHOfeNMPu/GbgZIDs7++QFCxYc1gcOVVVVRVpqKuNX/pCMig18OPkxapN6tWkbNX7Hj9+vZld1gB+enkKfbtHfaK2qqoq0tLTOPowuR+USnsolPJVLeCqX8Jorl2nTpn3knJsUbp1IWqEXAKHDZg3AC+NQc4AHnPdrIN/MtgAjgQ+D8y8APm4Ib4DQ12b2e+DlcDt3zs0F5gJMmjTJ5ebmRnDIkcnLyyO3exHs/hQu/AVTJn+1Tes757jzmRUUVBUy7/pTmDaiT4cdW2fKy8ujI8v5WKFyCU/lEp7KJTyVS3jtKZdIqovLgOFmNiRYk54NvNhkmc/xrnE31KxHAJtD5l9Fk9PnZpYT8vYy4LM2HXkHSKjd6/W4NvDUdnWX+uS7W/n7ikLuPPuEYya8RUQkOrRaA3fO1ZvZbcCrgA+Y55xbbWa3BOc/jncK/E9mtgow4G7nXDmAmXXDa8H+zSabftDMTsQ7hb41zPwjblj+E153qRe3vbvUZVt38eN/rOXsUX24bdqwI3SEIiIi4UXUkYtzbhGwqMm0x0NeFwLnNrPufuALF5adc9e06Ug72sbXyS5dCmfeA31GtmnV0opqvjX/Ywb0SOGhK04kLk6N1kRE5OiK/hZX7bVrC1WpQ+BLd7Vptdr6AN+a/zFV1fX83zWTyEzRCGMiInL0xW5XqqfezEf7jufM+KQ2rfbUe1tZvm03j141kRF904/QwYmIiLQsdmvggItr+++Xj7btZnCvblwyod8ROCIREZHIxHSAt8f64krVvEVEpNMpwNugus7P1p37GNE3o7MPRUREYpwCvA02llQRcDBSNXAREelkCvA2WFdcAaBT6CIi0ukU4G2wvriSpPg4BvdK7exDERGRGKcAb4P1JZUMz07Dp45bRESkkynA22BtUSUj1YBNRES6AAV4hHZW1VBeVaMGbCIi0iUowCO0vrgSUAM2ERHpGhTgEVqnABcRkS5EAR6h9cWV9ExNJCutbX2ni4iIHAkK8AitK6lkRHY6ZmqBLiIinU8BHoFAwLGxRH2gi4hI16EAj8D23fvZX+tXC3QREekyFOARUAM2ERHpahTgEWi4heyEbAW4iIh0DQrwCKwvruS4nt1ITYrv7EMREREBFOARWVdcodPnIiLSpSjAW1Fd52frzv1qwCYiIl2KArwV+aVV+ANONXAREelSFOCtaGjAphq4iIh0JQrwVqwvqSQxPo7BvVI7+1BEREQaRRTgZna+ma03s3wzuyfM/Ewze8nMPjWz1WY2J2TeVjNbZWYrzGx5yPSeZva6mW0MPvfomI/UsdYWVTC8TxrxPv3WERGRrqPVVDIzH/AYcAEwGrjKzEY3WexWYI1zbgKQCzxkZokh86c55050zk0KmXYPsMQ5NxxYEnzf5awvVheqIiLS9URSrZwM5DvnNjvnaoEFwIwmyzgg3byRPtKAXUB9K9udATwZfP0kcGmkB3207N5XS2llja5/i4hIlxNJgPcHtoe8LwhOC/UbYBRQCKwCvu2cCwTnOeA1M/vIzG4OWSfbOVcEEHzu047jP6IOdqGa0clHIiIicqhIuhYLN36ma/L+PGAFcBYwFHjdzP7lnKsApjrnCs2sT3D6OufcW5EeYDD0bwbIzs4mLy8v0lVbVVVV1eL2Xt9WB8CuzavIK4yda+CtlUusUrmEp3IJT+USnsolvPaUSyQBXgAMDHk/AK+mHWoO8IBzzgH5ZrYFGAl86JwrBHDOlZrZQrxT8m8BJWaW45wrMrMcoDTczp1zc4G5AJMmTXK5ubkRf7jW5OXl0dL2Xv3bSrp3K+bS86bF1DjgrZVLrFK5hKdyCU/lEp7KJbz2lEsk1cplwHAzGxJsmDYbeLHJMp8D0wHMLBsYAWw2s1QzSw9OTwXOBT4LrvMicF3w9XXAC2068qNgXXElI7LTYyq8RUQkOrRaA3fO1ZvZbcCrgA+Y55xbbWa3BOc/DtwP/MnMVuGdcr/bOVduZscDC4MBGA887Zx7JbjpB4BnzewGvB8Aszr4sx2WQMCxobiSmScP6OxDERER+YKIhtdyzi0CFjWZ9njI60K82nXT9TYDE5rZ5k6CtfauaMeeA+yr9asBm4iIdEmx0zKrjQ62QNctZCIi0vUowJuxvrgCUICLiEjXpABvxrriSgb0SCEtKaKrDCIiIkeVArwZ64sr1QObiIh0WQrwMGrq/Wwu38dINWATEZEuSgEexqbSffgDTte/RUSky1KAh7Eu2IBNp9BFRKSrUoCHsb64kkRfHIN7p3b2oYiIiISlAA9jXXElQ/ukkeBT8YiISNekhApDLdBFRKSrU4A3sXd/HcUV1WrAJiIiXZoCvIl16oFNRESigAK8ifUlXh/oOoUuIiJdmQK8iXXFlWQkx9M3I7mzD0VERKRZCvAmvAZsGQTHMBcREemSFOAhnHNsKK7U9W8REenyFOAhduw5QGVNvQJcRES6PAV4iPXFasAmIiLRQQEeYl0wwE9QgIuISBenAA+xvriS/t1TyEhO6OxDERERaZECPIS6UBURkWihAA+qrQ+wqaxKDdhERCQqKMCDNpVVUR9wCnAREYkKCvCggy3QMzr5SERERFqnAA9aV1xJgs84Piu1sw9FRESkVREFuJmdb2brzSzfzO4JMz/TzF4ys0/NbLWZzQlOH2hmb5rZ2uD0b4esc5+Z7TCzFcHHhR33sdpufXEFQ7PSSPDpN42IiHR98a0tYGY+4DHgHKAAWGZmLzrn1oQsdiuwxjl3sZllAevNbD5QD/y7c+5jM0sHPjKz10PW/aVz7hcd+onaaX1xJacM6dnZhyEiIhKRSKqbk4F859xm51wtsACY0WQZB6SbNwJIGrALqHfOFTnnPgZwzlUCa4H+HXb0HWTvgToK91arAZuIiESNSAK8P7A95H0BXwzh3wCjgEJgFfBt51wgdAEzGwxMBD4ImXybma00s3lm1qONx95hNmgMcBERiTLmnGt5AbNZwHnOuRuD768BJjvnbg9ZZiYwFbgLGAq8DkxwzlUE56cBS4GfOOf+FpyWDZTj1d7vB3Kcc98Is/+bgZsBsrOzT16wYMFhfeBQVVVVpKWl8cbndTy1ppaHzkyhV4qugTeUixxK5RKeyiU8lUt4KpfwmiuXadOmfeScmxRunVavgePVuAeGvB+AV9MONQd4wHm/BvLNbAswEvjQzBKA54H5DeEN4JwraXhtZr8HXg63c+fcXGAuwKRJk1xubm4EhxyZvLw8cnNzWfz3VaQnF3L5+dM0DjgHy0UOpXIJT+USnsolPJVLeO0pl0iqm8uA4WY2xMwSgdnAi02W+RyYDo016xHA5uA18SeAtc65h0NXMLOckLeXAZ+16cg70PriSkZkpyu8RUQkarRaA3fO1ZvZbcCrgA+Y55xbbWa3BOc/jncK/E9mtgow4G7nXLmZnQFcA6wysxXBTd7rnFsEPGhmJ+KdQt8KfLNDP1mEnHOsK67kkgn9OmP3IiIi7RLJKXSCgbuoybTHQ14XAueGWe9tvEAPt81r2nSkR0jR3moqq+sZmaMe2EREJHrEfIutg12oqgW6iIhEj5gP8HXBAD8hWwEuIiLRQwFeXEG/zGQyUxI6+1BEREQiFvMBvr64Uj2wiYhI1InpAK8PODaVVTFCQ4iKiEiUiekAL97nqPM7NWATEZGoE9MBXlDpddeuU+giIhJtYjvAqwLExxlDs9Qvr4iIRJeYDvDtlQGOz0olMT6mi0FERKJQTCdXQWVADdhERCQqxWyAV1bXsbNaDdhERCQ6xWyAbyjxemAboR7YREQkCsVsgDd0oaoW6CIiEo1iNsB3VdWSlgADeqR09qGIiIi0WUTDiR6Lbp8+nNFWgFnY0U5FRES6tJitgQP44hTeIiISnWI6wEVERKKVAlxERCQKKcBFRESikAJcREQkCinARUREopACXEREJAopwEVERKKQAlxERCQKKcBFRESiUEQBbmbnm9l6M8s3s3vCzM80s5fM7FMzW21mc1pb18x6mtnrZrYx+NyjYz6SiIjIsa/VADczH/AYcAEwGrjKzEY3WexWYI1zbgKQCzxkZomtrHsPsMQ5NxxYEnwvIiIiEYikBj4ZyHfObXbO1QILgBlNlnFAunkjg6QBu4D6VtadATwZfP0kcOnhfBAREZFYEkmA9we2h7wvCE4L9RtgFFAIrAK+7ZwLtLJutnOuCCD43KfNRy8iIhKjIhlONNyQXa7J+/OAFcBZwFDgdTP7V4Trtrxzs5uBm4Nvq8xsfVvWb0VvoLwDt3esULmEp3IJT+USnsolPJVLeM2Vy6DmVogkwAuAgSHvB+DVtEPNAR5wzjkg38y2ACNbWbfEzHKcc0VmlgOUhtu5c24uMDeC42wzM1vunJt0JLYdzVQu4alcwlO5hKdyCU/lEl57yiWSU+jLgOFmNsTMEoHZwItNlvkcmB48iGxgBLC5lXVfBK4Lvr4OeKEtBy4iIhLLWq2BO+fqzew24FXAB8xzzq02s1uC8x8H7gf+ZGar8E6b3+2cKwcIt25w0w8Az5rZDXg/AGZ17EcTERE5dkVyCh3n3CJgUZNpj4e8LgTOjXTd4PSdBGvtneiInJo/BqhcwlO5hKdyCU/lEp7KJbw2l4t5l61FREQkmqgrVRERkSgUswHeWvewscrMtprZKjNbYWbLO/t4OouZzTOzUjP7LGRazHf/20y53GdmO4LfmRVmdmFnHuPRZmYDzexNM1sb7Er628HpMf19aaFcYv37kmxmH4Z0Pf6j4PQ2f19i8hR6sIvXDcA5eLe6LQOucs6t6dQD6wLMbCswqaERYqwysy8DVcBTzrmxwWkPAruccw8Ef/T1cM7d3ZnHebQ1Uy73AVXOuV905rF1luBtsDnOuY/NLB34CK9nyeuJ4e9LC+VyBbH9fTEg1TlXZWYJwNvAt4HLaeP3JVZr4JF0DysxzDn3Fl6XwKFivvvfZsolpjnnipxzHwdfVwJr8XqcjOnvSwvlEtOcpyr4NiH4cLTj+xKrAR5J97CxygGvmdlHwV7w5CB1/9u828xsZfAUe0ydKg5lZoOBicAH6PvSqEm5QIx/X8zMZ2Yr8Dowe905167vS6wG+GF38XoMm+qcOwlvBLlbg6dMRVryO7wulE8EioCHOvVoOomZpQHPA99xzlV09vF0FWHKJea/L845v3PuRLzeSSeb2dj2bCdWAzyS7mFjUvCefpxzpcBCvMsN4ikJXtdruL4XtvvfWOOcKwn+QQoAvycGvzPBa5nPA/Odc38LTo7570u4ctH35SDn3B4gDzifdnxfYjXAI+keNuaYWWqwsQlmlorXOc9nLa8VU9T9bxgNf3SCLiPGvjPBRklPAGudcw+HzIrp70tz5aLvi2WZWffg6xTgbGAd7fi+xGQrdIDgrQu/4mAXrz/p3CPqfGZ2PF6tG7xe+p6O1XIxs78AuXgjBJUAPwT+DjwLHEew+1/nXEw16GqmXHLxToc6YCvwzYZrebHAzM4A/oU3lHIgOPlevOu9Mft9aaFcriK2vy/j8Rqp+fAq0c865/7HzHrRxu9LzAa4iIhINIvVU+giIiJRTQEuIiIShRTgIiIiUUgBLiIiEoUU4CIiIlFIAS4SQ8zMHzIK1IqOHInPzAaHjlImIkdWfGcfgIgcVQeCXTiKSJRTDVxEGsaB/3lwnOIPzWxYcPogM1sSHHhiiZkdF5yebWYLg2Maf2pmU4Kb8pnZ74PjHL8W7GlKRI4ABbhIbElpcgr9ypB5Fc65ycBv8HopJPj6KefceGA+8Ghw+qPAUufcBOAkYHVw+nDgMefcGGAP8NUj+mlEYph6YhOJIWZW5ZxLCzN9K3CWc25zcACKYudcLzMrB3Kcc3XB6UXOud5mVgYMcM7VhGxjMN7QiMOD7+8GEpxzPz4KH00k5qgGLiINXDOvm1smnJqQ137UzkbkiFGAi0iDK0Oe3wu+fhdvtD6Aq4G3g6+XAP8GYGY+M8s4WgcpIh79OhaJLSlmtiLk/SvOuYZbyZLM7AO8H/ZXBafdAcwzs/8EyoA5wenfBuaa2Q14Ne1/A2JmRCmRrkDXwEWk4Rr4JOdceWcfi4hERqfQRUREopBq4CIiIlFINXAREZEopAAXERGJQgpwERGRKKQAFxERiUIKcBERkSikABcREYlC/x9RISLgRErT9gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the history dictionary to a Pandas dataframe and extract the accuracies\n",
    "accuracies = pd.DataFrame(cnn_history.history)[['accuracy', 'val_accuracy']]\n",
    "\n",
    "# Plot the accuracies\n",
    "accuracies.plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0.8, 1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108dee17",
   "metadata": {},
   "source": [
    "The accuracy of validation set fluctuated since epoch = 12, thus, we set the epochs = 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba1bd8d",
   "metadata": {},
   "source": [
    "Best paras for K-nearest neighbors is:  \n",
    "'n_neighbors'= 3, 'p'= 1, 'weights'= 'distance'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa9ad490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the best paras\n",
    "knn_best_paras = dict({'n_neighbors': 3, 'p': 1, 'weights': 'distance'})\n",
    "\n",
    "knn = KNeighborsClassifier(**knn_best_paras)\n",
    "knn_runtime = time.time()\n",
    "knn.fit(X_train_full.reshape(X_train_full.shape[0], -1), y_train_full)\n",
    "knn_runtime = time.time() - knn_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f9cda9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN training time: 0.07 s\n",
      "KNN score on the test set: 0.8597\n"
     ]
    }
   ],
   "source": [
    "# Running in around 120s\n",
    "\n",
    "# Performance on test set.\n",
    "print(f\"KNN training time: {knn_runtime:.2f} s\")\n",
    "print(f\"KNN score on the test set: {knn.score(X_test.reshape(X_test.shape[0], -1), y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Fully connected neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000e22b",
   "metadata": {},
   "source": [
    "We settled the size of layers (784, 100, 60, 10) in section 2.2; and settled the the best paras of our experimental settings:   \n",
    "'activation_function': 'relu', 'optimizer': 'Adam', 'optimizer__lr': 0.001   \n",
    "epochs: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bab3c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 60)                6060      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                610       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,170\n",
      "Trainable params: 85,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.5115 - accuracy: 0.8196 - val_loss: 0.3997 - val_accuracy: 0.8535\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3767 - accuracy: 0.8640 - val_loss: 0.3651 - val_accuracy: 0.8670\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3391 - accuracy: 0.8759 - val_loss: 0.3397 - val_accuracy: 0.8723\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.3171 - accuracy: 0.8829 - val_loss: 0.3278 - val_accuracy: 0.8783\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2985 - accuracy: 0.8889 - val_loss: 0.3359 - val_accuracy: 0.8768\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2863 - accuracy: 0.8931 - val_loss: 0.3239 - val_accuracy: 0.8792\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2728 - accuracy: 0.8998 - val_loss: 0.3243 - val_accuracy: 0.8797\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2595 - accuracy: 0.9026 - val_loss: 0.3194 - val_accuracy: 0.8820\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2523 - accuracy: 0.9046 - val_loss: 0.3274 - val_accuracy: 0.8808\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 2s 1ms/step - loss: 0.2444 - accuracy: 0.9075 - val_loss: 0.3019 - val_accuracy: 0.8902\n"
     ]
    }
   ],
   "source": [
    "# Running in around 25s\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Set Random seed to 0\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# Build the final model\n",
    "mlp = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=IMAGE_SIZE),\n",
    "    keras.layers.Dense(100, activation=\"relu\", kernel_initializer=initializer),\n",
    "    keras.layers.Dense(60, activation=\"relu\", kernel_initializer=initializer),\n",
    "    keras.layers.Dense(10, activation=\"softmax\", kernel_initializer=initializer)\n",
    "])\n",
    "\n",
    "mlp.summary()\n",
    "\n",
    "# Complie the model\n",
    "mlp.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "mlp_runtime = time.time()\n",
    "mlp.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), verbose=1)\n",
    "mlp_runtime = time.time() - mlp_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae075211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 971us/step - loss: 0.3335 - accuracy: 0.8864\n",
      "MLP training time: 24.62 s\n",
      "MLP score on the test set: 0.8864\n"
     ]
    }
   ],
   "source": [
    "# Performance on test set.\n",
    "loss, accuracy = mlp.evaluate(X_test, y_test)\n",
    "print(f\"MLP training time: {mlp_runtime:.2f} s\")\n",
    "print(f\"MLP score on the test set: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e455d9a8",
   "metadata": {},
   "source": [
    "We designed a proper architecture of CNN in section 2.3; and settled the the best paras of our experimental settings:   \n",
    "'kernel_size': (3, 3), 'optimizer__lr': 0.001, 'strides': (1, 1)  \n",
    "epochs: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e515b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                23050     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,866\n",
      "Trainable params: 41,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/12\n",
      "1688/1688 [==============================] - 16s 10ms/step - loss: 0.5201 - accuracy: 0.8115 - val_loss: 0.3583 - val_accuracy: 0.8712\n",
      "Epoch 2/12\n",
      "1688/1688 [==============================] - 16s 9ms/step - loss: 0.3668 - accuracy: 0.8700 - val_loss: 0.3091 - val_accuracy: 0.8873\n",
      "Epoch 3/12\n",
      "1688/1688 [==============================] - 16s 9ms/step - loss: 0.3272 - accuracy: 0.8831 - val_loss: 0.2851 - val_accuracy: 0.8962\n",
      "Epoch 4/12\n",
      "1688/1688 [==============================] - 16s 9ms/step - loss: 0.3026 - accuracy: 0.8908 - val_loss: 0.2628 - val_accuracy: 0.9043\n",
      "Epoch 5/12\n",
      "1688/1688 [==============================] - 16s 9ms/step - loss: 0.2882 - accuracy: 0.8964 - val_loss: 0.2507 - val_accuracy: 0.9078\n",
      "Epoch 6/12\n",
      "1688/1688 [==============================] - 16s 9ms/step - loss: 0.2735 - accuracy: 0.9010 - val_loss: 0.2696 - val_accuracy: 0.8988\n",
      "Epoch 7/12\n",
      "1688/1688 [==============================] - 16s 9ms/step - loss: 0.2611 - accuracy: 0.9049 - val_loss: 0.2415 - val_accuracy: 0.9127\n",
      "Epoch 8/12\n",
      "1688/1688 [==============================] - 16s 9ms/step - loss: 0.2527 - accuracy: 0.9085 - val_loss: 0.2334 - val_accuracy: 0.9147\n",
      "Epoch 9/12\n",
      "1688/1688 [==============================] - 16s 9ms/step - loss: 0.2463 - accuracy: 0.9091 - val_loss: 0.2297 - val_accuracy: 0.9177\n",
      "Epoch 10/12\n",
      "1688/1688 [==============================] - 16s 9ms/step - loss: 0.2414 - accuracy: 0.9126 - val_loss: 0.2257 - val_accuracy: 0.9175\n",
      "Epoch 11/12\n",
      "1688/1688 [==============================] - 16s 9ms/step - loss: 0.2351 - accuracy: 0.9136 - val_loss: 0.2237 - val_accuracy: 0.9165\n",
      "Epoch 12/12\n",
      "1688/1688 [==============================] - 17s 10ms/step - loss: 0.2296 - accuracy: 0.9164 - val_loss: 0.2211 - val_accuracy: 0.9190\n"
     ]
    }
   ],
   "source": [
    "# Running in around 200s\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Set Random seed to 0\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=0)\n",
    "\n",
    "# Build the final model\n",
    "cnn = keras.Sequential([\n",
    "        # Specify the input shape\n",
    "        keras.Input(shape=(*IMAGE_SIZE, 1)),\n",
    "        \n",
    "        # Conv and pool block 1\n",
    "        keras.layers.Conv2D(\n",
    "            32, \n",
    "            kernel_size=(3, 3), \n",
    "            activation=\"relu\", \n",
    "            strides=(1, 1),\n",
    "            kernel_initializer=initializer\n",
    "            ),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'), # padding evenly\n",
    "        \n",
    "        # Conv and pool block 2\n",
    "        keras.layers.Conv2D(\n",
    "            64, \n",
    "            kernel_size=(3, 3), \n",
    "            activation=\"relu\", \n",
    "            strides=(1, 1),\n",
    "            kernel_initializer=initializer\n",
    "            ),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'), # padding evenly\n",
    "        \n",
    "        # Flatten and classify using dense output layer\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(len(class_names), activation=\"softmax\", kernel_initializer=initializer),\n",
    "    ])\n",
    "\n",
    "cnn.summary()\n",
    "\n",
    "# Complie the model\n",
    "cnn.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_runtime = time.time()\n",
    "cnn.fit(np.expand_dims(X_train, -1), y_train, epochs=12, validation_data=(np.expand_dims(X_valid, -1), y_valid), verbose=1)\n",
    "cnn_runtime = time.time() - cnn_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "487671f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2414 - accuracy: 0.9126\n",
      "CNN training time: 191.29 s\n",
      "CNN score on the test set: 0.9126\n"
     ]
    }
   ],
   "source": [
    "# Performance on test set.\n",
    "loss, accuracy = cnn.evaluate(np.expand_dims(X_test, -1), y_test)\n",
    "print(f\"CNN training time: {cnn_runtime:.2f} s\")\n",
    "print(f\"CNN score on the test set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a0245d",
   "metadata": {},
   "source": [
    "Finalized at 10/12 1:30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a8c4580c33f1a0ec11d1ec1ec999c7eec26eef61726f4404092dec9f099378c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
